{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2fb41fd-c50f-4033-9cbf-273df7637194",
   "metadata": {},
   "source": [
    "## Импорт необходимых зависимостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "973dc9dc-9e2e-4d2d-8c9d-98fa61b38d55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ml_env\\lib\\site-packages\\pytorch_lightning\\metrics\\__init__.py:43: LightningDeprecationWarning: `pytorch_lightning.metrics.*` module has been renamed to `torchmetrics.*` and split off to its own package (https://github.com/PyTorchLightning/metrics) since v1.3 and will be removed in v1.5\n",
      "  rank_zero_deprecation(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "\n",
    "import pickle\n",
    "import pathlib\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn import model_selection\n",
    "\n",
    "from pprint import pprint\n",
    "from random import choice\n",
    "from typing import List, Union\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a8e1272-bf97-4890-945c-6615ba132abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt', quiet=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6e4800-d29f-4ac2-88a3-e5ee010d9901",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91d515aa-7400-443e-9e9a-5c3c34d3ef65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RANDOM_STATE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c2c1321-83cd-4e5d-afaa-9ae0992153b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/lenta/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c448591d-3ea9-44d1-bfc0-26048460a7b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orig_texts</th>\n",
       "      <th>lemm_texts</th>\n",
       "      <th>nsubj</th>\n",
       "      <th>gender</th>\n",
       "      <th>tense</th>\n",
       "      <th>sent_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>782094</th>\n",
       "      <td>депутат от \"справедливой россии\" дмитрий гудко...</td>\n",
       "      <td>депутат от \"справедливый россия\"дмитрий гудков...</td>\n",
       "      <td>депутат</td>\n",
       "      <td>masc</td>\n",
       "      <td>past</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1589446</th>\n",
       "      <td>как сообщается, бои идут между иракцами, амери...</td>\n",
       "      <td>как сообщаться, бой идти между иракец, америка...</td>\n",
       "      <td>бои</td>\n",
       "      <td>masc</td>\n",
       "      <td>pres</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1268862</th>\n",
       "      <td>обращение подписали уже 15 тысяч человек, пише...</td>\n",
       "      <td>обращение подписать уже 15 тысяча человек, пис...</td>\n",
       "      <td>тысяч</td>\n",
       "      <td>fem</td>\n",
       "      <td>past</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392907</th>\n",
       "      <td>конфликт между ними произошел из-за брата осуж...</td>\n",
       "      <td>конфликт между они произойти из-за брат осуждё...</td>\n",
       "      <td>конфликт</td>\n",
       "      <td>masc</td>\n",
       "      <td>past</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486500</th>\n",
       "      <td>речь идет об эпизоде «суровый дом» (восьмая се...</td>\n",
       "      <td>речь идти о эпизод « суровый дом » (восьмой се...</td>\n",
       "      <td>речь</td>\n",
       "      <td>fem</td>\n",
       "      <td>pres</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1534870</th>\n",
       "      <td>чем больше времени дети проводят у телевизоров...</td>\n",
       "      <td>чем большой время ребёнок проводить у телевизо...</td>\n",
       "      <td>дети</td>\n",
       "      <td>masc</td>\n",
       "      <td>pres</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647177</th>\n",
       "      <td>этим и воспользовалась женщина (имя которой не...</td>\n",
       "      <td>это и воспользоваться женщина (имя который не ...</td>\n",
       "      <td>женщина</td>\n",
       "      <td>fem</td>\n",
       "      <td>past</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595915</th>\n",
       "      <td>представитель «народной армии донбасса» требов...</td>\n",
       "      <td>представитель « народный армия донбасс » требо...</td>\n",
       "      <td>представитель</td>\n",
       "      <td>masc</td>\n",
       "      <td>past</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1081552</th>\n",
       "      <td>по размеру розничной сети почтовый банк станет...</td>\n",
       "      <td>по размер розничный сеть почтовый банк стать к...</td>\n",
       "      <td>банк</td>\n",
       "      <td>masc</td>\n",
       "      <td>fut</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1054281</th>\n",
       "      <td>калининградское отделение кпрф приняло решение...</td>\n",
       "      <td>калининградский отделение кпрф принять решение...</td>\n",
       "      <td>отделение</td>\n",
       "      <td>neut</td>\n",
       "      <td>past</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                orig_texts  \\\n",
       "782094   депутат от \"справедливой россии\" дмитрий гудко...   \n",
       "1589446  как сообщается, бои идут между иракцами, амери...   \n",
       "1268862  обращение подписали уже 15 тысяч человек, пише...   \n",
       "392907   конфликт между ними произошел из-за брата осуж...   \n",
       "486500   речь идет об эпизоде «суровый дом» (восьмая се...   \n",
       "...                                                    ...   \n",
       "1534870  чем больше времени дети проводят у телевизоров...   \n",
       "647177   этим и воспользовалась женщина (имя которой не...   \n",
       "595915   представитель «народной армии донбасса» требов...   \n",
       "1081552  по размеру розничной сети почтовый банк станет...   \n",
       "1054281  калининградское отделение кпрф приняло решение...   \n",
       "\n",
       "                                                lemm_texts          nsubj  \\\n",
       "782094   депутат от \"справедливый россия\"дмитрий гудков...        депутат   \n",
       "1589446  как сообщаться, бой идти между иракец, америка...            бои   \n",
       "1268862  обращение подписать уже 15 тысяча человек, пис...          тысяч   \n",
       "392907   конфликт между они произойти из-за брат осуждё...       конфликт   \n",
       "486500   речь идти о эпизод « суровый дом » (восьмой се...           речь   \n",
       "...                                                    ...            ...   \n",
       "1534870  чем большой время ребёнок проводить у телевизо...           дети   \n",
       "647177   это и воспользоваться женщина (имя который не ...        женщина   \n",
       "595915   представитель « народный армия донбасс » требо...  представитель   \n",
       "1081552  по размер розничный сеть почтовый банк стать к...           банк   \n",
       "1054281  калининградский отделение кпрф принять решение...      отделение   \n",
       "\n",
       "        gender tense  sent_length  \n",
       "782094    masc  past         23.0  \n",
       "1589446   masc  pres         15.0  \n",
       "1268862    fem  past         15.0  \n",
       "392907    masc  past         23.0  \n",
       "486500     fem  pres         23.0  \n",
       "...        ...   ...          ...  \n",
       "1534870   masc  pres         13.0  \n",
       "647177     fem  past         29.0  \n",
       "595915    masc  past         11.0  \n",
       "1081552   masc   fut         15.0  \n",
       "1054281   neut  past         28.0  \n",
       "\n",
       "[50000 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(n=50000, random_state=RANDOM_STATE)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c8d7d0-c3b0-4d3a-b3a0-a9c513f0a39b",
   "metadata": {},
   "source": [
    "### Определение классов словаря и подготовщика данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "761fae45-0202-4290-926e-5c52ef489210",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, tokens=None, unk_id=None):\n",
    "        self.unk_id = unk_id\n",
    "        \n",
    "        self.tokens = tokens\n",
    "        self.tokens_to_ids = {token: id for id, token in enumerate(tokens)} if tokens is not None else None\n",
    "    \n",
    "    def id_to_token(self, id):\n",
    "        return self.tokens[id]\n",
    "    \n",
    "    def token_to_id(self, token):\n",
    "        return self.tokens_to_ids.get(token, self.unk_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d61e34b1-1eb1-4dfd-a2fe-d359eaf0308e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, vocab_size):\n",
    "        \n",
    "        self.tokenizer = nltk.tokenize.word_tokenize\n",
    "        \n",
    "        self.special_tokens = {\n",
    "            'pad_token'        : '<pad>',\n",
    "            'unk_token'        : '<unk>',\n",
    "            'sos_token'        : '<sos>',\n",
    "            'eos_token'        : '<eos>',\n",
    "            'g_masc_token'     : '<masc>',\n",
    "            'g_fem_token'      : '<fem>',\n",
    "            'g_neut_token'     : '<neut>',\n",
    "            'g_undefined_token': '<undef>',\n",
    "            't_past_token'     : '<past>',\n",
    "            't_pres_token'     : '<pres>',\n",
    "            't_fut_token'      : '<fut>'\n",
    "        }\n",
    "        \n",
    "        self.special_ids = {token: id for id, token in enumerate(self.special_tokens.keys())}\n",
    "        \n",
    "        self.pad_token = {'id'   : self.special_ids['pad_token'],\n",
    "                          'token': self.special_tokens['pad_token']}\n",
    "        \n",
    "        self.unk_token = {'id'   : self.special_ids['unk_token'],\n",
    "                          'token': self.special_tokens['unk_token']}\n",
    "        \n",
    "        self.sos_token = {'id'   : self.special_ids['sos_token'],\n",
    "                          'token': self.special_tokens['sos_token']}\n",
    "        \n",
    "        self.eos_token = {'id'   : self.special_ids['eos_token'],\n",
    "                          'token': self.special_tokens['eos_token']}\n",
    "        \n",
    "        self.gender_tokens = {\n",
    "            'masc'     : {'id'   : self.special_ids['g_masc_token'],\n",
    "                          'token': self.special_tokens['g_masc_token']},\n",
    "            \n",
    "            'fem'      : {'id'   : self.special_ids['g_fem_token'],\n",
    "                          'token': self.special_tokens['g_fem_token']},\n",
    "            \n",
    "            'neut'     : {'id'   : self.special_ids['g_neut_token'],\n",
    "                          'token': self.special_tokens['g_neut_token']},\n",
    "            \n",
    "            'undefined': {'id'   : self.special_ids['g_undefined_token'],\n",
    "                          'token': self.special_tokens['g_undefined_token']}\n",
    "        }\n",
    "        \n",
    "        self.tense_tokens = {\n",
    "            'past': {'id'   : self.special_ids['t_past_token'],\n",
    "                     'token': self.special_tokens['t_past_token']},\n",
    "            \n",
    "            'pres': {'id'   : self.special_ids['t_pres_token'],\n",
    "                     'token': self.special_tokens['t_pres_token']},\n",
    "            \n",
    "            'fut' : {'id'   : self.special_ids['t_fut_token'],\n",
    "                     'token': self.special_tokens['t_fut_token']}\n",
    "        }\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.vocab_cache_path = {\n",
    "            'dir': './data/cached',\n",
    "            'filename': 'vocab.pkl'\n",
    "        }\n",
    "        \n",
    "        self.vocab = None\n",
    "        \n",
    "    def _tokenize(self, input: Union[List[str], str]):\n",
    "        \"\"\"Input (Union[List[str], str]): a list of string sequences or a single sequence.\"\"\"\n",
    "        \n",
    "        if type(input) is list:\n",
    "            tokens = list(chain(*[self.tokenizer(text, 'russian') for text in tqdm(input, 'Tokenizing texts')]))\n",
    "        else:\n",
    "            tokens = self.tokenizer(input, 'russian')\n",
    "        return tokens\n",
    "    \n",
    "    def build_vocab(self, texts=None, save_vocab=True, load_vocab=False):\n",
    "        if load_vocab:\n",
    "            self.vocab = self.load_vocab(dir_path=self.vocab_cache_path['dir'], filename=self.vocab_cache_path['filename'])\n",
    "            if self.vocab is not None:\n",
    "                return\n",
    "        \n",
    "        print('Building vocab from texts...')\n",
    "        tokens = self._tokenize(texts)\n",
    "        \n",
    "        n_first = self.vocab_size - len(self.special_tokens)\n",
    "        \n",
    "        tokens = [token for token, _ in Counter(tokens).most_common(n_first)]\n",
    "        tokens = list(self.special_tokens.values()) + tokens\n",
    "                \n",
    "        self.vocab = Vocab(tokens, self.unk_token['id'])\n",
    "        print('Success')\n",
    "        \n",
    "        if save_vocab:\n",
    "            dir_path = self.vocab_cache_path['dir']\n",
    "            filename = self.vocab_cache_path['filename']\n",
    "            file_path = dir_path + '/' + filename\n",
    "            \n",
    "            print(f'Saving vocab at {file_path} ...')\n",
    "            self.save_vocab(dir_path=dir_path, filename=filename)\n",
    "    \n",
    "    def save_vocab(self, dir_path, filename):\n",
    "        try:\n",
    "            pathlib.Path(dir_path).mkdir(exist_ok=True)\n",
    "            file_path = dir_path + '/' + filename\n",
    "\n",
    "            with open(file_path, 'wb') as f:\n",
    "                pickle.dump(self.vocab, f)\n",
    "\n",
    "            print(f'Vocab is saved successfully at {file_path}')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'Failed to save vocab due to:\\n{e}')\n",
    "    \n",
    "    def load_vocab(self, dir_path, filename):\n",
    "        try:\n",
    "            file_path = dir_path + '/' + filename\n",
    "            \n",
    "            with open(file_path, 'rb') as f:\n",
    "                vocab = pickle.load(f)\n",
    "            \n",
    "            print(f'Vocab is loaded successfully from {file_path}')\n",
    "            \n",
    "            return vocab\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'Failed to load vocab due to:\\n{e}')\n",
    "            \n",
    "            return None\n",
    "        \n",
    "    def _pad_sequence(self, ids: List[int], max_seq_len) -> List[int]:\n",
    "        if len(ids) >= max_seq_len:\n",
    "            ids = ids[:max_seq_len]\n",
    "        else:\n",
    "            pad_len = max_seq_len - len(ids)\n",
    "            ids.extend(pad_len * [self.pad_token.get('id')])\n",
    "        \n",
    "        return ids\n",
    "    \n",
    "    def _add_special_tokens(self, ids: List[int], context=None) -> None:\n",
    "        ids.insert(0, self.sos_token.get('id'))\n",
    "        try:\n",
    "            eos_position = ids.index(self.pad_token.get('id'))\n",
    "        except ValueError:\n",
    "            eos_position = len(ids)\n",
    "        ids.insert(eos_position, self.eos_token.get('id'))\n",
    "        \n",
    "        if context is not None:\n",
    "            nsubj, gender, tense = context\n",
    "            \n",
    "            context_info = [\n",
    "                self.tense_tokens[tense].get('id'),\n",
    "                self.gender_tokens[gender].get('id'),\n",
    "                self.vocab.token_to_id(nsubj)\n",
    "            ]\n",
    "            \n",
    "            for item in context_info:\n",
    "                ids.insert(0, item)\n",
    "        \n",
    "    def encode(self, input: Union[str, List[str]], context=None, add_special_tokens=True, max_seq_len=None, return_tensor=False):\n",
    "        \"\"\"Input (Union[List[str], str]): a list of string sequences or a single sequence.\"\"\"\n",
    "        # context = (nsubj, gender, tense)\n",
    "        if type(input) is str:\n",
    "            tokens = self._tokenize(input)\n",
    "            ids = [self.vocab.token_to_id(token) for token in tokens]\n",
    "            \n",
    "            if max_seq_len is not None:\n",
    "                ids = self._pad_sequence(ids, max_seq_len)\n",
    "            \n",
    "            if add_special_tokens:\n",
    "                self._add_special_tokens(ids, context)\n",
    "            \n",
    "            if not return_tensor:\n",
    "                return ids\n",
    "            else:\n",
    "                return torch.tensor(ids)\n",
    "            \n",
    "        else:\n",
    "            tokenized_sents = [self._tokenize(sent) for sent in input]\n",
    "            sents_ids = [[self.vocab.token_to_id(token) for token in sent] for sent in tokenized_sents]\n",
    "            \n",
    "            max_seq_len = max(map(len, sents_ids)) if max_seq_len is None else max_seq_len\n",
    "            \n",
    "            padded_sequences = [self._pad_sequence(ids, max_seq_len) for ids in sents_ids]\n",
    "            padded_seq_and_context = zip(padded_sequences, context) if context is not None else None\n",
    "            \n",
    "            if add_special_tokens:\n",
    "                if padded_seq_and_context is not None:\n",
    "                    for ids, context in padded_seq_and_context:\n",
    "                        self._add_special_tokens(ids, context)\n",
    "                else:\n",
    "                    for ids in padded_sequences:\n",
    "                        self._add_special_tokens(ids)\n",
    "            \n",
    "            if not return_tensor:\n",
    "                return padded_sequences\n",
    "            else:\n",
    "                return torch.tensor(padded_sequences)\n",
    "            \n",
    "    def decode(self, encoded_seq: Union[List[int], torch.Tensor], remove_special_tokens=False, return_tokenized=True):\n",
    "        if type(encoded_seq) is list:\n",
    "            \n",
    "            if remove_special_tokens:\n",
    "                encoded_seq = encoded_seq[4:-1]\n",
    "            \n",
    "            decoded_seq = [self.vocab.id_to_token(id) for id in encoded_seq]\n",
    "            \n",
    "            if return_tokenized:\n",
    "                return decoded_seq\n",
    "            else:\n",
    "                return ' '.join(decoded_seq)\n",
    "        \n",
    "        else:\n",
    "            if len(encoded_seq.shape) > 1:\n",
    "                encoded_seq.squeeze_(0)\n",
    "                \n",
    "            if remove_special_tokens:\n",
    "                encoded_seq = encoded_seq[4:-1]\n",
    "            \n",
    "            decoded_seq = [self.vocab.id_to_token(id.item()) for id in encoded_seq]\n",
    "            \n",
    "            if return_tokenized:\n",
    "                return decoded_seq\n",
    "            else:\n",
    "                return ' '.join(decoded_seq)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "649b0f77-c913-4cfb-abe2-f25bb50cff3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 100000\n",
    "max_seq_len = None\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f03839d-c4c4-4e58-b1c6-cc5d8eb69f14",
   "metadata": {},
   "source": [
    "### Разбиение данных на обучающие, тестовые и валидационные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6445806b-5852-45de-8faf-f68a4ff2132f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = model_selection.train_test_split(df, train_size=0.9)\n",
    "test_df, val_df = model_selection.train_test_split(test_df, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e176ed0-02ae-46eb-9cf5-739472febefe",
   "metadata": {},
   "source": [
    "### Подготовка словаря"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d27d6a15-994c-4f6c-88f1-0bc8e665b402",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "593b3be1-ac02-4571-a306-f88b6462b171",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = train_df.lemm_texts.to_list() + train_df.orig_texts.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27bd9d94-61c7-4c1e-867e-853221e72492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab is loaded successfully from ./data/cached/vocab.pkl\n"
     ]
    }
   ],
   "source": [
    "tokenizer.build_vocab(load_vocab=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cce9a7-d49e-495e-8448-08b3898dc5d9",
   "metadata": {},
   "source": [
    "### Разбиение данных на батчи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1bc5c99-5ba5-4794-8c75-22cbb05b9ef3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_batched_dataset(df, max_seq_len=max_seq_len, tokenizer=tokenizer, batch_size=batch_size):\n",
    "    n_batches = len(df) // batch_size\n",
    "    \n",
    "    for n_batch in range(n_batches):\n",
    "        \n",
    "        orig   = df.orig_texts.to_list()[batch_size * n_batch:batch_size * (n_batch + 1)]\n",
    "        lemm   = df.lemm_texts.to_list()[batch_size * n_batch:batch_size * (n_batch + 1)]\n",
    "        nsubj  = df.nsubj.to_list()[batch_size * n_batch:batch_size * (n_batch + 1)]\n",
    "        gender = df.gender.to_list()[batch_size * n_batch:batch_size * (n_batch + 1)]\n",
    "        tense  = df.tense.to_list()[batch_size * n_batch:batch_size * (n_batch + 1)]\n",
    "        \n",
    "        context = list(zip(nsubj, gender, tense))\n",
    "        \n",
    "        encoded_input  = tokenizer.encode(lemm, context=context, add_special_tokens=True, max_seq_len=max_seq_len, return_tensor=True)\n",
    "        encoded_target = tokenizer.encode(orig, add_special_tokens=True, max_seq_len=max_seq_len, return_tensor=True)\n",
    "        \n",
    "        batch = (encoded_input.permute(1, 0), encoded_target.permute(1, 0))\n",
    "        \n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "deef4354-7d0f-4544-9e0b-954ae786c48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_n_batches = len(train_df) // batch_size\n",
    "val_n_batches = len(val_df) // batch_size\n",
    "test_n_batches = len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95bb279b-d31d-4636-8b59-cec9b39d9287",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = make_batched_dataset(train_df)\n",
    "val_data = make_batched_dataset(val_df)\n",
    "test_data = make_batched_dataset(test_df, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48dd5b26-8f9a-4c57-9dd2-85a69d02a7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_processed_data(train_data, val_data, test_data):\n",
    "    path = {\n",
    "        'dir': './data/cached',\n",
    "        'name': 'processed_data.pkl'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        pathlib.Path(path['dir']).mkdir(exist_ok=True)\n",
    "        file_path = path['dir'] + '/' + path['name']\n",
    "\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump((train_data, val_data, test_data), f)\n",
    "\n",
    "        print(f'Data is saved successfully at {file_path}')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Failed to save data due to:\\n{e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba42694e-6b07-4406-8e2c-d04a9d8a9c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_processed_data(path='./data/cached/processed_data.pkl'):\n",
    "    try:\n",
    "        with open(path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "\n",
    "        print(f'Data is loaded successfully from {path}')\n",
    "\n",
    "        return data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Failed to load data due to:\\n{e}')\n",
    "\n",
    "        return [None] * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b94794e-adbc-4e8f-ab39-ba1a2cd47cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is loaded successfully from ./data/cached/processed_data.pkl\n",
      "Data is saved successfully at ./data/cached/processed_data.pkl\n"
     ]
    }
   ],
   "source": [
    "load_data = True\n",
    "save_data = True\n",
    "\n",
    "if load_data:\n",
    "    train_data, val_data, test_data = load_processed_data()\n",
    "\n",
    "if not load_data or train_data is None:\n",
    "    train_data = [batch for batch in tqdm(make_batched_dataset(train_df), desc='Unpacking train batches', total=train_n_batches)]\n",
    "    val_data = [batch for batch in tqdm(make_batched_dataset(val_df), desc='Unpacking validation batches', total=val_n_batches)]\n",
    "    test_data = [batch for batch in tqdm(make_batched_dataset(test_df, batch_size=1), desc='Unpacking test batches', total=test_n_batches)]\n",
    "\n",
    "if save_data:\n",
    "    save_processed_data(train_data, val_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a064933c-35cb-4339-93b9-453058b2017b",
   "metadata": {},
   "source": [
    "## Определение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def4e99a-5a2c-413d-9fe4-109fae20d624",
   "metadata": {},
   "source": [
    "### Определение класса модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd302268-7e66-435c-93e3-cebdafc7f9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, embedding_size, nhead,\n",
    "                 num_encoder_layers, num_decoder_layers, \n",
    "                 dim_feedforward, dropout, vocab_size,\n",
    "                 max_seq_len, pad_token_id, device):\n",
    "        \n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        \n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        self.to(device)\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        self.word_embedding = nn.Embedding(vocab_size, embedding_size, pad_token_id)\n",
    "        self.input_pos_encoding = nn.Embedding(max_seq_len, embedding_size)\n",
    "        self.target_pos_encoding = nn.Embedding(max_seq_len, embedding_size)\n",
    "        \n",
    "        self.transformer = nn.Transformer(embedding_size, nhead, num_encoder_layers,\n",
    "                                          num_decoder_layers, dim_feedforward, dropout)\n",
    "        \n",
    "        self.fc_out = nn.Linear(embedding_size, vocab_size)\n",
    "        \n",
    "    def get_padding_mask(self, input):\n",
    "        # input shape: (seq_len, batch_size)\n",
    "        padding_mask = input.permute(1, 0) == self.pad_token_id\n",
    "        return padding_mask.to(self.device)\n",
    "    \n",
    "    def forward(self, input, target):\n",
    "        # input shape: (input_seq_len, batch_size)\n",
    "        # target shape: (target_seq_len, batch_size)\n",
    "    \n",
    "        embedded_input = self.word_embedding(input)\n",
    "        embedded_target = self.word_embedding(target)\n",
    "        # embedded_input shape: (input_seq_len, batch_size, embedding_size)\n",
    "        # embedded_target shape: (target_seq_len, batch_size, embedding_size)\n",
    "        \n",
    "        batch_size = input.shape[1]\n",
    "        \n",
    "        input_seq_len = input.shape[0]\n",
    "        target_seq_len = target.shape[0]\n",
    "    \n",
    "        input_positions = torch.arange(0, input_seq_len).unsqueeze(1).expand(input_seq_len, batch_size).to(self.device)\n",
    "        target_positions = torch.arange(0, target_seq_len).unsqueeze(1).expand(target_seq_len, batch_size).to(self.device)\n",
    "        # input_positions shape: (input_seq_len, batch_size)\n",
    "        # target_positions shape: (target_seq_len, batch_size)\n",
    "        \n",
    "        input_positions = self.input_pos_encoding(input_positions)\n",
    "        target_positions = self.target_pos_encoding(target_positions)\n",
    "        # input_positions shape: (input_seq_len, batch_size, embedding_size)\n",
    "        # target_positions shape: (target_seq_len, batch_size, embedding_size)\n",
    "        \n",
    "        embedded_input += input_positions\n",
    "        embedded_target += target_positions\n",
    "        \n",
    "        input_padding_mask = self.get_padding_mask(input)\n",
    "        # input_padding_mask shape: (batch_size, input_seq_len)\n",
    "        \n",
    "        target_mask = self.transformer.generate_square_subsequent_mask(target_seq_len).to(self.device)\n",
    "        # target_mask shape: (target_seq_len, target_seq_len)\n",
    "        \n",
    "        output = self.transformer(embedded_input, embedded_target,\n",
    "                                  tgt_mask=target_mask,\n",
    "                                  src_key_padding_mask=input_padding_mask)\n",
    "        # output shape: (target_seq_len, batch_size, embedding_size)\n",
    "        \n",
    "        output = self.fc_out(output)\n",
    "        # output shape: (target_seq_len, batch_size, vocab_size)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f444e6-837f-4bda-ba0f-0ca350ac9d6a",
   "metadata": {},
   "source": [
    "## Определение функций-утилит"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1237f0-267b-4b76-b02c-eb43c09c5fda",
   "metadata": {},
   "source": [
    "### Сохранение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42ef352d-2d48-4253-ae38-6bd6e66fa44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, epoch, val_loss, train_loss, path='./models/seq2seq_transformer.model'):\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'val_loss': val_loss,\n",
    "        'train_loss': train_loss\n",
    "    }\n",
    "    \n",
    "    torch.save(checkpoint, path)\n",
    "    print(f'\\n\\tModel saved successfully at {path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267bb350-d049-49ef-9e6c-aaeda118da03",
   "metadata": {},
   "source": [
    "### Загрузка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b36d6ed-f3c1-459c-b2d7-7d123d09be66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, optimizer=None, path='./model/seq2seq_transformer.model', device=torch.device('cpu')):\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    if optimizer is not None:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        \n",
    "    epoch      = checkpoint['epoch']\n",
    "    val_loss   = checkpoint['val_loss']\n",
    "    train_loss = checkpoint['train_loss']\n",
    "\n",
    "    return {'epoch': epoch, 'val_loss': val_loss, 'train_loss': train_loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee772c3-9c5d-418e-855d-4664a8648e29",
   "metadata": {},
   "source": [
    "## Место хранения модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3863bdb5-fa7f-4902-95a0-ecf2d434403a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = {\n",
    "    'dir': './models/',\n",
    "    'name': 'seq2seq_transformer.model'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d62843a-1792-4baf-a43d-c8fb375d3939",
   "metadata": {},
   "source": [
    "## Обучение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db7ee35-94fe-403c-af6c-a23114bb8335",
   "metadata": {},
   "source": [
    "### Определение параметров обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df74d24e-eff1-4f04-96b1-8c42c69d0381",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_params = {\n",
    "    'learning_rate': 1e-03,\n",
    "    'epochs': 10,\n",
    "    'max_norm': 1.0,\n",
    "    'patience': 3\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11598852-e1f4-44ef-ab75-62eca9d6f88b",
   "metadata": {},
   "source": [
    "### Определение параметров сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4be2183-3f5f-4c04-b828-373e545eb03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'embedding_size': 512,\n",
    "    'nhead': 8,\n",
    "    'num_encoder_layers': 6,\n",
    "    'num_decoder_layers': 6,\n",
    "    'dim_feedforward': 2048,\n",
    "    'dropout': 0.1,\n",
    "    'vocab_size': vocab_size,\n",
    "    'max_seq_len': 50,\n",
    "    'pad_token_id': tokenizer.pad_token['id'],\n",
    "    'device': torch.device('cuda')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841711eb-3ac5-499d-997a-1053ac80b226",
   "metadata": {},
   "source": [
    "### Инициализация модели, оптимизатора и функции потерь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b96307a-a84a-43f3-9013-d48396eb493a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2SeqTransformer(**params).to(params['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e23044da-4d9d-46ba-9870-9c298cbd2beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_params['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5b4de224-4a7e-41f9-9989-46331d41de01",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=params['pad_token_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b78a97c9-c080-4c72-ab09-da1bc969b3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_pretrained_model = False\n",
    "train_state = None\n",
    "\n",
    "if load_pretrained_model:\n",
    "    try:\n",
    "        train_state = load_model(\n",
    "            model, optimizer,\n",
    "            model_path['dir'] + model_path['name'],\n",
    "            params['device']\n",
    "        )\n",
    "        print(f\"Model loaded successfully from {model_path.get('dir') + model_path.get('name')}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'Load failed due to:\\n{e}')\n",
    "\n",
    "epoch = train_state['epoch'] if train_state is not None else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2d24b033-d3d2-422e-9bb7-13af77a36aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_writer = SummaryWriter('./runs/loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc781b75-6033-4132-87ee-0dce6bbfcc58",
   "metadata": {},
   "source": [
    "### Train-скрипт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8a49626b-708f-4afd-906f-7b952771afcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model, optimizer, criterion,\n",
    "    train_data, val_data, test_data,\n",
    "    epochs, max_norm, patience, current_epoch,\n",
    "    device, tokenizer, model_path, max_seq_len,\n",
    "    train_loss_writer, n_prints=10\n",
    "):\n",
    "    \n",
    "    min_mean_val_loss = float('+inf')\n",
    "    initial_patience = patience\n",
    "    print_every = len(train_data) // n_prints\n",
    "    \n",
    "    for epoch in tqdm(range(current_epoch, epochs), 'Epochs'):\n",
    "        print(f'\\nEpoch [{epoch} / {epochs}]')\n",
    "        \n",
    "        model.train()\n",
    "        for iteration, (input, target) in enumerate(tqdm(train_data, desc='Training iterations')):\n",
    "            input  = input.to(device)\n",
    "            target = target.to(device)\n",
    "            # input shape : (input_seq_len, batch_size)\n",
    "            # target shape: (target_seq_len, batch_size)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(input, target[:-1])\n",
    "            # output shape: (target_seq_len, batch_size, vocab_size)\n",
    "            \n",
    "            vocab_size = output.shape[2]\n",
    "            \n",
    "            output = output.reshape(-1, vocab_size)\n",
    "            # output shape: (target_seq_len * batch_size, vocab_size)\n",
    "            \n",
    "            target = target[1:].reshape(-1)\n",
    "            # target shape: (target_seq_len * batch_size)\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            \n",
    "            global_step = epoch * (len(train_data) + 1) + iteration\n",
    "            train_loss_writer.add_scalar('Training loss', loss, global_step=global_step)\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            if iteration % print_every == 0:\n",
    "                print(f'\\tIteration #{iteration}: training loss = {loss.item()}')\n",
    "            elif iteration == len(train_data):\n",
    "                print(f'\\tIteration #{iteration}: training loss = {loss.item()}')\n",
    "            \n",
    "            \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            \n",
    "            val_loss = []\n",
    "            \n",
    "            for iteration, (input, target) in enumerate(tqdm(val_data, desc='Validating iterations')):\n",
    "                input  = input.to(device)\n",
    "                target = target.to(device)\n",
    "                \n",
    "                output = model(input, target[:-1])\n",
    "                vocab_size = output.shape[2]\n",
    "                output = output.reshape(-1, vocab_size)\n",
    "                \n",
    "                target = target[1:].reshape(-1)\n",
    "                \n",
    "                local_val_loss = criterion(output, target)\n",
    "                val_loss.append(local_val_loss.item())\n",
    "                \n",
    "            mean_val_loss = sum(val_loss) / len(val_loss)\n",
    "            print(f'\\tValidation loss = {mean_val_loss}')\n",
    "            \n",
    "            if mean_val_loss < min_mean_val_loss:\n",
    "                try:\n",
    "                    save_model(model, optimizer, epoch, mean_val_loss, loss)\n",
    "                    min_mean_val_loss = mean_val_loss\n",
    "                    patience = initial_patience\n",
    "                except Exception as e:\n",
    "                    print(f'Model training stopped due to unhandled exception:\\n{e}')\n",
    "            else:\n",
    "                patience -= 1\n",
    "                \n",
    "            \n",
    "            test_sample = choice(test_data)\n",
    "            \n",
    "            predictions = [tokenizer.sos_token.get('id')]\n",
    "            for i in range(max_seq_len):\n",
    "                target = torch.tensor(predictions, device=device).unsqueeze(1)\n",
    "                \n",
    "                output = model(test_sample[0].to(device), target)\n",
    "                best_prediction = output.argmax(2)[-1].item()\n",
    "                predictions.append(best_prediction)\n",
    "                \n",
    "                if best_prediction == tokenizer.eos_token.get('id'):\n",
    "                    break\n",
    "            \n",
    "            decoded_output = tokenizer.decode(predictions,    return_tokenized=False)\n",
    "            decoded_input  = tokenizer.decode(test_sample[0], return_tokenized=False)\n",
    "            decoded_target = tokenizer.decode(test_sample[1], return_tokenized=False)\n",
    "            \n",
    "            print(f'\\tInput : {decoded_input}')\n",
    "            print(f'\\tOutput: {decoded_output}')\n",
    "            print(f'\\tTarget: {decoded_target}')\n",
    "            \n",
    "        if patience == 0:\n",
    "            print(f'\\nModel learning finished due to early stopping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0d0d3549-7aed-485d-8b49-d742ef811f7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32ff402eed05489793dfcd9fe5b5657a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [0 / 10]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6837ba13aace4d8ab7fbf83d19643da0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training iterations:   0%|          | 0/5625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tIteration #0: training loss = 11.731514930725098\n",
      "\tIteration #562: training loss = 8.86849308013916\n",
      "\tIteration #1124: training loss = 8.179290771484375\n",
      "\tIteration #1686: training loss = 8.475604057312012\n",
      "\tIteration #2248: training loss = 8.275514602661133\n",
      "\tIteration #2810: training loss = 8.345961570739746\n",
      "\tIteration #3372: training loss = 7.761917591094971\n",
      "\tIteration #3934: training loss = 7.775726318359375\n",
      "\tIteration #4496: training loss = 7.59976053237915\n",
      "\tIteration #5058: training loss = 7.839240550994873\n",
      "\tIteration #5620: training loss = 7.198651313781738\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b76f6de500343bda6d164f5a4b5f86b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating iterations:   0%|          | 0/312 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValidation loss = 7.380691569585067\n",
      "\n",
      "\tModel saved successfully at ./models/seq2seq_transformer.model\n",
      "\tInput : ракеты-носителя <fem> <fut> <sos> два ракета-носитель `` союз '' , который в настоящий время готовиться к запуск с космодром кура в французский гвиана , потребовать замена двигатель третий ступень . <eos>\n",
      "\tOutput: <sos> , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\tTarget: <sos> две ракеты-носителя `` союз '' , которые в настоящее время готовятся к запуску с космодрома <unk> во французской <unk> , потребуют замены двигателей третьих <unk> . <eos>\n",
      "\n",
      "Epoch [1 / 10]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "085018240b2441b08ba941287411332b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training iterations:   0%|          | 0/5625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tIteration #0: training loss = 8.152949333190918\n",
      "\tIteration #562: training loss = 7.9065446853637695\n",
      "\tIteration #1124: training loss = 7.742770671844482\n",
      "\tIteration #1686: training loss = 7.861391067504883\n",
      "\tIteration #2248: training loss = 7.821498394012451\n",
      "\tIteration #2810: training loss = 7.954856872558594\n",
      "\tIteration #3372: training loss = 7.529905796051025\n",
      "\tIteration #3934: training loss = 7.576259136199951\n",
      "\tIteration #4496: training loss = 7.493650436401367\n",
      "\tIteration #5058: training loss = 7.678737163543701\n",
      "\tIteration #5620: training loss = 7.119010925292969\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b471df6ca0424974a9a0c11c2c2f5608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating iterations:   0%|          | 0/312 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValidation loss = 7.412302384009728\n",
      "\tInput : водитель <masc> <past> <sos> в алма-ата водитель иномарка не справиться с управление и врезаться в стойка билборд с надпись `` алкоголь выносить мозг '' . <eos>\n",
      "\tOutput: <sos> , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\tTarget: <sos> в алма-ате водитель иномарки не справился с управлением и врезался в стойку <unk> с надписью `` алкоголь выносит мозг '' . <eos>\n",
      "\n",
      "Epoch [2 / 10]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26cdf46d5a9c4f37b43275fd8899301a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training iterations:   0%|          | 0/5625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tIteration #0: training loss = 8.158324241638184\n",
      "\tIteration #562: training loss = 7.682116985321045\n",
      "\tIteration #1124: training loss = 7.636659622192383\n",
      "\tIteration #1686: training loss = 7.760990619659424\n",
      "\tIteration #2248: training loss = 7.660263538360596\n",
      "\tIteration #2810: training loss = 7.795551300048828\n",
      "\tIteration #3372: training loss = 7.416667461395264\n",
      "\tIteration #3934: training loss = 7.444700241088867\n",
      "\tIteration #4496: training loss = 7.3356709480285645\n",
      "\tIteration #5058: training loss = 7.52575159072876\n",
      "\tIteration #5620: training loss = 7.024961948394775\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9983c04a9a934772b7764eedf6fa861a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating iterations:   0%|          | 0/312 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValidation loss = 7.518873295722863\n",
      "\tInput : он <masc> <past> <sos> за полугодие он уменьшиться на 2,5 процентный пункт . <eos>\n",
      "\tOutput: <sos> , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\tTarget: <sos> за полугодие он уменьшился на 2,5 процентного пункта . <eos>\n",
      "\n",
      "Epoch [3 / 10]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71150ebcb5ba4f4aa08545345883de1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training iterations:   0%|          | 0/5625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tIteration #0: training loss = 8.004989624023438\n",
      "\tIteration #562: training loss = 7.636898517608643\n",
      "\tIteration #1124: training loss = 7.837103843688965\n",
      "\tIteration #1686: training loss = 7.7204813957214355\n",
      "\tIteration #2248: training loss = 7.560414791107178\n",
      "\tIteration #2810: training loss = 7.654486656188965\n",
      "\tIteration #3372: training loss = 7.342111587524414\n",
      "\tIteration #3934: training loss = 7.346912384033203\n",
      "\tIteration #4496: training loss = 7.246090412139893\n",
      "\tIteration #5058: training loss = 7.455216407775879\n",
      "\tIteration #5620: training loss = 6.976412296295166\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5348551b8bba4750adba10154e43bd55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating iterations:   0%|          | 0/312 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValidation loss = 7.609918635625106\n",
      "\tInput : встреча <fem> <past> <sos> как сообщаться на официальный сайт лига , встреча закончиться разгром `` лос-анджелес '' с счёт 7:0 . <eos>\n",
      "\tOutput: <sos> , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\tTarget: <sos> как сообщается на официальном сайте лиги , встреча закончилась <unk> `` лос-анджелеса '' со счетом 7:0 . <eos>\n",
      "\n",
      "Model learning finished due to early stopping\n",
      "\n",
      "Epoch [4 / 10]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d7bb9b10fa04f32adfcf7bbe422b318",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training iterations:   0%|          | 0/5625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tIteration #0: training loss = 7.908359527587891\n",
      "\tIteration #562: training loss = 7.618069648742676\n",
      "\tIteration #1124: training loss = 7.673225402832031\n",
      "\tIteration #1686: training loss = 7.685710430145264\n",
      "\tIteration #2248: training loss = 7.537512302398682\n",
      "\tIteration #2810: training loss = 7.876941680908203\n",
      "\tIteration #3372: training loss = 7.318387985229492\n",
      "\tIteration #3934: training loss = 7.424988269805908\n",
      "\tIteration #4496: training loss = 7.355624198913574\n",
      "\tIteration #5058: training loss = 7.512282848358154\n",
      "\tIteration #5620: training loss = 6.975497722625732\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0586a2ef05a4be897dcfd04c30bde27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating iterations:   0%|          | 0/312 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValidation loss = 7.535137223891723\n",
      "\tInput : <unk> <masc> <past> <sos> тем не менее , <unk> - талантливый <unk> - высказать свой опасение по повод дальнейший карьера в это вид спорт . <eos>\n",
      "\tOutput: <sos> , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\tTarget: <sos> тем не менее , <unk> - талантливый <unk> - высказал свои опасения по поводу дальнейшей карьеры в этом виде спорта . <eos>\n",
      "\n",
      "Epoch [5 / 10]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0700aca227d440a18d925e9ce894cd33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training iterations:   0%|          | 0/5625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tIteration #0: training loss = 8.067510604858398\n",
      "\tIteration #562: training loss = 7.654517650604248\n",
      "\tIteration #1124: training loss = 7.587922096252441\n",
      "\tIteration #1686: training loss = 7.665658473968506\n",
      "\tIteration #2248: training loss = 7.566557884216309\n",
      "\tIteration #2810: training loss = 7.687728404998779\n",
      "\tIteration #3372: training loss = 7.328155994415283\n",
      "\tIteration #3934: training loss = 7.421534061431885\n",
      "\tIteration #4496: training loss = 7.293044090270996\n",
      "\tIteration #5058: training loss = 7.455260276794434\n",
      "\tIteration #5620: training loss = 6.948154926300049\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c5b872f4971446ca867f1d10d0ef799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating iterations:   0%|          | 0/312 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValidation loss = 7.559796134630839\n",
      "\tInput : делегация <fem> <past> <sos> делегация `` реал '' прилететь в москва 19 февраль , то есть за два день до матч , чтобы приспособиться к холодный погода . <eos>\n",
      "\tOutput: <sos> , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\tTarget: <sos> делегация `` реала '' прилетела в москву 19 февраля , то есть за два дня до матча , чтобы приспособиться к холодной погоде . <eos>\n",
      "\n",
      "Epoch [6 / 10]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd621c4b0c614329a618f0d08b8f2e5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training iterations:   0%|          | 0/5625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tIteration #0: training loss = 7.979887008666992\n",
      "\tIteration #562: training loss = 7.850822925567627\n",
      "\tIteration #1124: training loss = 7.756778717041016\n",
      "\tIteration #1686: training loss = 7.604801654815674\n",
      "\tIteration #2248: training loss = 7.521182537078857\n",
      "\tIteration #2810: training loss = 7.7130889892578125\n",
      "\tIteration #3372: training loss = 7.320453643798828\n",
      "\tIteration #3934: training loss = 7.388503074645996\n",
      "\tIteration #4496: training loss = 7.258874416351318\n",
      "\tIteration #5058: training loss = 7.454766750335693\n",
      "\tIteration #5620: training loss = 6.940962791442871\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcc163afc2b14123923b143ed9efde04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating iterations:   0%|          | 0/312 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValidation loss = 7.564279284232702\n",
      "\tInput : дума <fem> <past> <sos> государственный дума принять в первый чтение поправка о ограничение « золотой парашют » в госкомпания . <eos>\n",
      "\tOutput: <sos> , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\tTarget: <sos> государственная дума приняла в первом чтении поправки об ограничении « золотых <unk> » в <unk> . <eos>\n",
      "\n",
      "Epoch [7 / 10]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8afb0da849194d7588b60e96b2ff0d49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training iterations:   0%|          | 0/5625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tIteration #0: training loss = 7.9565019607543945\n",
      "\tIteration #562: training loss = 7.659062385559082\n",
      "\tIteration #1124: training loss = 7.723927974700928\n",
      "\tIteration #1686: training loss = 7.557610988616943\n",
      "\tIteration #2248: training loss = 7.65216588973999\n",
      "\tIteration #2810: training loss = 7.607192516326904\n",
      "\tIteration #3372: training loss = 7.271047592163086\n",
      "\tIteration #3934: training loss = 7.389272212982178\n",
      "\tIteration #4496: training loss = 7.248394966125488\n",
      "\tIteration #5058: training loss = 7.422877311706543\n",
      "\tIteration #5620: training loss = 6.964548587799072\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12310802f0c04626ac34d14982fe762b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating iterations:   0%|          | 0/312 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValidation loss = 7.574142634868622\n",
      "\tInput : участники <masc> <fut> <sos> участник шествие пройти по большой якиманка пройти до болотный площадь , где в 17:30 должный состояться митинг . <eos>\n",
      "\tOutput: <sos> , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\tTarget: <sos> участники шествия пройдут по большой <unk> пройдут до болотной площади , где в 17:30 должен состояться митинг . <eos>\n",
      "\n",
      "Epoch [8 / 10]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abf7b9e6e942429e8ebf0dc02ff1f047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training iterations:   0%|          | 0/5625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tIteration #0: training loss = 7.966848850250244\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-e51e219a56c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m train(\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mlearning_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'epochs'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'max_norm'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mlearning_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'patience'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'device'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-26b1a7aaa1e3>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, criterion, train_data, val_data, test_data, epochs, max_norm, patience, current_epoch, device, tokenizer, model_path, max_seq_len, train_loss_writer, n_prints)\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[1;31m# target shape: (target_seq_len, batch_size)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\ml_env\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[1;34m(self, set_to_none)\u001b[0m\n\u001b[0;32m    214\u001b[0m                             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m                                 \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m                             \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(\n",
    "    model, optimizer, criterion,\n",
    "    train_data, val_data, test_data,\n",
    "    learning_params['epochs'], learning_params['max_norm'],\n",
    "    learning_params['patience'], epoch, params['device'], tokenizer,\n",
    "    model_path['dir'] + model_path['name'], params['max_seq_len'],\n",
    "    train_loss_writer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733e205e-fc80-4462-b9d8-0a2e8e7376b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
