{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2fb41fd-c50f-4033-9cbf-273df7637194",
   "metadata": {},
   "source": [
    "## Импорт необходимых зависимостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "973dc9dc-9e2e-4d2d-8c9d-98fa61b38d55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import pathlib\n",
    "\n",
    "from pprint import pprint\n",
    "from random import choice\n",
    "from typing import List, Union\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from functools import reduce\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn import model_selection\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a8e1272-bf97-4890-945c-6615ba132abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt', quiet=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6e4800-d29f-4ac2-88a3-e5ee010d9901",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91d515aa-7400-443e-9e9a-5c3c34d3ef65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RANDOM_STATE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c2c1321-83cd-4e5d-afaa-9ae0992153b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/lenta/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c448591d-3ea9-44d1-bfc0-26048460a7b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orig_texts</th>\n",
       "      <th>lemm_texts</th>\n",
       "      <th>nsubj</th>\n",
       "      <th>gender</th>\n",
       "      <th>tense</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>вице-премьер по социальным вопросам татьяна го...</td>\n",
       "      <td>вице-премьер по социальный вопрос татьяна голи...</td>\n",
       "      <td>вице-премьер</td>\n",
       "      <td>masc</td>\n",
       "      <td>past</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>по словам голиковой, чаще всего онкологические...</td>\n",
       "      <td>по слово голиков, частый весь онкологический з...</td>\n",
       "      <td>заболевания</td>\n",
       "      <td>neut</td>\n",
       "      <td>past</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>вице-премьер напомнила, что главные факторы см...</td>\n",
       "      <td>вице-премьер напомнить, что главный фактор сме...</td>\n",
       "      <td>вице-премьер</td>\n",
       "      <td>masc</td>\n",
       "      <td>past</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>австрийские правоохранительные органы не предс...</td>\n",
       "      <td>австрийский правоохранительный орган не предст...</td>\n",
       "      <td>органы</td>\n",
       "      <td>masc</td>\n",
       "      <td>past</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>об этом сообщил посол россии в вене дмитрий лю...</td>\n",
       "      <td>о это сообщить посол россия в вена дмитрий люб...</td>\n",
       "      <td>посол</td>\n",
       "      <td>masc</td>\n",
       "      <td>past</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1852718</th>\n",
       "      <td>как сообщили \"интерфаксу\" во временном пресс-ц...</td>\n",
       "      <td>как сообщить \"интерфакс\"в временной пресс-цент...</td>\n",
       "      <td>они</td>\n",
       "      <td>undefined</td>\n",
       "      <td>pres</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1852719</th>\n",
       "      <td>всего, по имеющимся в пресс-центре сведениям, ...</td>\n",
       "      <td>весь, по имеющийся в пресс-центр сведение, гру...</td>\n",
       "      <td>группировка</td>\n",
       "      <td>fem</td>\n",
       "      <td>pres</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1852720</th>\n",
       "      <td>намеченная на сегодняшний день церемония вступ...</td>\n",
       "      <td>намеченный на сегодняшний день церемония вступ...</td>\n",
       "      <td>церемония</td>\n",
       "      <td>fem</td>\n",
       "      <td>fut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1852721</th>\n",
       "      <td>владимир семенов находится в москве, где вмест...</td>\n",
       "      <td>владимир семён находиться в москва, где вместе...</td>\n",
       "      <td>владимир</td>\n",
       "      <td>masc</td>\n",
       "      <td>pres</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1852722</th>\n",
       "      <td>на состоявшейся сегодня в москве пресс-конфере...</td>\n",
       "      <td>на состоявшийся сегодня в москва пресс-конфере...</td>\n",
       "      <td>шеф</td>\n",
       "      <td>masc</td>\n",
       "      <td>past</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1852723 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                orig_texts  \\\n",
       "0        вице-премьер по социальным вопросам татьяна го...   \n",
       "1        по словам голиковой, чаще всего онкологические...   \n",
       "2        вице-премьер напомнила, что главные факторы см...   \n",
       "3        австрийские правоохранительные органы не предс...   \n",
       "4        об этом сообщил посол россии в вене дмитрий лю...   \n",
       "...                                                    ...   \n",
       "1852718  как сообщили \"интерфаксу\" во временном пресс-ц...   \n",
       "1852719  всего, по имеющимся в пресс-центре сведениям, ...   \n",
       "1852720  намеченная на сегодняшний день церемония вступ...   \n",
       "1852721  владимир семенов находится в москве, где вмест...   \n",
       "1852722  на состоявшейся сегодня в москве пресс-конфере...   \n",
       "\n",
       "                                                lemm_texts         nsubj  \\\n",
       "0        вице-премьер по социальный вопрос татьяна голи...  вице-премьер   \n",
       "1        по слово голиков, частый весь онкологический з...   заболевания   \n",
       "2        вице-премьер напомнить, что главный фактор сме...  вице-премьер   \n",
       "3        австрийский правоохранительный орган не предст...        органы   \n",
       "4        о это сообщить посол россия в вена дмитрий люб...         посол   \n",
       "...                                                    ...           ...   \n",
       "1852718  как сообщить \"интерфакс\"в временной пресс-цент...           они   \n",
       "1852719  весь, по имеющийся в пресс-центр сведение, гру...   группировка   \n",
       "1852720  намеченный на сегодняшний день церемония вступ...     церемония   \n",
       "1852721  владимир семён находиться в москва, где вместе...      владимир   \n",
       "1852722  на состоявшийся сегодня в москва пресс-конфере...           шеф   \n",
       "\n",
       "            gender tense  \n",
       "0             masc  past  \n",
       "1             neut  past  \n",
       "2             masc  past  \n",
       "3             masc  past  \n",
       "4             masc  past  \n",
       "...            ...   ...  \n",
       "1852718  undefined  pres  \n",
       "1852719        fem  pres  \n",
       "1852720        fem   fut  \n",
       "1852721       masc  pres  \n",
       "1852722       masc  past  \n",
       "\n",
       "[1852723 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = df.sample(n=50000, random_state=RANDOM_STATE)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c8d7d0-c3b0-4d3a-b3a0-a9c513f0a39b",
   "metadata": {},
   "source": [
    "### Определение классов словаря и подготовщика данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "761fae45-0202-4290-926e-5c52ef489210",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, tokens=None, unk_id=None):\n",
    "        self.unk_id = unk_id\n",
    "        \n",
    "        self.tokens = tokens\n",
    "        self.tokens_to_ids = {token: id for id, token in enumerate(tokens)} if tokens is not None else None\n",
    "    \n",
    "    def id_to_token(self, id):\n",
    "        return self.tokens[id]\n",
    "    \n",
    "    def token_to_id(self, token):\n",
    "        return self.tokens_to_ids.get(token, self.unk_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d61e34b1-1eb1-4dfd-a2fe-d359eaf0308e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, vocab_size):\n",
    "        \n",
    "        self.tokenizer = nltk.tokenize.word_tokenize\n",
    "        \n",
    "        self.special_tokens = {\n",
    "            'pad_token'        : '<pad>',\n",
    "            'unk_token'        : '<unk>',\n",
    "            'sos_token'        : '<sos>',\n",
    "            'eos_token'        : '<eos>',\n",
    "            'g_masc_token'     : '<masc>',\n",
    "            'g_fem_token'      : '<fem>',\n",
    "            'g_neut_token'     : '<neut>',\n",
    "            'g_undefined_token': '<undef>',\n",
    "            't_past_token'     : '<past>',\n",
    "            't_pres_token'     : '<pres>',\n",
    "            't_fut_token'      : '<fut>'\n",
    "        }\n",
    "        \n",
    "        self.special_ids = {token: id for id, token in enumerate(self.special_tokens.keys())}\n",
    "        \n",
    "        self.pad_token = {'id'   : self.special_ids['pad_token'],\n",
    "                          'token': self.special_tokens['pad_token']}\n",
    "        \n",
    "        self.unk_token = {'id'   : self.special_ids['unk_token'],\n",
    "                          'token': self.special_tokens['unk_token']}\n",
    "        \n",
    "        self.sos_token = {'id'   : self.special_ids['sos_token'],\n",
    "                          'token': self.special_tokens['sos_token']}\n",
    "        \n",
    "        self.eos_token = {'id'   : self.special_ids['eos_token'],\n",
    "                          'token': self.special_tokens['eos_token']}\n",
    "        \n",
    "        self.gender_tokens = {\n",
    "            'masc'     : {'id'   : self.special_ids['g_masc_token'],\n",
    "                          'token': self.special_tokens['g_masc_token']},\n",
    "            \n",
    "            'fem'      : {'id'   : self.special_ids['g_fem_token'],\n",
    "                          'token': self.special_tokens['g_fem_token']},\n",
    "            \n",
    "            'neut'     : {'id'   : self.special_ids['g_neut_token'],\n",
    "                          'token': self.special_tokens['g_neut_token']},\n",
    "            \n",
    "            'undefined': {'id'   : self.special_ids['g_undefined_token'],\n",
    "                          'token': self.special_tokens['g_undefined_token']}\n",
    "        }\n",
    "        \n",
    "        self.tense_tokens = {\n",
    "            'past': {'id'   : self.special_ids['t_past_token'],\n",
    "                     'token': self.special_tokens['t_past_token']},\n",
    "            \n",
    "            'pres': {'id'   : self.special_ids['t_pres_token'],\n",
    "                     'token': self.special_tokens['t_pres_token']},\n",
    "            \n",
    "            'fut' : {'id'   : self.special_ids['t_fut_token'],\n",
    "                     'token': self.special_tokens['t_fut_token']}\n",
    "        }\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.vocab_cache_path = {\n",
    "            'dir': './data/cached',\n",
    "            'filename': 'vocab.pkl'\n",
    "        }\n",
    "        \n",
    "        self.vocab = None\n",
    "        \n",
    "    def _tokenize(self, input: Union[List[str], str]):\n",
    "        \"\"\"Input (Union[List[str], str]): a list of string sequences or a single sequence.\"\"\"\n",
    "        \n",
    "        if type(input) is list:\n",
    "            tokens = list(chain(*[self.tokenizer(text, 'russian') for text in tqdm(input, 'Tokenizing texts')]))\n",
    "        else:\n",
    "            tokens = self.tokenizer(input, 'russian')\n",
    "        return tokens\n",
    "    \n",
    "    def build_vocab(self, texts=None, save_vocab=True, load_vocab=False):\n",
    "        if load_vocab:\n",
    "            self.vocab = self.load_vocab(dir_path=self.vocab_cache_path['dir'], filename=self.vocab_cache_path['filename'])\n",
    "            if self.vocab is not None:\n",
    "                return\n",
    "        \n",
    "        print('Building vocab from texts...')\n",
    "        tokens = self._tokenize(texts)\n",
    "        \n",
    "        n_first = self.vocab_size - len(self.special_tokens)\n",
    "        \n",
    "        tokens = [token for token, _ in Counter(tokens).most_common(n_first)]\n",
    "        tokens = list(self.special_tokens.values()) + tokens\n",
    "                \n",
    "        self.vocab = Vocab(tokens, self.unk_token['id'])\n",
    "        print('Success')\n",
    "        \n",
    "        if save_vocab:\n",
    "            dir_path = self.vocab_cache_path['dir']\n",
    "            filename = self.vocab_cache_path['filename']\n",
    "            file_path = dir_path + '/' + filename\n",
    "            \n",
    "            print(f'Saving vocab at {file_path} ...')\n",
    "            self.save_vocab(dir_path=dir_path, filename=filename)\n",
    "    \n",
    "    def save_vocab(self, dir_path, filename):\n",
    "        try:\n",
    "            pathlib.Path(dir_path).mkdir(exist_ok=True)\n",
    "            file_path = dir_path + '/' + filename\n",
    "\n",
    "            with open(file_path, 'wb') as f:\n",
    "                pickle.dump(self.vocab, f)\n",
    "\n",
    "            print(f'Vocab is saved successfully at {file_path}')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'Failed to save vocab due to:\\n{e}')\n",
    "    \n",
    "    def load_vocab(self, dir_path, filename):\n",
    "        try:\n",
    "            file_path = dir_path + '/' + filename\n",
    "            \n",
    "            with open(file_path, 'rb') as f:\n",
    "                vocab = pickle.load(f)\n",
    "            \n",
    "            print(f'Vocab is loaded successfully from {file_path}')\n",
    "            \n",
    "            return vocab\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'Failed to load vocab due to:\\n{e}')\n",
    "            \n",
    "            return None\n",
    "        \n",
    "    def _pad_sequence(self, ids: List[int], max_seq_len) -> List[int]:\n",
    "        if len(ids) >= max_seq_len:\n",
    "            ids = ids[:max_seq_len]\n",
    "        else:\n",
    "            pad_len = max_seq_len - len(ids)\n",
    "            ids.extend(pad_len * [self.pad_token.get('id')])\n",
    "        \n",
    "        return ids\n",
    "    \n",
    "    def _add_special_tokens(self, ids: List[int], context=None) -> None:\n",
    "        ids.insert(0, self.sos_token.get('id'))\n",
    "        try:\n",
    "            eos_position = ids.index(self.pad_token.get('id'))\n",
    "        except ValueError:\n",
    "            eos_position = len(ids)\n",
    "        ids.insert(eos_position, self.eos_token.get('id'))\n",
    "        \n",
    "        if context is not None:\n",
    "            nsubj, gender, tense = context\n",
    "            \n",
    "            context_info = [\n",
    "                self.tense_tokens[tense].get('id'),\n",
    "                self.gender_tokens[gender].get('id'),\n",
    "                self.vocab.token_to_id(nsubj)\n",
    "            ]\n",
    "            \n",
    "            for item in context_info:\n",
    "                ids.insert(0, item)\n",
    "        \n",
    "    def encode(self, input: Union[str, List[str]], context=None, add_special_tokens=True, max_seq_len=None, return_tensor=False):\n",
    "        \"\"\"Input (Union[List[str], str]): a list of string sequences or a single sequence.\"\"\"\n",
    "        # context = (nsubj, gender, tense)\n",
    "        if type(input) is str:\n",
    "            tokens = self._tokenize(input)\n",
    "            ids = [self.vocab.token_to_id(token) for token in tokens]\n",
    "            \n",
    "            if max_seq_len is not None:\n",
    "                ids = self._pad_sequence(ids, max_seq_len)\n",
    "            \n",
    "            if add_special_tokens:\n",
    "                self._add_special_tokens(ids, context)\n",
    "            \n",
    "            if not return_tensor:\n",
    "                return ids\n",
    "            else:\n",
    "                return torch.tensor(ids)\n",
    "            \n",
    "        else:\n",
    "            tokenized_sents = [self._tokenize(sent) for sent in input]\n",
    "            sents_ids = [[self.vocab.token_to_id(token) for token in sent] for sent in tokenized_sents]\n",
    "            \n",
    "            max_seq_len = max(map(len, sents_ids)) if max_seq_len is None else max_seq_len\n",
    "            \n",
    "            padded_sequences = [self._pad_sequence(ids, max_seq_len) for ids in sents_ids]\n",
    "            padded_seq_and_context = zip(padded_sequences, context) if context is not None else None\n",
    "            \n",
    "            if add_special_tokens:\n",
    "                if padded_seq_and_context is not None:\n",
    "                    for ids, context in padded_seq_and_context:\n",
    "                        self._add_special_tokens(ids, context)\n",
    "                else:\n",
    "                    for ids in padded_sequences:\n",
    "                        self._add_special_tokens(ids)\n",
    "                        ids.extend(3 * [self.pad_token.get('id')])\n",
    "            \n",
    "            if not return_tensor:\n",
    "                return padded_sequences\n",
    "            else:\n",
    "                return torch.tensor(padded_sequences)\n",
    "            \n",
    "    def decode(self, encoded_seq: Union[List[int], torch.Tensor], remove_special_tokens=False, return_tokenized=True):\n",
    "        if type(encoded_seq) is list:\n",
    "            \n",
    "            if remove_special_tokens:\n",
    "                encoded_seq = encoded_seq[4:-1]\n",
    "            \n",
    "            decoded_seq = [self.vocab.id_to_token(id) for id in encoded_seq]\n",
    "            \n",
    "            if return_tokenized:\n",
    "                return decoded_seq\n",
    "            else:\n",
    "                return ' '.join(decoded_seq)\n",
    "        \n",
    "        else:\n",
    "            if len(encoded_seq.shape) > 1:\n",
    "                encoded_seq.unsqueeze_(0)\n",
    "                \n",
    "            if remove_special_tokens:\n",
    "                encoded_seq = encoded_seq[4:-1]\n",
    "            \n",
    "            decoded_seq = [self.vocab.id_to_token(id.item()) for id in encoded_seq]\n",
    "            \n",
    "            if return_tokenized:\n",
    "                return decoded_seq\n",
    "            else:\n",
    "                return ' '.join(decoded_seq)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "649b0f77-c913-4cfb-abe2-f25bb50cff3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 100000\n",
    "max_seq_len = 50\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f03839d-c4c4-4e58-b1c6-cc5d8eb69f14",
   "metadata": {},
   "source": [
    "### Разбиение данных на обучающие, тестовые и валидационные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6445806b-5852-45de-8faf-f68a4ff2132f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = model_selection.train_test_split(df, train_size=0.9)\n",
    "test_df, val_df = model_selection.train_test_split(test_df, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e176ed0-02ae-46eb-9cf5-739472febefe",
   "metadata": {},
   "source": [
    "### Подготовка словаря"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d27d6a15-994c-4f6c-88f1-0bc8e665b402",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "593b3be1-ac02-4571-a306-f88b6462b171",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = train_df.lemm_texts.to_list() + train_df.orig_texts.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27bd9d94-61c7-4c1e-867e-853221e72492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab is loaded successfully from ./data/cached/vocab.pkl\n"
     ]
    }
   ],
   "source": [
    "tokenizer.build_vocab(load_vocab=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cce9a7-d49e-495e-8448-08b3898dc5d9",
   "metadata": {},
   "source": [
    "### Разбиение данных на батчи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1bc5c99-5ba5-4794-8c75-22cbb05b9ef3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_batched_dataset(df, max_seq_len=max_seq_len, tokenizer=tokenizer, batch_size=batch_size):\n",
    "    n_batches = len(df) // batch_size\n",
    "    \n",
    "    for n_batch in range(n_batches):\n",
    "        \n",
    "        orig   = df.orig_texts.to_list()[batch_size * n_batch:batch_size * (n_batch + 1)]\n",
    "        lemm   = df.lemm_texts.to_list()[batch_size * n_batch:batch_size * (n_batch + 1)]\n",
    "        nsubj  = df.nsubj.to_list()[batch_size * n_batch:batch_size * (n_batch + 1)]\n",
    "        gender = df.gender.to_list()[batch_size * n_batch:batch_size * (n_batch + 1)]\n",
    "        tense  = df.tense.to_list()[batch_size * n_batch:batch_size * (n_batch + 1)]\n",
    "        \n",
    "        encoded_input  = tokenizer.encode(lemm, context=list(zip(nsubj, gender, tense)), max_seq_len=max_seq_len, return_tensor=True)\n",
    "        encoded_target = tokenizer.encode(orig, max_seq_len=max_seq_len, return_tensor=True)\n",
    "        \n",
    "        batch = (encoded_input, encoded_target)\n",
    "        \n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "deef4354-7d0f-4544-9e0b-954ae786c48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_n_batches = len(train_df) // batch_size\n",
    "val_n_batches = len(val_df) // batch_size\n",
    "test_n_batches = len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95bb279b-d31d-4636-8b59-cec9b39d9287",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = make_batched_dataset(train_df)\n",
    "val_data = make_batched_dataset(val_df)\n",
    "test_data = make_batched_dataset(test_df, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c1080b2-08ee-446f-bda7-80de20b4e6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = []\n",
    "for i, sample in enumerate(test_data):\n",
    "    test_samples.append(sample)\n",
    "    if i == 1000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "48dd5b26-8f9a-4c57-9dd2-85a69d02a7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_processed_data(train_data, val_data, test_data):\n",
    "#     path = {\n",
    "#         'dir': './data/cached',\n",
    "#         'name': 'processed_data.pkl'\n",
    "#     }\n",
    "    \n",
    "#     try:\n",
    "#         pathlib.Path(path['dir']).mkdir(exist_ok=True)\n",
    "#         file_path = path['dir'] + '/' + path['name']\n",
    "\n",
    "#         with open(file_path, 'wb') as f:\n",
    "#             pickle.dump((train_data, val_data, test_data), f)\n",
    "\n",
    "#         print(f'Data is saved successfully at {file_path}')\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f'Failed to save data due to:\\n{e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ba42694e-6b07-4406-8e2c-d04a9d8a9c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_processed_data(path='./data/cached/processed_data.pkl'):\n",
    "#     try:\n",
    "#         with open(path, 'rb') as f:\n",
    "#             data = pickle.load(f)\n",
    "\n",
    "#         print(f'Data is loaded successfully from {path}')\n",
    "\n",
    "#         return data\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f'Failed to load data due to:\\n{e}')\n",
    "\n",
    "#         return [None] * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4b94794e-adbc-4e8f-ab39-ba1a2cd47cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_data = True\n",
    "# save_data = True\n",
    "\n",
    "# if load_data:\n",
    "#     train_data, val_data, test_data = load_processed_data()\n",
    "\n",
    "# if not load_data or train_data is None:\n",
    "#     train_data = [batch for batch in tqdm(make_batched_dataset(train_df), desc='Unpacking train batches', total=train_n_batches)]\n",
    "#     val_data = [batch for batch in tqdm(make_batched_dataset(val_df), desc='Unpacking validation batches', total=val_n_batches)]\n",
    "#     test_data = [batch for batch in tqdm(make_batched_dataset(test_df, batch_size=1), desc='Unpacking test batches', total=test_n_batches)]\n",
    "\n",
    "# if save_data:\n",
    "#     save_processed_data(train_data, val_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a064933c-35cb-4339-93b9-453058b2017b",
   "metadata": {},
   "source": [
    "## Определение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def4e99a-5a2c-413d-9fe4-109fae20d624",
   "metadata": {},
   "source": [
    "### Определение класса модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dd302268-7e66-435c-93e3-cebdafc7f9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, embedding_size, nhead,\n",
    "                 num_encoder_layers, num_decoder_layers, \n",
    "                 dim_feedforward, dropout, layer_norm_eps,\n",
    "                 vocab_size, max_seq_len, pad_token_id,\n",
    "                 batch_first, device):\n",
    "        \n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        \n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        self.to(device)\n",
    "        \n",
    "        self.word_embedding = nn.Embedding(vocab_size, embedding_size, pad_token_id, device=device)\n",
    "        \n",
    "        self.positional_encoding = torch.zeros(max_seq_len, embedding_size, device=device)\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, embedding_size, 2):\n",
    "                self.positional_encoding[pos, i] = np.sin(pos / (10000 ** ((2 * i) / embedding_size)))\n",
    "                self.positional_encoding[pos, i + 1] = np.cos(pos / (10000 ** ((2 * (i + 1)) / embedding_size)))\n",
    "        \n",
    "        self.transformer = nn.Transformer(embedding_size, nhead, num_encoder_layers, num_decoder_layers,\n",
    "                                          dim_feedforward, dropout, layer_norm_eps=layer_norm_eps, \n",
    "                                          batch_first=batch_first, device=device)\n",
    "        \n",
    "        self.fc_out = nn.Linear(embedding_size, vocab_size)\n",
    "        \n",
    "    def get_padding_mask(self, input):\n",
    "        # input shape: (batch_size, max_seq_len)\n",
    "        padding_mask = input == self.pad_token_id\n",
    "        return padding_mask\n",
    "    \n",
    "    def forward(self, input, target):\n",
    "        # input shape: (batch_size, max_seq_len)\n",
    "        # target shape: (batch_size, max_seq_len)\n",
    "        \n",
    "        embedded_input = self.word_embedding(input) * np.sqrt(self.embedding_size)\n",
    "        embedded_target = self.word_embedding(target) * np.sqrt(self.embedding_size)\n",
    "        # embedded_input shape: (batch_size, max_seq_len, embedding_size)\n",
    "        # embedded_target shape: (batch_size, target_seq_len, embedding_size)\n",
    "        \n",
    "        target_seq_len = target.shape[1]\n",
    "        \n",
    "        embedded_input[:, :self.max_seq_len] += self.positional_encoding\n",
    "        embedded_target[:, :target_seq_len] += self.positional_encoding[:target_seq_len]\n",
    "        \n",
    "        input_padding_mask = self.get_padding_mask(input)\n",
    "        target_padding_mask = self.get_padding_mask(target)\n",
    "        # input_padding_mask shape: (batch_size, max_seq_len)\n",
    "        # target_padding_mask shape: (batch_size, target_seq_len)\n",
    "        \n",
    "        input_mask = self.transformer.generate_square_subsequent_mask(self.max_seq_len)\n",
    "        target_mask = self.transformer.generate_square_subsequent_mask(target_seq_len)\n",
    "        # input_mask_shape: (max_seq_len, max_seq_len)\n",
    "        # target_mask shape: (target_seq_len, target_seq_len)\n",
    "        \n",
    "        output = self.transformer(embedded_input, embedded_target,\n",
    "                                  input_mask, target_mask,\n",
    "                                  src_key_padding_mask=input_padding_mask,\n",
    "                                  tgt_key_padding_mask=target_padding_mask)\n",
    "        # output shape: (batch_size, max_seq_len, embedding_size)\n",
    "        \n",
    "        fc_output = self.fc_out(output)\n",
    "        # fc_output shape: (batch_size, max_seq_len, vocab_size)\n",
    "        \n",
    "        return fc_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f444e6-837f-4bda-ba0f-0ca350ac9d6a",
   "metadata": {},
   "source": [
    "## Определение функций-утилит"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1237f0-267b-4b76-b02c-eb43c09c5fda",
   "metadata": {},
   "source": [
    "### Сохранение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "42ef352d-2d48-4253-ae38-6bd6e66fa44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, epoch, val_loss, train_loss, path='./models/seq2seq_transformer.model'):\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'val_loss': val_loss,\n",
    "        'train_loss': train_loss\n",
    "    }\n",
    "    \n",
    "    torch.save(checkpoint, path)\n",
    "    print(f'\\n\\tModel saved successfully at {path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267bb350-d049-49ef-9e6c-aaeda118da03",
   "metadata": {},
   "source": [
    "### Загрузка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9b36d6ed-f3c1-459c-b2d7-7d123d09be66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, optimizer=None, path='./model/seq2seq_transformer.model', device=torch.device('cpu')):\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    if optimizer is not None:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        \n",
    "    epoch      = checkpoint['epoch']\n",
    "    val_loss   = checkpoint['val_loss']\n",
    "    train_loss = checkpoint['train_loss']\n",
    "\n",
    "    return {'epoch': epoch, 'val_loss': val_loss, 'train_loss': train_loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee772c3-9c5d-418e-855d-4664a8648e29",
   "metadata": {},
   "source": [
    "## Место хранения модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3863bdb5-fa7f-4902-95a0-ecf2d434403a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = {\n",
    "    'dir': './models/',\n",
    "    'name': 'seq2seq_transformer.model'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d62843a-1792-4baf-a43d-c8fb375d3939",
   "metadata": {},
   "source": [
    "## Обучение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db7ee35-94fe-403c-af6c-a23114bb8335",
   "metadata": {},
   "source": [
    "### Определение параметров обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "df74d24e-eff1-4f04-96b1-8c42c69d0381",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_params = {\n",
    "    'learning_rate': 1e-03,\n",
    "    'epochs': 30,\n",
    "    'max_norm': 1.0,\n",
    "    'patience': 3\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11598852-e1f4-44ef-ab75-62eca9d6f88b",
   "metadata": {},
   "source": [
    "### Определение параметров сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b4be2183-3f5f-4c04-b828-373e545eb03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'embedding_size': 512,\n",
    "    'nhead': 8,\n",
    "    'num_encoder_layers': 6,\n",
    "    'num_decoder_layers': 6,\n",
    "    'dim_feedforward': 2048,\n",
    "    'dropout': 0.1,\n",
    "    'layer_norm_eps': 1e-05,\n",
    "    'vocab_size': vocab_size,\n",
    "    'max_seq_len': max_seq_len + 5,\n",
    "    'pad_token_id': tokenizer.pad_token['id'],\n",
    "    'batch_first': True,\n",
    "    'device': torch.device('cpu')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841711eb-3ac5-499d-997a-1053ac80b226",
   "metadata": {},
   "source": [
    "### Инициализация модели, оптимизатора и функции потерь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5b96307a-a84a-43f3-9013-d48396eb493a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2SeqTransformer(**params).to(params['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e23044da-4d9d-46ba-9870-9c298cbd2beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_params['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5b4de224-4a7e-41f9-9989-46331d41de01",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=params['pad_token_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b78a97c9-c080-4c72-ab09-da1bc969b3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_pretrained_model = False\n",
    "train_state = None\n",
    "\n",
    "if load_pretrained_model:\n",
    "    try:\n",
    "        train_state = load_model(\n",
    "            model, optimizer,\n",
    "            model_path['dir'] + model_path['name'],\n",
    "            params['device']\n",
    "        )\n",
    "        print(f\"Model loaded successfully from {model_path.get('dir') + model_path.get('name')}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'Load failed due to:\\n{e}')\n",
    "\n",
    "epoch = train_state['epoch'] if train_state is not None else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc781b75-6033-4132-87ee-0dce6bbfcc58",
   "metadata": {},
   "source": [
    "### Train-скрипт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8a49626b-708f-4afd-906f-7b952771afcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model, optimizer, criterion,\n",
    "    train_data, val_data, test_data,\n",
    "    train_n_batches, val_n_batches, test_n_batches,\n",
    "    epochs, max_norm, patience, current_epoch,\n",
    "    device, tokenizer, model_path, n_prints=5\n",
    "):\n",
    "    \n",
    "    min_mean_val_loss = float('+inf')\n",
    "    initial_patience = patience\n",
    "    print_every = train_n_batches // n_prints\n",
    "    \n",
    "    for epoch in tqdm(range(current_epoch, epochs), 'Epochs'):\n",
    "        print(f'\\nEpoch [{epoch} / {epochs}]')\n",
    "        \n",
    "        model.train()\n",
    "        for iteration, (input, target) in enumerate(tqdm(train_data, desc='Training iterations', total=train_n_batches)):\n",
    "            input  = input.to(device)\n",
    "            target = target.to(device)\n",
    "            # input shape : (batch_size, max_seq_len)\n",
    "            # target shape: (batch_size, max_seq_len)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(input, target[:, :-1])\n",
    "            # output shape: (batch_size, max_seq_len, vocab_size)\n",
    "            \n",
    "            vocab_size = output.shape[2]\n",
    "            \n",
    "            output = output.reshape(-1, vocab_size)\n",
    "            # output shape: (batch_size * max_seq_len, vocab_size)\n",
    "            \n",
    "            target = target[:, 1:].reshape(-1)\n",
    "            # target shape: (batch_size * max_seq_len)\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            if iteration % print_every == 0:\n",
    "                print(f'\\tIteration #{iteration}: training loss = {loss.item()}')\n",
    "            elif iteration == train_n_batches:\n",
    "                print(f'\\tIteration #{iteration}: training loss = {loss.item()}')\n",
    "            \n",
    "            \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            \n",
    "            val_loss = []\n",
    "            \n",
    "            for input, target in tqdm(val_data, desc='Validating iterations', total=val_n_batches):\n",
    "                input  = input.to(device)\n",
    "                target = target.to(device)\n",
    "                \n",
    "                output = model(input, target[:, :-1])\n",
    "                vocab_size = output.shape[2]\n",
    "                output = output.reshape(-1, vocab_size)\n",
    "                \n",
    "                target = target[:, 1:].reshape(-1)\n",
    "                \n",
    "                local_val_loss = criterion(output, target).item()\n",
    "                val_loss.append(local_val_loss)\n",
    "            \n",
    "            mean_val_loss = sum(val_loss) / len(val_loss)\n",
    "            print(f'\\tValidation loss = {mean_val_loss}')\n",
    "            \n",
    "            if mean_val_loss < min_mean_val_loss:\n",
    "                try:\n",
    "                    save_model(model, optimizer, epoch, mean_val_loss, loss)\n",
    "                    min_mean_val_loss = mean_val_loss\n",
    "                    patience = initial_patience\n",
    "                except Exception as e:\n",
    "                    print(f'Model training stopped due to unhandled exception:\\n{e}')\n",
    "            else:\n",
    "                patience -= 1\n",
    "                \n",
    "            \n",
    "            test_sample = choice(test_data)\n",
    "            seq_len = test_sample[0].shape[1]\n",
    "            \n",
    "            predictions = [tokenizer.pad_token.get('id')]\n",
    "            for i in range(seq_len):\n",
    "                target = torch.tensor(predictions, device=device).unsqueeze(0)\n",
    "                \n",
    "                output = model(test_sample[0], target)\n",
    "                best_prediction = output.argmax(2)[:, -1].item()                \n",
    "                predictions.append(best_prediction)\n",
    "                \n",
    "                if best_prediction == tokenizer.eos_token.get('id'):\n",
    "                    break\n",
    "            \n",
    "            decoded_output = tokenizer.decode(predictions,    return_tokenized=False)\n",
    "            decoded_input  = tokenizer.decode(test_sample[0], return_tokenized=False)\n",
    "            decoded_target = tokenizer.decode(test_sample[1], return_tokenized=False)\n",
    "            \n",
    "            print(f'\\tInput : {decoded_input}')\n",
    "            print(f'\\tOutput: {decoded_output}')\n",
    "            print(f'\\tTarget: {decoded_target}')\n",
    "            \n",
    "        if patience == 0:\n",
    "            print(f'\\nModel learning finished due to early stopping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0d0d3549-7aed-485d-8b49-d742ef811f7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c95e9e6af80140e38a41ed9f6466d39d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epochs', max=30.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [0 / 30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd2de00dbd5d430cb4cf23e72cdadb81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Training iterations', max=5625.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tIteration #0: training loss = 11.89489459991455\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'generator' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-88-f7c84d35ee7d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m train(\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtrain_n_batches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_n_batches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_n_batches\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mlearning_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'epochs'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'max_norm'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-87-367036e3644f>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, criterion, train_data, val_data, test_data, train_n_batches, val_n_batches, test_n_batches, epochs, max_norm, patience, current_epoch, device, tokenizer, model_path, n_prints)\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0miteration\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mprint_every\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'\\tIteration #{iteration}: training loss = {loss.item()}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m             \u001b[1;32melif\u001b[0m \u001b[0miteration\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'\\tIteration #{iteration}: training loss = {loss.item()}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'generator' has no len()"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    model, optimizer, criterion,\n",
    "    train_data, val_data, list(test_data),\n",
    "    train_n_batches, val_n_batches, test_n_batches,\n",
    "    learning_params['epochs'], learning_params['max_norm'],\n",
    "    learning_params['patience'], epoch, params['device'], tokenizer,\n",
    "    model_path['dir'] + model_path['name']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423e8bce-7db9-489d-99ce-17cf8b322451",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
