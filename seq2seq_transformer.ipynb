{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2fb41fd-c50f-4033-9cbf-273df7637194",
   "metadata": {},
   "source": [
    "## Импорт необходимых зависимостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "973dc9dc-9e2e-4d2d-8c9d-98fa61b38d55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import pathlib\n",
    "\n",
    "from pprint import pprint\n",
    "from random import random, sample\n",
    "from typing import List, Union\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from functools import reduce\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn import model_selection\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a8e1272-bf97-4890-945c-6615ba132abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt', quiet=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91d515aa-7400-443e-9e9a-5c3c34d3ef65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RANDOM_STATE = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6e4800-d29f-4ac2-88a3-e5ee010d9901",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c2c1321-83cd-4e5d-afaa-9ae0992153b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/lenta/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c448591d-3ea9-44d1-bfc0-26048460a7b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orig_texts</th>\n",
       "      <th>lemm_texts</th>\n",
       "      <th>nsubj</th>\n",
       "      <th>gender</th>\n",
       "      <th>tense</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1245806</th>\n",
       "      <td>об этом сообщает риа новости со ссылкой на мат...</td>\n",
       "      <td>о это сообщать риа новость с ссылка на мать по...</td>\n",
       "      <td>риа</td>\n",
       "      <td>neut</td>\n",
       "      <td>pres</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594042</th>\n",
       "      <td>генеральный прокурор рф владимир устинов счита...</td>\n",
       "      <td>генеральный прокурор рф владимир устинов счита...</td>\n",
       "      <td>прокурор</td>\n",
       "      <td>masc</td>\n",
       "      <td>pres</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705659</th>\n",
       "      <td>телеканал «дождь» восстановил вещание, прерван...</td>\n",
       "      <td>телеканал « дождь » восстановить вещание, прер...</td>\n",
       "      <td>телеканал</td>\n",
       "      <td>masc</td>\n",
       "      <td>past</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603796</th>\n",
       "      <td>соответствующее требование прозвучало во время...</td>\n",
       "      <td>соответствующий требование прозвучать в время ...</td>\n",
       "      <td>требование</td>\n",
       "      <td>neut</td>\n",
       "      <td>past</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1430273</th>\n",
       "      <td>в пятницу вечером на сайтах \"единой россии\", \"...</td>\n",
       "      <td>в пятница вечером на сайт \"единый россия\", \"гр...</td>\n",
       "      <td>заявления</td>\n",
       "      <td>neut</td>\n",
       "      <td>past</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1741764</th>\n",
       "      <td>в россии может появиться новый деловой телевиз...</td>\n",
       "      <td>в россия мочь появиться новый деловой телевизи...</td>\n",
       "      <td>канал</td>\n",
       "      <td>masc</td>\n",
       "      <td>pres</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243076</th>\n",
       "      <td>пресс-секретарь президента эрнесто абелла назв...</td>\n",
       "      <td>пресс-секретарь президент эрнесто абелла назва...</td>\n",
       "      <td>пресс-секретарь</td>\n",
       "      <td>masc</td>\n",
       "      <td>past</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1809379</th>\n",
       "      <td>а вот винсент дамфусс, который должен был игра...</td>\n",
       "      <td>а вот винсент дамфусс, который должный быть иг...</td>\n",
       "      <td>винсент</td>\n",
       "      <td>masc</td>\n",
       "      <td>fut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690325</th>\n",
       "      <td>задача исследователей заключается в том, чтобы...</td>\n",
       "      <td>задача исследователь заключаться в тот, чтобы ...</td>\n",
       "      <td>задача</td>\n",
       "      <td>fem</td>\n",
       "      <td>pres</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1310713</th>\n",
       "      <td>сам магуайр планирует сыграть главную роль в л...</td>\n",
       "      <td>сам магуайра планировать сыграть главный роль ...</td>\n",
       "      <td>магуайр</td>\n",
       "      <td>masc</td>\n",
       "      <td>pres</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                orig_texts  \\\n",
       "1245806  об этом сообщает риа новости со ссылкой на мат...   \n",
       "1594042  генеральный прокурор рф владимир устинов счита...   \n",
       "705659   телеканал «дождь» восстановил вещание, прерван...   \n",
       "603796   соответствующее требование прозвучало во время...   \n",
       "1430273  в пятницу вечером на сайтах \"единой россии\", \"...   \n",
       "...                                                    ...   \n",
       "1741764  в россии может появиться новый деловой телевиз...   \n",
       "243076   пресс-секретарь президента эрнесто абелла назв...   \n",
       "1809379  а вот винсент дамфусс, который должен был игра...   \n",
       "690325   задача исследователей заключается в том, чтобы...   \n",
       "1310713  сам магуайр планирует сыграть главную роль в л...   \n",
       "\n",
       "                                                lemm_texts            nsubj  \\\n",
       "1245806  о это сообщать риа новость с ссылка на мать по...              риа   \n",
       "1594042  генеральный прокурор рф владимир устинов счита...         прокурор   \n",
       "705659   телеканал « дождь » восстановить вещание, прер...        телеканал   \n",
       "603796   соответствующий требование прозвучать в время ...       требование   \n",
       "1430273  в пятница вечером на сайт \"единый россия\", \"гр...        заявления   \n",
       "...                                                    ...              ...   \n",
       "1741764  в россия мочь появиться новый деловой телевизи...            канал   \n",
       "243076   пресс-секретарь президент эрнесто абелла назва...  пресс-секретарь   \n",
       "1809379  а вот винсент дамфусс, который должный быть иг...          винсент   \n",
       "690325   задача исследователь заключаться в тот, чтобы ...           задача   \n",
       "1310713  сам магуайра планировать сыграть главный роль ...          магуайр   \n",
       "\n",
       "        gender tense  \n",
       "1245806   neut  pres  \n",
       "1594042   masc  pres  \n",
       "705659    masc  past  \n",
       "603796    neut  past  \n",
       "1430273   neut  past  \n",
       "...        ...   ...  \n",
       "1741764   masc  pres  \n",
       "243076    masc  past  \n",
       "1809379   masc   fut  \n",
       "690325     fem  pres  \n",
       "1310713   masc  pres  \n",
       "\n",
       "[50000 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(n=50000, random_state=RANDOM_STATE)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c8d7d0-c3b0-4d3a-b3a0-a9c513f0a39b",
   "metadata": {},
   "source": [
    "### Определение классов словаря и подготовщика данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "761fae45-0202-4290-926e-5c52ef489210",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, tokens=None, unk_id=None):\n",
    "        self.unk_id = unk_id\n",
    "        \n",
    "        self.tokens = tokens\n",
    "        self.tokens_to_ids = {token: id for id, token in enumerate(tokens)} if tokens is not None else None\n",
    "    \n",
    "    def id_to_token(self, id):\n",
    "        return self.tokens[id]\n",
    "    \n",
    "    def token_to_id(self, token):\n",
    "        return self.tokens_to_ids.get(token, self.unk_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d61e34b1-1eb1-4dfd-a2fe-d359eaf0308e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreperator:\n",
    "    def __init__(self, vocab_size):\n",
    "        \n",
    "        self.tokenizer = nltk.tokenize.word_tokenize\n",
    "        \n",
    "        self.special_tokens = {\n",
    "            'pad_token'        : '<pad>',\n",
    "            'unk_token'        : '<unk>',\n",
    "            'sos_token'        : '<sos>',\n",
    "            'eos_token'        : '<eos>',\n",
    "            'g_masc_token'     : '<masc>',\n",
    "            'g_fem_token'      : '<fem>',\n",
    "            'g_neut_token'     : '<neut>',\n",
    "            'g_undefined_token': '<undef>',\n",
    "            't_past_token'     : '<past>',\n",
    "            't_pres_token'     : '<pres>',\n",
    "            't_fut_token'      : '<fut>'\n",
    "        }\n",
    "        \n",
    "        self.special_ids = {token: id for id, token in enumerate(self.special_tokens.keys())}\n",
    "        \n",
    "        self.pad_token = {'id'   : self.special_ids['pad_token'],\n",
    "                          'token': self.special_tokens['pad_token']}\n",
    "        \n",
    "        self.unk_token = {'id'   : self.special_ids['unk_token'],\n",
    "                          'token': self.special_tokens['unk_token']}\n",
    "        \n",
    "        self.sos_token = {'id'   : self.special_ids['sos_token'],\n",
    "                          'token': self.special_tokens['sos_token']}\n",
    "        \n",
    "        self.eos_token = {'id'   : self.special_ids['eos_token'],\n",
    "                          'token': self.special_tokens['eos_token']}\n",
    "        \n",
    "        self.gender_tokens = {\n",
    "            'masc'     : {'id'   : self.special_ids['g_masc_token'],\n",
    "                          'token': self.special_tokens['g_masc_token']},\n",
    "            \n",
    "            'fem'      : {'id'   : self.special_ids['g_fem_token'],\n",
    "                          'token': self.special_tokens['g_fem_token']},\n",
    "            \n",
    "            'neut'     : {'id'   : self.special_ids['g_neut_token'],\n",
    "                          'token': self.special_tokens['g_neut_token']},\n",
    "            \n",
    "            'undefined': {'id'   : self.special_ids['g_undefined_token'],\n",
    "                          'token': self.special_tokens['g_undefined_token']}\n",
    "        }\n",
    "        \n",
    "        self.tense_tokens = {\n",
    "            'past': {'id'   : self.special_ids['t_past_token'],\n",
    "                     'token': self.special_tokens['t_past_token']},\n",
    "            \n",
    "            'pres': {'id'   : self.special_ids['t_pres_token'],\n",
    "                     'token': self.special_tokens['t_pres_token']},\n",
    "            \n",
    "            'fut' : {'id'   : self.special_ids['t_fut_token'],\n",
    "                     'token': self.special_tokens['t_fut_token']}\n",
    "        }\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.vocab_cache_path = {\n",
    "            'dir': './data/cached',\n",
    "            'filename': 'vocab.pkl'\n",
    "        }\n",
    "        \n",
    "        self.vocab = None\n",
    "        \n",
    "    def _tokenize(self, input: Union[List[str], str]):\n",
    "        \"\"\"Input (Union[List[str], str]): a list of string sequences or a single sequence.\"\"\"\n",
    "        \n",
    "        if type(input) is list:\n",
    "            tokens = list(chain(*[self.tokenizer(text, 'russian') for text in tqdm(input, 'Tokenizing texts')]))\n",
    "        else:\n",
    "            tokens = self.tokenizer(input, 'russian')\n",
    "        return tokens\n",
    "    \n",
    "    def build_vocab(self, texts=None, save_vocab=True, load_vocab=False):\n",
    "        if load_vocab:\n",
    "            self.vocab = self.load_vocab(dir_path=self.vocab_cache_path['dir'], filename=self.vocab_cache_path['filename'])\n",
    "            if self.vocab is not None:\n",
    "                return\n",
    "        \n",
    "        print('Building vocab from texts...')\n",
    "        tokens = self._tokenize(texts)\n",
    "        \n",
    "        n_first = self.vocab_size - len(self.special_tokens)\n",
    "        \n",
    "        tokens = [token for token, _ in Counter(tokens).most_common(n_first)]\n",
    "        tokens = list(self.special_tokens.values()) + tokens\n",
    "                \n",
    "        self.vocab = Vocab(tokens, self.unk_token['id'])\n",
    "        print('Success')\n",
    "        \n",
    "        if save_vocab:\n",
    "            dir_path = self.vocab_cache_path['dir']\n",
    "            filename = self.vocab_cache_path['filename']\n",
    "            file_path = dir_path + '/' + filename\n",
    "            \n",
    "            print(f'Saving vocab at {file_path} ...')\n",
    "            self.save_vocab(dir_path=dir_path, filename=filename)\n",
    "    \n",
    "    def save_vocab(self, dir_path, filename):\n",
    "        try:\n",
    "            pathlib.Path(dir_path).mkdir(exist_ok=True)\n",
    "            file_path = dir_path + '/' + filename\n",
    "\n",
    "            with open(file_path, 'wb') as f:\n",
    "                pickle.dump(self.vocab, f)\n",
    "\n",
    "            print(f'Vocab is saved successfully at {file_path}')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'Failed to save vocab due to:\\n{e}')\n",
    "    \n",
    "    def load_vocab(self, dir_path, filename):\n",
    "        try:\n",
    "            file_path = dir_path + '/' + filename\n",
    "            \n",
    "            with open(file_path, 'rb') as f:\n",
    "                vocab = pickle.load(f)\n",
    "            \n",
    "            print(f'Vocab is loaded successfully from {file_path}')\n",
    "            \n",
    "            return vocab\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'Failed to load vocab due to:\\n{e}')\n",
    "            \n",
    "            return None\n",
    "        \n",
    "    def _pad_sequence(self, ids: List[int], max_seq_len) -> List[int]:\n",
    "        if len(ids) >= max_seq_len:\n",
    "            ids = ids[:max_seq_len]\n",
    "        else:\n",
    "            pad_len = max_seq_len - len(ids)\n",
    "            ids.extend(pad_len * [self.pad_token.get('id')])\n",
    "        \n",
    "        return ids\n",
    "    \n",
    "    def _add_special_tokens(self, ids: List[int], context=None) -> None:\n",
    "        ids.insert(0, self.sos_token.get('id'))\n",
    "        try:\n",
    "            eos_position = ids.index(self.pad_token.get('id'))\n",
    "        except ValueError:\n",
    "            eos_position = len(ids)\n",
    "        ids.insert(eos_position, self.eos_token.get('id'))\n",
    "        \n",
    "        if context is not None:\n",
    "            nsubj, gender, tense = context\n",
    "            \n",
    "            context_info = [\n",
    "                self.tense_tokens[tense].get('id'),\n",
    "                self.gender_tokens[gender].get('id'),\n",
    "                self.vocab.token_to_id(nsubj)\n",
    "            ]\n",
    "            \n",
    "            for item in context_info:\n",
    "                ids.insert(0, item)\n",
    "        \n",
    "    def encode(self, input: Union[str, List[str]], context=None, add_special_tokens=True, max_seq_len=None, return_tensor=False):\n",
    "        \"\"\"Input (Union[List[str], str]): a list of string sequences or a single sequence.\"\"\"\n",
    "        # context = (nsubj, gender, tense)\n",
    "        if type(input) is str:\n",
    "            tokens = self._tokenize(input)\n",
    "            ids = [self.vocab.token_to_id(token) for token in tokens]\n",
    "            \n",
    "            if max_seq_len is not None:\n",
    "                ids = self._pad_sequence(ids, max_seq_len)\n",
    "            \n",
    "            if add_special_tokens:\n",
    "                self._add_special_tokens(ids, context)\n",
    "            \n",
    "            if not return_tensor:\n",
    "                return ids\n",
    "            else:\n",
    "                return torch.tensor(ids)\n",
    "            \n",
    "        else:\n",
    "            tokenized_sents = [self._tokenize(sent) for sent in input]\n",
    "            sents_ids = [[self.vocab.token_to_id(token) for token in sent] for sent in tokenized_sents]\n",
    "            \n",
    "            max_seq_len = max(map(len, sents_ids)) if max_seq_len is None else max_seq_len\n",
    "            \n",
    "            padded_sequences = [self._pad_sequence(ids, max_seq_len) for ids in sents_ids]\n",
    "            padded_seq_and_context = zip(padded_sequences, context) if context is not None else None\n",
    "            \n",
    "            if add_special_tokens:\n",
    "                if padded_seq_and_context is not None:\n",
    "                    for ids, context in padded_seq_and_context:\n",
    "                        self._add_special_tokens(ids, context)\n",
    "                else:\n",
    "                    for ids in padded_sequences:\n",
    "                        self._add_special_tokens(ids)\n",
    "            \n",
    "            if not return_tensor:\n",
    "                return padded_sequences\n",
    "            else:\n",
    "                return torch.tensor(padded_sequences)\n",
    "            \n",
    "    def decode(self, encoded_seq: Union[List[int], torch.Tensor], remove_special_tokens=False, return_tokenized=True):\n",
    "        if type(encoded_seq) is list:\n",
    "            \n",
    "            if remove_special_tokens:\n",
    "                encoded_seq = encoded_seq[4:-1]\n",
    "            \n",
    "            decoded_seq = [self.vocab.id_to_token(id) for id in encoded_seq]\n",
    "            \n",
    "            if return_tokenized:\n",
    "                return decoded_seq\n",
    "            else:\n",
    "                return ' '.join(decoded_seq)\n",
    "        \n",
    "        else:\n",
    "            if len(encoded_seq.shape) > 1:\n",
    "                encoded_seq.unsqueeze_(0)\n",
    "                \n",
    "            if remove_special_tokens:\n",
    "                encoded_seq = encoded_seq[4:-1]\n",
    "            \n",
    "            decoded_seq = [self.vocab.id_to_token(id.item()) for id in encoded_seq]\n",
    "            \n",
    "            if return_tokenized:\n",
    "                return decoded_seq\n",
    "            else:\n",
    "                return ' '.join(decoded_seq)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "649b0f77-c913-4cfb-abe2-f25bb50cff3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 100000\n",
    "max_seq_len = None\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f03839d-c4c4-4e58-b1c6-cc5d8eb69f14",
   "metadata": {},
   "source": [
    "### Разбиение данных на обучающие, тестовые и валидационные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6445806b-5852-45de-8faf-f68a4ff2132f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = model_selection.train_test_split(df, train_size=0.9)\n",
    "test_df, val_df = model_selection.train_test_split(test_df, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e176ed0-02ae-46eb-9cf5-739472febefe",
   "metadata": {},
   "source": [
    "### Подготовка словаря"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d27d6a15-994c-4f6c-88f1-0bc8e665b402",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preperator = DataPreperator(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "593b3be1-ac02-4571-a306-f88b6462b171",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = train_df.lemm_texts.to_list() + train_df.orig_texts.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27bd9d94-61c7-4c1e-867e-853221e72492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab is loaded successfully from ./data/cached/vocab.pkl\n"
     ]
    }
   ],
   "source": [
    "data_preperator.build_vocab(load_vocab=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cce9a7-d49e-495e-8448-08b3898dc5d9",
   "metadata": {},
   "source": [
    "### Разбиение данных на батчи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1bc5c99-5ba5-4794-8c75-22cbb05b9ef3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_batched_dataset(df, data_preperator=data_preperator, batch_size=batch_size):\n",
    "    n_batches = len(df) // batch_size\n",
    "    \n",
    "    for n_batch in range(n_batches):\n",
    "        orig   = df.orig_texts.to_list()[batch_size * n_batch:batch_size * (n_batch + 1)]\n",
    "        lemm   = df.lemm_texts.to_list()[batch_size * n_batch:batch_size * (n_batch + 1)]\n",
    "        nsubj  = df.nsubj.to_list()[batch_size * n_batch:batch_size * (n_batch + 1)]\n",
    "        gender = df.gender.to_list()[batch_size * n_batch:batch_size * (n_batch + 1)]\n",
    "        tense  = df.tense.to_list()[batch_size * n_batch:batch_size * (n_batch + 1)]\n",
    "        \n",
    "        encoded_input  = data_preperator.encode(lemm, context=list(zip(nsubj, gender, tense)), return_tensor=True)\n",
    "        encoded_target = data_preperator.encode(orig, return_tensor=True)\n",
    "        \n",
    "        batch = (encoded_input, encoded_target)\n",
    "        \n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b94794e-adbc-4e8f-ab39-ba1a2cd47cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = make_batched_dataset(train_df)\n",
    "val_data = make_batched_dataset(val_df)\n",
    "test_data = make_batched_dataset(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11598852-e1f4-44ef-ab75-62eca9d6f88b",
   "metadata": {},
   "source": [
    "### Определение параметров сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4be2183-3f5f-4c04-b828-373e545eb03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'd_model': 512,\n",
    "    'nhead': 8,\n",
    "    'num_encoder_layers': 6,\n",
    "    'num_decoder_layers': 6,\n",
    "    'dim_feedforward': 2048,\n",
    "    'dropout': 0.1,\n",
    "    'activation': 'relu',\n",
    "    'layer_norm_eps': 1e-05,\n",
    "    'embedding_size': 300,\n",
    "    'vocab_size': vocab_size,\n",
    "    'pad_token_id': data_preperator.pad_token['id'],\n",
    "    'batch_first': True,\n",
    "    'device': torch.device('cpu')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ba3a81-1b6e-4b56-8b92-f4f8d0f421e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d_model, nhead, num_encoder_layers, num_decoder_layers,\n",
    "#                  dim_feedforward, dropout, activation,\n",
    "#                  custom_encoder, custom_decoder, layer_norm_eps,\n",
    "#                  batch_first, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd302268-7e66-435c-93e3-cebdafc7f9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model, n_head,\n",
    "        num_encoder_layers, num_decoder_layers,\n",
    "        dim_feedforward, dropout,\n",
    "        activation, layer_norm_eps,\n",
    "        embedding_size, vocab_size,\n",
    "        pad_token_id, batch_first,\n",
    "        device):\n",
    "        \n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        \n",
    "        self.transformer = nn.Transformer(d_model, n_head, num_encoder_layers,\n",
    "                                          num_decoder_layers, dim_feedforward,\n",
    "                                          dropout, activation,\n",
    "                                          layer_norm_eps=layer_norm_eps,\n",
    "                                          batch_first=batch_first, device=device)\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size, pad_token_id, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68fbc27-1c26-4889-a660-c61167b124db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
