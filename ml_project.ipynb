{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "stable-fluid",
   "metadata": {},
   "source": [
    "## Импорт необходимых библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cultural-nutrition",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import csv\n",
    "import torch\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "horizontal-watch",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "elementary-insert",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('ru_core_news_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "therapeutic-numbers",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data/some_data/processed_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "serial-passport",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path + \"original_texts.txt\", 'r', encoding='utf-8') as f:\n",
    "    orig_texts = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "developed-think",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'morphologizer', 'parser', 'ner', 'attribute_ruler', 'lemmatizer']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = nlp.pipe_names.copy()\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "usual-heart",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ner', 'attribute_ruler']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.remove('parser')\n",
    "pipeline.remove('tok2vec')\n",
    "pipeline.remove('lemmatizer')\n",
    "pipeline.remove('morphologizer')\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bigger-coordinate",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_lemm_texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "banned-inspection",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "360279it [09:45, 615.54it/s]\n"
     ]
    }
   ],
   "source": [
    "for text in tqdm(nlp.pipe(orig_texts, disable=pipeline)):\n",
    "    lemm_text = []\n",
    "    for token in text:\n",
    "        if token.dep_ == 'nsubj':\n",
    "            lemm_text.append('<PH>')\n",
    "        else:\n",
    "            lemm_text.append(token.lemma_.lower())\n",
    "    tokenized_lemm_texts.append(lemm_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "tracked-humanity",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemm_texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "substantial-richards",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 360279/360279 [00:01<00:00, 219633.93it/s]\n"
     ]
    }
   ],
   "source": [
    "for text in tqdm(tokenized_lemm_texts):\n",
    "    assembled = \"\"\n",
    "    for token in text:\n",
    "        assembled += token + ' '\n",
    "    assembled = re.sub(r'\\s([?.!\"](?:\\s|$))', r'\\1', assembled).rstrip()\n",
    "    lemm_texts.append(assembled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instant-internship",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "given-reach",
   "metadata": {},
   "source": [
    "### Построение csv-файла с необходимыми признаками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "subject-league",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'morphologizer', 'parser', 'ner', 'attribute_ruler', 'lemmatizer']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = nlp.pipe_names.copy()\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "tough-congress",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ner', 'attribute_ruler', 'lemmatizer']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.remove('parser')\n",
    "pipeline.remove('tok2vec')\n",
    "pipeline.remove('morphologizer')\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "successful-parish",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "360279it [02:26, 2465.22it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "\n",
    "for i, text in tqdm(enumerate((nlp.pipe(orig_texts, disable=pipeline)))):\n",
    "    for token in text:\n",
    "        if token.dep_ == 'nsubj':\n",
    "            dataset.append({\n",
    "                'orig_texts': orig_texts[i].lower(),\n",
    "                'lemm_texts': lemm_texts[i].lower(),\n",
    "                'nsubj': token.text.lower(),\n",
    "            })\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "compatible-hungarian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'orig_texts': 'я предлагаю оригинальный подарок для малыша!',\n",
       "  'lemm_texts': '<PH> предлагать оригинальный подарок для малыш!',\n",
       "  'nsubj': 'я'},\n",
       " {'orig_texts': 'я обезательно перезвоню в любом случае.',\n",
       "  'lemm_texts': '<PH> обезательно перезвонить в любой случай.',\n",
       "  'nsubj': 'я'},\n",
       " {'orig_texts': 'цены на память я не помню.',\n",
       "  'lemm_texts': 'цена на память <PH> не помнить.',\n",
       "  'nsubj': 'я'},\n",
       " {'orig_texts': 'я не помню, где находились.',\n",
       "  'lemm_texts': '<PH> не помнить, где находиться.',\n",
       "  'nsubj': 'я'},\n",
       " {'orig_texts': 'я работаю на высококачественных американских материалах.',\n",
       "  'lemm_texts': '<PH> работать на высококачественный американский материал.',\n",
       "  'nsubj': 'я'}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fresh-freeware",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 358502/358502 [00:00<00:00, 445856.68it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(data_path + 'dataset.csv', 'w', encoding='utf-8') as f:\n",
    "    csv_writer = csv.writer(f, delimiter=',')\n",
    "    csv_writer.writerow(['orig_texts', 'lemm_texts', 'nsubj'])\n",
    "    for item in tqdm(dataset):\n",
    "        csv_writer.writerow(item.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "parallel-petroleum",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEncoderFeatures:\n",
    "    def __init__(self, tokens_idx: List[int], nsubj_idx: int):\n",
    "        self.tokens_idx = tokens_idx\n",
    "        self.nsubj_idx = nsubj_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "primary-voluntary",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, tokens: List[str], unk_idx: int):\n",
    "        self._tokens = tokens\n",
    "        self._token_to_idx = {token: i for i, token in enumerate(tokens) if token not in self._token_to_idx}\n",
    "        self._unk_idx = unk_idx\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self._tokens)\n",
    "    \n",
    "    def word_to_idx(self, word):\n",
    "        return self._token_to_idx.get(word, self._unk_idx)\n",
    "    \n",
    "    def idx_to_word(self, idx):\n",
    "        return self._tokens[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "induced-moisture",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTransformer:\n",
    "    def __init__(self, vocab_size):\n",
    "        self.vocab = None\n",
    "        self.vocab_size = vocab_size\n",
    "        self.special_words_to_idx = {'<PH>': 0, '<UNK>': 1, '<EOS>': 2, '<SOS>': 3, '<PAD>': 4}\n",
    "        self._tokenizer = nltk.tokenize.wordpunct_tokenize\n",
    "        \n",
    "    def tokenize(self, text):\n",
    "        return self._tokenizer(text.lower())\n",
    "\n",
    "    def build_vocab(self, tokens):\n",
    "        inp_tokens = [special_word for special_word in self.special_words_to_idx.keys()]\n",
    "        for token, _ in Counter(tokens).most_common(self.max_vocab_size - len(self.special_words)):\n",
    "            inp_tokens.append(token)\n",
    "        \n",
    "        self.vocab = Vocab(inp_tokens, self.unk_idx)\n",
    "        \n",
    "    def transform_single_text(self, text):\n",
    "        tokens = self.tokenize(text)\n",
    "        idxs = [self.vocab.word_to_idx(token) for token in tokens]\n",
    "        return idxs\n",
    "        \n",
    "    def transform(self, texts):\n",
    "        result = []\n",
    "        for text in texts:\n",
    "            result.append(self.transform_single_text(text))\n",
    "        return result\n",
    "    \n",
    "    def fit_transform(self, texts):\n",
    "        result = []\n",
    "        tokenized_texts = [self.tokenize(text) for text in texts]\n",
    "        self.build_vocab(chain(tokenized_texts))\n",
    "        for tokens in tokenized_texts:\n",
    "            idxs = [self.vocab.word_to_idx(token) for token in tokens]\n",
    "            result.append(idxs)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "rough-boundary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_encoder_features(token_idxs: List[int], nsubj_idx: int, special_idxs: dict, max_seq_len=10):\n",
    "    pad_idx = special_idxs['<PAD>']\n",
    "    if len(token_idxs) >= max_seq_len:\n",
    "        inp_idxs = token_idxs[:max_seq_len]\n",
    "    else:\n",
    "        pad_completion_size = len(token_idxs - max_seq_len)\n",
    "        inp_idxs = token_idxs + [pad_idx for i in range(pad_completion_size)]\n",
    "    \n",
    "    sos_idx = special_idxs['<SOS>']\n",
    "    ph_idx = special_idxs['<PH>']\n",
    "    eos_idx = special_idxs['<EOS>']\n",
    "    \n",
    "    inp_idxs.insert(0, sos_idx)\n",
    "    inp_idxs.insert(0, ph_idx)\n",
    "    inp_idxs.append(eos_idx)\n",
    "    \n",
    "    return InputEncoderFeatures(inp_idxs, nsubj_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "french-secret",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_to_tensor(features, for_encoder=False):\n",
    "    if for_encoder:\n",
    "        text_tensor = torch.tensor([feature.tokens_idx for feature in features])\n",
    "        nsubj_tensor = torch.tensor([feature.nsubj_idx for feature in features])\n",
    "        return text_tensor, nsubj_tensor\n",
    "    else:\n",
    "        text_tensor = torch.tensor([feature for feature in features])\n",
    "        return text_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "exterior-tsunami",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data/some_data/processed_data/dataset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "piano-universe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "possible-tunnel",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.lemm_texts.str.startswith('<PH>')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "incredible-simpson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orig_texts</th>\n",
       "      <th>lemm_texts</th>\n",
       "      <th>nsubj</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>я предлагаю оригинальный подарок для малыша!</td>\n",
       "      <td>&lt;PH&gt; предлагать оригинальный подарок для малыш!</td>\n",
       "      <td>я</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>я обезательно перезвоню в любом случае.</td>\n",
       "      <td>&lt;PH&gt; обезательно перезвонить в любой случай.</td>\n",
       "      <td>я</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>я не помню, где находились.</td>\n",
       "      <td>&lt;PH&gt; не помнить, где находиться.</td>\n",
       "      <td>я</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>я работаю на высококачественных американских м...</td>\n",
       "      <td>&lt;PH&gt; работать на высококачественный американск...</td>\n",
       "      <td>я</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>я же продам за 400 руб.</td>\n",
       "      <td>&lt;PH&gt; же продать за 400 руб.</td>\n",
       "      <td>я</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358476</th>\n",
       "      <td>хелена ясно выразилась на этот счет.</td>\n",
       "      <td>&lt;PH&gt; ясно выразиться на этот счёт.</td>\n",
       "      <td>хелена</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358477</th>\n",
       "      <td>она ясно прочерчивалась среди опадающей листвы.</td>\n",
       "      <td>&lt;PH&gt; ясно прочерчивалась среди опадать листва.</td>\n",
       "      <td>она</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358478</th>\n",
       "      <td>она ясно видела сквозь обманывающую маску.</td>\n",
       "      <td>&lt;PH&gt; ясно видеть сквозь обманывать маска.</td>\n",
       "      <td>она</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358479</th>\n",
       "      <td>я ясно видел против пламени.</td>\n",
       "      <td>&lt;PH&gt; ясно видеть против пламя.</td>\n",
       "      <td>я</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358481</th>\n",
       "      <td>это ястреб спустился на четвертую ступень.</td>\n",
       "      <td>&lt;PH&gt; &lt;PH&gt; спуститься на четвёртый ступень.</td>\n",
       "      <td>это</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>336607 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               orig_texts  \\\n",
       "0            я предлагаю оригинальный подарок для малыша!   \n",
       "1                 я обезательно перезвоню в любом случае.   \n",
       "3                             я не помню, где находились.   \n",
       "4       я работаю на высококачественных американских м...   \n",
       "5                                 я же продам за 400 руб.   \n",
       "...                                                   ...   \n",
       "358476               хелена ясно выразилась на этот счет.   \n",
       "358477    она ясно прочерчивалась среди опадающей листвы.   \n",
       "358478         она ясно видела сквозь обманывающую маску.   \n",
       "358479                       я ясно видел против пламени.   \n",
       "358481         это ястреб спустился на четвертую ступень.   \n",
       "\n",
       "                                               lemm_texts   nsubj  \n",
       "0         <PH> предлагать оригинальный подарок для малыш!       я  \n",
       "1            <PH> обезательно перезвонить в любой случай.       я  \n",
       "3                        <PH> не помнить, где находиться.       я  \n",
       "4       <PH> работать на высококачественный американск...       я  \n",
       "5                             <PH> же продать за 400 руб.       я  \n",
       "...                                                   ...     ...  \n",
       "358476                 <PH> ясно выразиться на этот счёт.  хелена  \n",
       "358477     <PH> ясно прочерчивалась среди опадать листва.     она  \n",
       "358478          <PH> ясно видеть сквозь обманывать маска.     она  \n",
       "358479                     <PH> ясно видеть против пламя.       я  \n",
       "358481         <PH> <PH> спуститься на четвёртый ступень.     это  \n",
       "\n",
       "[336607 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "external-reference",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = model_selection.train_test_split(df, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fourth-tower",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df, val_df = model_selection.train_test_split(test_df, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-brief",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
