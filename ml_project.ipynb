{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "stable-fluid",
   "metadata": {},
   "source": [
    "## Импорт необходимых библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cultural-nutrition",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import csv\n",
    "import torch\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "horizontal-watch",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "elementary-insert",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('ru_core_news_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "therapeutic-numbers",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data/some_data/processed_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "serial-passport",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path + \"original_texts.txt\", 'r', encoding='utf-8') as f:\n",
    "    orig_texts = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "developed-think",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'morphologizer', 'parser', 'ner', 'attribute_ruler', 'lemmatizer']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = nlp.pipe_names.copy()\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "usual-heart",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ner', 'attribute_ruler']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.remove('parser')\n",
    "pipeline.remove('tok2vec')\n",
    "pipeline.remove('lemmatizer')\n",
    "pipeline.remove('morphologizer')\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bigger-coordinate",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_lemm_texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "banned-inspection",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "360279it [09:51, 608.84it/s]\n"
     ]
    }
   ],
   "source": [
    "for text in tqdm(nlp.pipe(orig_texts, disable=pipeline)):\n",
    "    lemm_text = []\n",
    "    for token in text:\n",
    "        if token.dep_ == 'nsubj':\n",
    "            lemm_text.append('<PH>')\n",
    "        else:\n",
    "            lemm_text.append(token.lemma_.lower())\n",
    "    tokenized_lemm_texts.append(lemm_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "tracked-humanity",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemm_texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "substantial-richards",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 360279/360279 [00:02<00:00, 172355.88it/s]\n"
     ]
    }
   ],
   "source": [
    "for text in tqdm(tokenized_lemm_texts):\n",
    "    assembled = \"\"\n",
    "    for token in text:\n",
    "        assembled += token + ' '\n",
    "    assembled = re.sub(r'\\s([?.!\"](?:\\s|$))', r'\\1', assembled).rstrip()\n",
    "    lemm_texts.append(assembled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instant-internship",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "given-reach",
   "metadata": {},
   "source": [
    "### Построение csv-файла с необходимыми признаками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "subject-league",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'morphologizer', 'parser', 'ner', 'attribute_ruler', 'lemmatizer']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = nlp.pipe_names.copy()\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "tough-congress",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ner', 'attribute_ruler', 'lemmatizer']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.remove('parser')\n",
    "pipeline.remove('tok2vec')\n",
    "pipeline.remove('morphologizer')\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "successful-parish",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "360279it [03:11, 1883.13it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "\n",
    "for i, text in tqdm(enumerate((nlp.pipe(orig_texts, disable=pipeline)))):\n",
    "    for token in text:\n",
    "        if token.dep_ == 'nsubj':\n",
    "            dataset.append({\n",
    "                'orig_texts': orig_texts[i].lower(),\n",
    "                'lemm_texts': lemm_texts[i].lower(),\n",
    "                'nsubj': token.text.lower(),\n",
    "            })\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "compatible-hungarian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'orig_texts': 'я предлагаю оригинальный подарок для малыша!',\n",
       "  'lemm_texts': '<ph> предлагать оригинальный подарок для малыш!',\n",
       "  'nsubj': 'я'},\n",
       " {'orig_texts': 'я обезательно перезвоню в любом случае.',\n",
       "  'lemm_texts': '<ph> обезательно перезвонить в любой случай.',\n",
       "  'nsubj': 'я'},\n",
       " {'orig_texts': 'цены на память я не помню.',\n",
       "  'lemm_texts': 'цена на память <ph> не помнить.',\n",
       "  'nsubj': 'я'},\n",
       " {'orig_texts': 'я не помню, где находились.',\n",
       "  'lemm_texts': '<ph> не помнить , где находиться.',\n",
       "  'nsubj': 'я'},\n",
       " {'orig_texts': 'я работаю на высококачественных американских материалах.',\n",
       "  'lemm_texts': '<ph> работать на высококачественный американский материал.',\n",
       "  'nsubj': 'я'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fresh-freeware",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 358502/358502 [00:01<00:00, 323956.07it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(data_path + 'dataset.csv', 'w', encoding='utf-8') as f:\n",
    "    csv_writer = csv.writer(f, delimiter=',')\n",
    "    csv_writer.writerow(['orig_texts', 'lemm_texts', 'nsubj'])\n",
    "    for item in tqdm(dataset):\n",
    "        csv_writer.writerow(item.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "parallel-petroleum",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEncoderFeatures:\n",
    "    def __init__(self, tokens_idx: List[int], nsubj_idx: int):\n",
    "        self.tokens_idx = tokens_idx\n",
    "        self.nsubj_idx = nsubj_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "primary-voluntary",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, tokens: List[str], unk_idx: int):\n",
    "        self._tokens = tokens\n",
    "        self.token_to_idx = {token: i for i, token in enumerate(tokens)}\n",
    "        self._unk_idx = unk_idx\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self._tokens)\n",
    "    \n",
    "    def word_to_idx(self, word):\n",
    "        return self.token_to_idx.get(word, self._unk_idx)\n",
    "    \n",
    "    def idx_to_word(self, idx):\n",
    "        return self._tokens[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "induced-moisture",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTransformer:\n",
    "    def __init__(self, vocab_size):\n",
    "        self.vocab = None\n",
    "        self.vocab_size = vocab_size\n",
    "        self.special_words_to_idx = {'<PH>': 0, '<UNK>': 1, '<EOS>': 2, '<SOS>': 3, '<PAD>': 4}\n",
    "        self._tokenizer = nltk.tokenize.wordpunct_tokenize\n",
    "        \n",
    "    def tokenize(self, text):\n",
    "        return self._tokenizer(text.lower())\n",
    "\n",
    "    def build_vocab(self, tokens):\n",
    "        inp_tokens = [special_word for special_word in self.special_words_to_idx.keys()]\n",
    "        for token, _ in Counter(tokens).most_common(self.vocab_size - len(self.special_words_to_idx)):\n",
    "            inp_tokens.append(token)\n",
    "        \n",
    "        self.vocab = Vocab(inp_tokens, self.special_words_to_idx['<UNK>'])\n",
    "        \n",
    "    def transform_single_text(self, text):\n",
    "        tokens = self.tokenize(text)\n",
    "        idxs = [self.vocab.word_to_idx(token) for token in tokens]\n",
    "        return idxs\n",
    "        \n",
    "    def transform(self, texts):\n",
    "        result = []\n",
    "        for text in texts:\n",
    "            result.append(self.transform_single_text(text))\n",
    "        return result\n",
    "    \n",
    "    def get_vocab(self):\n",
    "        return self.vocab.token_to_idx\n",
    "    \n",
    "    def fit_transform(self, texts):\n",
    "        result = []\n",
    "        tokenized_texts = [self.tokenize(text) for text in texts]\n",
    "        self.build_vocab(chain(*tokenized_texts))\n",
    "        for tokens in tokenized_texts:\n",
    "            idxs = [self.vocab.word_to_idx(token) for token in tokens]\n",
    "            result.append(idxs)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "rough-boundary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_encoder_features(token_idxs: List[int], nsubj_idx: int, special_idxs: dict, max_seq_len=10):\n",
    "    inp_idxs = token_idxs.copy()\n",
    "    pad_idx = special_idxs['<PAD>']\n",
    "      \n",
    "    if len(inp_idxs) >= max_seq_len:\n",
    "        inp_idxs = inp_idxs[:max_seq_len]\n",
    "    else:\n",
    "        pad_completion_size = abs(max_seq_len - len(token_idxs))\n",
    "        inp_idxs = inp_idxs + [pad_idx for i in range(pad_completion_size)]\n",
    "    \n",
    "    ph_idx = special_idxs['<PH>']\n",
    "    sos_idx = special_idxs['<SOS>']\n",
    "    eos_idx = special_idxs['<EOS>']\n",
    "    \n",
    "    inp_idxs.insert(0, ph_idx)\n",
    "    inp_idxs.insert(0, sos_idx)\n",
    "    inp_idxs.append(eos_idx)\n",
    "    \n",
    "    return InputEncoderFeatures(inp_idxs, nsubj_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "french-secret",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_to_tensor(features, for_encoder=False):\n",
    "    if for_encoder:\n",
    "        text_tensor = torch.tensor([feature.tokens_idx for feature in features], dtype=torch.long)\n",
    "        nsubj_tensor = torch.tensor([feature.nsubj_idx for feature in features], dtype=torch.long)\n",
    "        return text_tensor, nsubj_tensor\n",
    "    else:\n",
    "        text_tensor = torch.tensor([feature for feature in features], dtype=torch.long)\n",
    "        return text_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "exterior-tsunami",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data/some_data/processed_data/dataset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "piano-universe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6e9fd68-0611-413d-94d8-ad8c583f4b16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           <PH> предлагать оригинальный подарок для малыш!\n",
       "1              <PH> обезательно перезвонить в любой случай.\n",
       "2                           цена на память <PH> не помнить.\n",
       "3                         <PH> не помнить , где находиться.\n",
       "4         <PH> работать на высококачественный американск...\n",
       "                                ...                        \n",
       "358497       другая <PH> медленно подбрести к свой товарка.\n",
       "358498         зелёный <PH> застынуть на мраморный ступень.\n",
       "358499                      большой <PH> шмыгнуть по песок.\n",
       "358500          домашний <PH> быстро пробежать вдоль штора.\n",
       "358501                      крошечный <PH> сбежать с валун.\n",
       "Name: lemm_texts, Length: 358502, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.lemm_texts.str.replace('<ph>', '<PH>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81f15b76-471a-4b64-83db-4afdf9d0a8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lemm_texts'] = df.lemm_texts.str.replace('<ph>', '<PH>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3e62746-00c4-499d-8289-52d41dfd1602",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.lemm_texts.str.startswith('<PH>')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a241665-74c7-46db-868a-2e9052cb5268",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lemm_texts'] = df.lemm_texts.str.replace('<PH> <PH>', '<PH>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e012a8e-b705-44fa-affb-f74a0ef0bea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2340"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df.lemm_texts.str.startswith('<PH> <PH>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a44cef9d-ffb5-4adf-be1e-1c7ce2f61beb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "336607"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d9f40e2-bf19-4115-819c-e553ea6efeba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orig_texts</th>\n",
       "      <th>lemm_texts</th>\n",
       "      <th>nsubj</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>245060</th>\n",
       "      <td>мы пошли по улочке hanoman</td>\n",
       "      <td>&lt;PH&gt; пойти по улочка hanoman</td>\n",
       "      <td>мы</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        orig_texts                    lemm_texts nsubj\n",
       "245060  мы пошли по улочке hanoman  <PH> пойти по улочка hanoman    мы"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.lemm_texts.str.find('h') != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "external-reference",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = model_selection.train_test_split(df, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fourth-tower",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df, val_df = model_selection.train_test_split(test_df, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "complete-brief",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_id = TextTransformer(vocab_size=15000)\n",
    "nsubj_to_id = TextTransformer(vocab_size=15000)\n",
    "\n",
    "train_idx = text_to_id.fit_transform([token[4:] for token in train_df['lemm_texts']])\n",
    "val_idx = text_to_id.transform(val_df['lemm_texts'])\n",
    "test_idx = text_to_id.transform(test_df['lemm_texts'])\n",
    "\n",
    "nsubj_train_idx = nsubj_to_id.fit_transform([token for token in train_df['nsubj']])\n",
    "nsubj_val_idx = nsubj_to_id.transform(val_df['nsubj'])\n",
    "nsubj_test_idx = nsubj_to_id.transform(test_df['nsubj'])\n",
    "\n",
    "nsubj_train_to_id = {nsubj: i for i, nsubj in enumerate(train_df['nsubj'])}\n",
    "nsubj_val_to_id = {nsubj: i for i, nsubj in enumerate(val_df['nsubj'])}\n",
    "nsubj_test_to_id = {nsubj: i for i, nsubj in enumerate(test_df['nsubj'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f5f7a1c-819f-49cc-a491-8a26fb1bc837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "302946"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "60711374-36b9-4e21-b59b-7dbf9e2f2ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = [build_encoder_features(tokens_idx, nsubj_idx, special_idxs=text_to_id.special_words_to_idx) \n",
    "                 for tokens_idx, nsubj_idx in zip(train_idx, chain(*nsubj_train_idx))]\n",
    "\n",
    "val_features = [build_encoder_features(tokens_idx, nsubj_idx, special_idxs=text_to_id.special_words_to_idx) \n",
    "                 for tokens_idx, nsubj_idx in zip(val_idx, chain(*nsubj_val_idx))]\n",
    "\n",
    "test_features = [build_encoder_features(tokens_idx, nsubj_idx, special_idxs=text_to_id.special_words_to_idx) \n",
    "                 for tokens_idx, nsubj_idx in zip(val_idx, chain(*nsubj_test_idx))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3117f28e-cc2a-4e35-907e-c575546bbcc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "302946"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8fa2cca3-17af-4c51-adc5-cb2654ffe98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens_idx': [3, 0, 664, 74, 7, 1548, 5, 4, 4, 4, 4, 4, 2], 'nsubj_idx': 9}\n",
      "{'tokens_idx': [3, 0, 14, 7, 57, 5, 4, 4, 4, 4, 4, 4, 2], 'nsubj_idx': 1633}\n",
      "{'tokens_idx': [3, 0, 2799, 105, 6, 665, 5, 4, 4, 4, 4, 4, 2], 'nsubj_idx': 9}\n",
      "{'tokens_idx': [3, 0, 67, 23, 16, 36, 5, 4, 4, 4, 4, 4, 2], 'nsubj_idx': 9}\n",
      "{'tokens_idx': [3, 0, 1515, 17, 7506, 5, 4, 4, 4, 4, 4, 4, 2], 'nsubj_idx': 170}\n",
      "{'tokens_idx': [3, 0, 2968, 9, 217, 5, 4, 4, 4, 4, 4, 4, 2], 'nsubj_idx': 7}\n",
      "{'tokens_idx': [3, 0, 285, 58, 5768, 5, 4, 4, 4, 4, 4, 4, 2], 'nsubj_idx': 6}\n",
      "{'tokens_idx': [3, 0, 76, 17, 15, 489, 5, 4, 4, 4, 4, 4, 2], 'nsubj_idx': 5}\n",
      "{'tokens_idx': [3, 0, 7053, 3032, 35, 850, 141, 5, 4, 4, 4, 4, 2], 'nsubj_idx': 7}\n",
      "{'tokens_idx': [3, 0, 1195, 10, 150, 5, 4, 4, 4, 4, 4, 4, 2], 'nsubj_idx': 282}\n"
     ]
    }
   ],
   "source": [
    "for example in train_features[:10]:\n",
    "    print(example.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "01e0321c-815a-4573-8fb1-4bdcfcc6d978",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_tensor, train_nsubj_tensor = features_to_tensor(train_features, for_encoder=True)\n",
    "val_text_tensor, val_nsubj_tensor = features_to_tensor(val_features, for_encoder=True)\n",
    "test_text_tensor, test_nsubj_tensor = features_to_tensor(test_features, for_encoder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b64ced96-262c-405c-8812-cd472670d927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   3,    0,  664,  ...,    4,    4,    2],\n",
      "        [   3,    0,   14,  ...,    4,    4,    2],\n",
      "        [   3,    0, 2799,  ...,    4,    4,    2],\n",
      "        ...,\n",
      "        [   3,    0,  365,  ...,    4,    4,    2],\n",
      "        [   3,    0,   45,  ...,    4,    4,    2],\n",
      "        [   3,    0, 7649,  ...,    4,    4,    2]])\n"
     ]
    }
   ],
   "source": [
    "print(train_text_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "47834eb9-1302-41bc-94d9-e0fbdc9e3a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   9, 1633,    9,  ...,    5,    5, 1425])\n"
     ]
    }
   ],
   "source": [
    "print(train_nsubj_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ed33ab99-cd3f-4e67-81a2-c1b3424cf9cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([302946, 13])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dfb85b8d-03d9-4a69-853d-bcf488bd60bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3d577e72-0d91-41b4-880f-6ad1a27fde89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size,  hidden_size)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e8f701de-8832-4489-99ea-d5671f0ac586",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "65148196-bbde-4829-b91a-d683ed3e0636",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 2 required positional arguments: 'input_size' and 'hidden_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-93ea38917fb1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mencoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEncoderRNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDecoderRNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mteacher_forcing_ratio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'input_size' and 'hidden_size'"
     ]
    }
   ],
   "source": [
    "encoder = EncoderRNN()\n",
    "decoder = DecoderRNN()\n",
    "teacher_forcing_ratio = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51389b66-5881-4774-b008-48511a0d889a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(lemm_tensor, orig_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
