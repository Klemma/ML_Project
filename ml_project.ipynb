{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "higher-destiny",
   "metadata": {},
   "source": [
    "## Импорт необходимых библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "favorite-acrobat",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import csv\n",
    "import torch\n",
    "import nltk\n",
    "\n",
    "from itertools import chain\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broke-testing",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beginning-benjamin",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('ru_core_news_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "genetic-creator",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data/some_data/processed_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "scenic-alexandria",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path + \"original_texts.txt\", 'r', encoding='utf-8') as f:\n",
    "    orig_texts = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "grand-international",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'morphologizer', 'parser', 'ner', 'attribute_ruler', 'lemmatizer']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = nlp.pipe_names.copy()\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "blank-timeline",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ner', 'attribute_ruler']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.remove('parser')\n",
    "pipeline.remove('tok2vec')\n",
    "pipeline.remove('lemmatizer')\n",
    "pipeline.remove('morphologizer')\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fundamental-austria",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_lemm_texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "virgin-committee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "360279it [09:45, 615.54it/s]\n"
     ]
    }
   ],
   "source": [
    "for text in tqdm(nlp.pipe(orig_texts, disable=pipeline)):\n",
    "    lemm_text = []\n",
    "    for token in text:\n",
    "        if token.dep_ == 'nsubj':\n",
    "            lemm_text.append('<PH>')\n",
    "        else:\n",
    "            lemm_text.append(token.lemma_.lower())\n",
    "    tokenized_lemm_texts.append(lemm_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bottom-granny",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemm_texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cooked-wireless",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 360279/360279 [00:01<00:00, 219633.93it/s]\n"
     ]
    }
   ],
   "source": [
    "for text in tqdm(tokenized_lemm_texts):\n",
    "    assembled = \"\"\n",
    "    for token in text:\n",
    "        assembled += token + ' '\n",
    "    assembled = re.sub(r'\\s([?.!\"](?:\\s|$))', r'\\1', assembled).rstrip()\n",
    "    lemm_texts.append(assembled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honest-maine",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "graphic-weekend",
   "metadata": {},
   "source": [
    "### Построение csv-файла с необходимыми признаками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "committed-correspondence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'morphologizer', 'parser', 'ner', 'attribute_ruler', 'lemmatizer']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = nlp.pipe_names.copy()\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "reasonable-method",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ner', 'attribute_ruler', 'lemmatizer']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.remove('parser')\n",
    "pipeline.remove('tok2vec')\n",
    "pipeline.remove('morphologizer')\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ideal-equivalent",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "360279it [02:26, 2465.22it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "\n",
    "for i, text in tqdm(enumerate((nlp.pipe(orig_texts, disable=pipeline)))):\n",
    "    for token in text:\n",
    "        if token.dep_ == 'nsubj':\n",
    "            dataset.append({\n",
    "                'orig_texts': orig_texts[i].lower(),\n",
    "                'lemm_texts': lemm_texts[i].lower(),\n",
    "                'nsubj': token.text.lower(),\n",
    "            })\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "downtown-official",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'orig_texts': 'я предлагаю оригинальный подарок для малыша!',\n",
       "  'lemm_texts': '<PH> предлагать оригинальный подарок для малыш!',\n",
       "  'nsubj': 'я'},\n",
       " {'orig_texts': 'я обезательно перезвоню в любом случае.',\n",
       "  'lemm_texts': '<PH> обезательно перезвонить в любой случай.',\n",
       "  'nsubj': 'я'},\n",
       " {'orig_texts': 'цены на память я не помню.',\n",
       "  'lemm_texts': 'цена на память <PH> не помнить.',\n",
       "  'nsubj': 'я'},\n",
       " {'orig_texts': 'я не помню, где находились.',\n",
       "  'lemm_texts': '<PH> не помнить, где находиться.',\n",
       "  'nsubj': 'я'},\n",
       " {'orig_texts': 'я работаю на высококачественных американских материалах.',\n",
       "  'lemm_texts': '<PH> работать на высококачественный американский материал.',\n",
       "  'nsubj': 'я'}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "reserved-influence",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 358502/358502 [00:00<00:00, 445856.68it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(data_path + 'dataset.csv', 'w', encoding='utf-8') as f:\n",
    "    csv_writer = csv.writer(f, delimiter=',')\n",
    "    csv_writer.writerow(['orig_texts', 'lemm_texts', 'nsubj'])\n",
    "    for item in tqdm(dataset):\n",
    "        csv_writer.writerow(item.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metropolitan-basin",
   "metadata": {},
   "source": [
    "### Токенизация предложений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "friendly-summer",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts = [nltk.tokenize.wordpunct_tokenize(text.lower()) for text in tqdm(lemm_texts)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunset-description",
   "metadata": {},
   "source": [
    "### Определяем класс словаря, в котором каждому токену ставится в соответствие число"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "measured-pontiac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, tokens, unk_index):\n",
    "        self._tokens = tokens\n",
    "        self._token_to_id = { token: i for i, token in enumerate(tokens) }\n",
    "        self._unk_index = unk_index\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self._tokens)\n",
    "    \n",
    "    def word_to_id(self, word):\n",
    "        return self._token_to_id.get(word, self._unk_index)\n",
    "    \n",
    "    def id_to_word(self, idx):\n",
    "        return self._tokens[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fixed-labor",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
