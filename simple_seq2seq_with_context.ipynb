{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Импорт необходимых зависимостей","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport nltk\nimport torch\nimport torch.nn as nn\nimport torch.optim\nimport time\nimport pickle\nimport torch.nn.functional as F\n\nfrom random import random, sample\nfrom typing import List\nfrom collections import Counter\nfrom itertools import chain\nfrom functools import reduce\nfrom tqdm.auto import tqdm\nfrom sklearn import model_selection\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelBinarizer\nfrom torch.utils.data import DataLoader, TensorDataset","metadata":{"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"## Подготовка данных","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/seq2seq-dataset/dataset.csv')","metadata":{"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"                                               lemm_texts  \\\n0            я предлагать оригинальный подарок для малыш!   \n1              я обезательный перезвонить в любой случай.   \n2                            цена на память я не помнить.   \n3                          я не помнить , где находиться.   \n4       я работать на высококачественный американский ...   \n...                                                   ...   \n356967  другой ящерица медленно подбрести к свой товарка.   \n356968     зелёный ящерица застылый на мраморный ступень.   \n356969                 больший ящерица шмыгнуть по песок.   \n356970     домашний ящерица быстро пробежать вдоль штора.   \n356971                  крошечный ящерка сбежать с валун.   \n\n                                               orig_texts    nsubj     gender  \\\n0            я предлагаю оригинальный подарок для малыша!        я  undefined   \n1                 я обезательно перезвоню в любом случае.        я  undefined   \n2                              цены на память я не помню.        я  undefined   \n3                             я не помню, где находились.        я  undefined   \n4       я работаю на высококачественных американских м...        я  undefined   \n...                                                   ...      ...        ...   \n356967  другая ящерица медленно подбрела к своей товарке.  ящерица        fem   \n356968      зеленая ящерица застыла на мраморной ступени.  ящерица        fem   \n356969                 большая ящерица шмыгнула по песку.  ящерица        fem   \n356970      домашняя ящерица быстро пробежала вдоль штор.  ящерица        fem   \n356971                 крошечная ящерка сбежала с валуна.   ящерка        fem   \n\n       tense number  \n0       pres   sing  \n1        fut   sing  \n2       pres   sing  \n3       pres   sing  \n4       pres   sing  \n...      ...    ...  \n356967  past   sing  \n356968  past   sing  \n356969  past   sing  \n356970  past   sing  \n356971  past   sing  \n\n[356972 rows x 6 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lemm_texts</th>\n      <th>orig_texts</th>\n      <th>nsubj</th>\n      <th>gender</th>\n      <th>tense</th>\n      <th>number</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>я предлагать оригинальный подарок для малыш!</td>\n      <td>я предлагаю оригинальный подарок для малыша!</td>\n      <td>я</td>\n      <td>undefined</td>\n      <td>pres</td>\n      <td>sing</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>я обезательный перезвонить в любой случай.</td>\n      <td>я обезательно перезвоню в любом случае.</td>\n      <td>я</td>\n      <td>undefined</td>\n      <td>fut</td>\n      <td>sing</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>цена на память я не помнить.</td>\n      <td>цены на память я не помню.</td>\n      <td>я</td>\n      <td>undefined</td>\n      <td>pres</td>\n      <td>sing</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>я не помнить , где находиться.</td>\n      <td>я не помню, где находились.</td>\n      <td>я</td>\n      <td>undefined</td>\n      <td>pres</td>\n      <td>sing</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>я работать на высококачественный американский ...</td>\n      <td>я работаю на высококачественных американских м...</td>\n      <td>я</td>\n      <td>undefined</td>\n      <td>pres</td>\n      <td>sing</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>356967</th>\n      <td>другой ящерица медленно подбрести к свой товарка.</td>\n      <td>другая ящерица медленно подбрела к своей товарке.</td>\n      <td>ящерица</td>\n      <td>fem</td>\n      <td>past</td>\n      <td>sing</td>\n    </tr>\n    <tr>\n      <th>356968</th>\n      <td>зелёный ящерица застылый на мраморный ступень.</td>\n      <td>зеленая ящерица застыла на мраморной ступени.</td>\n      <td>ящерица</td>\n      <td>fem</td>\n      <td>past</td>\n      <td>sing</td>\n    </tr>\n    <tr>\n      <th>356969</th>\n      <td>больший ящерица шмыгнуть по песок.</td>\n      <td>большая ящерица шмыгнула по песку.</td>\n      <td>ящерица</td>\n      <td>fem</td>\n      <td>past</td>\n      <td>sing</td>\n    </tr>\n    <tr>\n      <th>356970</th>\n      <td>домашний ящерица быстро пробежать вдоль штора.</td>\n      <td>домашняя ящерица быстро пробежала вдоль штор.</td>\n      <td>ящерица</td>\n      <td>fem</td>\n      <td>past</td>\n      <td>sing</td>\n    </tr>\n    <tr>\n      <th>356971</th>\n      <td>крошечный ящерка сбежать с валун.</td>\n      <td>крошечная ящерка сбежала с валуна.</td>\n      <td>ящерка</td>\n      <td>fem</td>\n      <td>past</td>\n      <td>sing</td>\n    </tr>\n  </tbody>\n</table>\n<p>356972 rows × 6 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# def fit_contexts(df):\n#     contexts = []\n#     for nsubj, gender, tense in zip(df['nsubj'], df['gender'], df['tense']):\n#         context = []\n#         context.append(nsubj)\n\n#         if gender == 'undefined': context.append(0)\n#         elif gender == 'masc': context.append(1)\n#         elif gender == 'fem': context.append(2)\n#         else: context.append(3)\n\n#         if tense == 'past': context.append(0)\n#         elif tense == 'pres': context.append(1)\n#         elif tense == 'fut': context.append(2)    \n#         else: context.append(3)\n#         contexts.append(context)\n        \n#     return contexts","metadata":{"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"### Определение классов словаря и трансформера текста","metadata":{}},{"cell_type":"code","source":"class Vocab:\n    def __init__(self, tokens: List[str], unk_idx: int):\n        self._tokens = tokens\n        self._token_to_idx = {token: idx for idx, token in enumerate(tokens)}\n        self._unk_idx = unk_idx\n        \n    def token_to_idx(self, token: str) -> int:\n        return self._token_to_idx.get(token, self._unk_idx)\n    \n    def idx_to_token(self, idx: int) -> str:\n        return self._tokens[idx]","metadata":{"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"class TextTransformer:\n    def __init__(self, vocab_size: int):\n        self.vocab = None\n        self.vocab_size = vocab_size\n        self.special_tokens_to_idx = {'<UNK>': 0, '<PAD>': 1, '<SOS>': 2, '<EOS>': 3}\n        self._tokenizer = nltk.tokenize.wordpunct_tokenize\n    \n    def tokenize(self, text) -> List[str]:\n        return self._tokenizer(text.lower())\n    \n    def build_vocab(self, tokens: List[str]):\n        tokens_ = [special_token for special_token in self.special_tokens_to_idx.keys()]\n        special_tokens_amount = len(self.special_tokens_to_idx)\n        \n        for token, _ in Counter(tokens).most_common(self.vocab_size - special_tokens_amount):\n            tokens_.append(token)\n        \n        unk_idx = self.special_tokens_to_idx.get('<UNK>')\n        self.vocab = Vocab(tokens_, unk_idx)\n        \n    def transform_text(self, text: str) -> List[int]:\n        tokenized_text = self.tokenize(text)\n        transformed = [self.vocab.token_to_idx(token) for token in tokenized_text]\n        return transformed\n    \n    def fit_transform(self, texts: List[str]) -> None:\n        transformed_texts = []\n        \n        tokenized_texts = [self.tokenize(text) for text in tqdm(texts, 'Tokenizing texts')]\n        tokens = chain(*tokenized_texts)\n        self.build_vocab(tokens)\n        \n        for tokenized_text in tqdm(tokenized_texts, 'Transforming texts'):\n            transformed = [self.vocab.token_to_idx(token) for token in tokenized_text]\n            transformed_texts.append(transformed)\n    \n    def transform_texts(self, texts: List[str]) -> List[List[int]]:\n        transformed_texts = [transform_text(text) for text in tqdm(texts, 'Transforming texts')]\n        return transformed_texts\n    \n    def text_to_tensor(self, text: str, max_seq_len=8) -> torch.tensor:\n        transformed_text = self.transform_text(text)\n        pad_idx = self.special_tokens_to_idx.get('<PAD>')\n        sos_idx = self.special_tokens_to_idx.get('<SOS>')\n        eos_idx = self.special_tokens_to_idx.get('<EOS>')\n        \n        pad_size = 0\n        if len(transformed_text) >= max_seq_len:\n            transformed_text = transformed_text[:max_seq_len]\n        else:\n            pad_size = max_seq_len - len(transformed_text)\n            transformed_text.extend([pad_idx] * pad_size)   \n        transformed_text.insert(0, sos_idx)\n        transformed_text.insert(len(transformed_text) - pad_size, eos_idx)\n        \n        tensor = torch.tensor(transformed_text, dtype=torch.long)\n        return tensor.unsqueeze(0)\n    \n    def texts_to_tensor(self, texts: List[str], max_seq_len=8) -> torch.tensor:\n        pad_idx = self.special_tokens_to_idx.get('<PAD>')\n        sos_idx = self.special_tokens_to_idx.get('<SOS>')\n        eos_idx = self.special_tokens_to_idx.get('<EOS>')\n        transformed_texts = []\n        \n        for text in tqdm(texts, 'Building tensor'):\n            transformed_text = self.transform_text(text)\n            pad_size = 0\n            if len(transformed_text) >= max_seq_len:\n                transformed_text = transformed_text[:max_seq_len]\n            else:\n                pad_size = max_seq_len - len(transformed_text)\n                transformed_text.extend([pad_idx] * pad_size)   \n            transformed_text.insert(0, sos_idx)\n            transformed_text.insert(len(transformed_text) - pad_size, eos_idx)\n            transformed_texts.append(transformed_text)\n        \n        tensor = torch.tensor(transformed_texts, dtype=torch.long).permute(1, 0)\n        return tensor","metadata":{"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"### Разбиение данных на обучающую, тестовую и валидационную выборки","metadata":{}},{"cell_type":"code","source":"train_df, test_df = model_selection.train_test_split(df, test_size=0.1)","metadata":{"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"test_df, val_df = model_selection.train_test_split(test_df, test_size=0.25)","metadata":{"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"### Токенизация текстов и индексация токенов","metadata":{}},{"cell_type":"code","source":"lemm_vocab_size = 35000\norig_vocab_size = 60000","metadata":{"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"lemm_text_transformer = TextTransformer(lemm_vocab_size)\norig_text_transformer = TextTransformer(orig_vocab_size)","metadata":{"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"lemm_text_transformer.fit_transform(train_df.lemm_texts)","metadata":{"trusted":true},"execution_count":42,"outputs":[{"output_type":"display_data","data":{"text/plain":"Tokenizing texts:   0%|          | 0/321274 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f488a640601e4583abfccbc3680fab83"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Transforming texts:   0%|          | 0/321274 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a03923d370c6440d8a1dfb0938af6b61"}},"metadata":{}}]},{"cell_type":"code","source":"orig_text_transformer.fit_transform(train_df.orig_texts)","metadata":{"trusted":true},"execution_count":43,"outputs":[{"output_type":"display_data","data":{"text/plain":"Tokenizing texts:   0%|          | 0/321274 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd8e75d4b4fc418c80c785f45136cb75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Transforming texts:   0%|          | 0/321274 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"583a56f2ed2f4579af081186c7d47960"}},"metadata":{}}]},{"cell_type":"markdown","source":"### Перевод данных в тензоры","metadata":{}},{"cell_type":"code","source":"train_lemm_tensor = lemm_text_transformer.texts_to_tensor(train_df.lemm_texts.to_list())\ntest_lemm_tensor = lemm_text_transformer.texts_to_tensor(test_df.lemm_texts.to_list())\nval_lemm_tensor = lemm_text_transformer.texts_to_tensor(val_df.lemm_texts.to_list())","metadata":{"trusted":true},"execution_count":44,"outputs":[{"output_type":"display_data","data":{"text/plain":"Building tensor:   0%|          | 0/321274 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15afa252eda44577b94fa9012ca58a12"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Building tensor:   0%|          | 0/26773 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e7abeb255964392bff89f138fb12ebc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Building tensor:   0%|          | 0/8925 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad4dcad1dee140ce9601f70f4874619d"}},"metadata":{}}]},{"cell_type":"code","source":"train_orig_tensor = orig_text_transformer.texts_to_tensor(train_df.orig_texts.to_list())\ntest_orig_tensor = orig_text_transformer.texts_to_tensor(test_df.orig_texts.to_list())\nval_orig_tensor = orig_text_transformer.texts_to_tensor(val_df.orig_texts.to_list())","metadata":{"trusted":true},"execution_count":45,"outputs":[{"output_type":"display_data","data":{"text/plain":"Building tensor:   0%|          | 0/321274 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de84997a270f43f4af888d25b2926569"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Building tensor:   0%|          | 0/26773 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56fb697b2c2546cb990d2d77e80f0248"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Building tensor:   0%|          | 0/8925 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"238a422b9cad456096ede6cddfc15532"}},"metadata":{}}]},{"cell_type":"code","source":"def transform_context(df, df_type: str):\n    gender_rows = pd.get_dummies(df.gender).iterrows()\n    tense_rows = pd.get_dummies(df.tense).iterrows()\n    nsubj_to_idx = orig_text_transformer.vocab.token_to_idx\n    \n    transformed_genders = [row[1].to_list() for row in tqdm(gender_rows, f'Transforming genders ({df_type})')]\n    transformed_tenses = [row[1].to_list() for row in tqdm(tense_rows, f'Transforming tenses ({df_type})')]\n    transformed_nsubjes = [nsubj_to_idx(nsubj) for nsubj in tqdm(df.nsubj, f'Transforming nsubjes ({df_type})')]\n    \n    context = [transformed_nsubjes, transformed_genders, transformed_tenses]\n    return context","metadata":{"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"train_context = transform_context(train_df, 'train')\ntest_context = transform_context(test_df, 'test')\nval_context = transform_context(val_df, 'validation')","metadata":{"trusted":true},"execution_count":47,"outputs":[{"output_type":"display_data","data":{"text/plain":"Transforming genders (train): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4def8fbb14714414a827174f3d8cd545"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Transforming tenses (train): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2923fffd6387495facd1f75840616137"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Transforming nsubjes (train):   0%|          | 0/321274 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ebd1be0a72b4e868e1087a5b88b8116"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Transforming genders (test): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d319c475cdb444c1bcdd9ff008d9f30c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Transforming tenses (test): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40034099f7274bf091ecc79008dbb4e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Transforming nsubjes (test):   0%|          | 0/26773 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89f945bd64ff429c907a30cd29e3d903"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Transforming genders (validation): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef11f0859c944bc6a8f884d12bd2489a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Transforming tenses (validation): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35c6c605869c403f8534f0b6d293e7fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Transforming nsubjes (validation):   0%|          | 0/8925 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5569a598f6e14ea4bb70e9a7a8007f5d"}},"metadata":{}}]},{"cell_type":"code","source":"def context_to_tensors(nsubj_list, gender_list, tense_list):\n    nsubj_tensor = torch.tensor(nsubj_list)\n    gender_tensor = torch.tensor(gender_list, dtype=torch.float32)\n    tense_tensor = torch.tensor(tense_list, dtype=torch.float32)\n    \n    context_tensors = [nsubj_tensor, gender_tensor, tense_tensor]\n    return context_tensors","metadata":{"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"train_context_tensors = context_to_tensors(*train_context)\ntest_context_tensors = context_to_tensors(*test_context)\nval_context_tensors = context_to_tensors(*val_context)","metadata":{"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"# transformed_train_contexts = transform_contexts(train_contexts)\n# transformed_train_contexts_tensor = torch.tensor(transformed_train_contexts, dtype=torch.float32)\n\n# transformed_test_contexts = transform_contexts(test_contexts)\n# transformed_test_contexts_tensor = torch.tensor(transformed_test_contexts, dtype=torch.long)\n\n# transformed_val_contexts = transform_contexts(val_contexts)\n# transformed_val_contexts_tensor = torch.tensor(transformed_val_contexts, dtype=torch.long)","metadata":{"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"# transformed_train_contexts_tensor.shape","metadata":{"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"def cut_to_fit_batch(tensor: torch.Tensor, batch_size: int):\n    n_samples = tensor.shape[1]\n    new_n_samples = (n_samples // batch_size) * batch_size\n    result, _ = tensor.split(new_n_samples, dim=1)\n    return torch.transpose(result, 1, 0)","metadata":{"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"# class ContextMem(nn.Module):\n#     def __init__(self, input_dim, output_dim, hidden_ff):\n#         super(ContextMem, self).__init__()\n#         self.fc_norm = nn.Linear(input_dim, hidden_ff)\n#         self.hff     = nn.Linear(hidden_ff, output_dim)\n#         self.fc_gate = nn.Linear(output_dim, output_dim)\n\n#     def forward(self, context):\n#         #context shape = (batch_size, input_dim=3)\n#         context = self.fc_norm(context)\n#         #context shape = (batch_size, hidden_ff)\n#         context = self.hff(context)\n#         #context shape = (batch_size, output_dim)\n#         context_norm = F.tanh(context)\n        \n#         #context shape = (batch_size, output_dim)\n#         context_gate = self.fc_gate(context)\n#         #context_gate shape = (batch_size, output_dim)\n#         context_gate = F.sigmoid(context)\n\n#         return context_norm * context_gate","metadata":{"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"## Построение модели","metadata":{}},{"cell_type":"code","source":"class ContextMem(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size, nsubj_embedding_size, device):\n        super(ContextMem, self).__init__()\n        \n        self.device = device\n        \n        self.gender_proj = nn.Linear(input_size, hidden_size, bias=False)\n        self.tense_proj = nn.Linear(input_size, hidden_size, bias=False)\n        self.fc_out = nn.Linear(hidden_size * 2 + nsubj_embedding_size, output_size, bias=False)\n        \n    def forward(self, nsubj_embedding, gender, tense):\n        # nsubj_embedding_shape: (batch_size, embedding_size)\n        # gender_shape: (batch_size, input_size)\n        # tense_shape: (batch_size, input_size)\n        \n        gender = self.gender_proj(gender)\n        # gender_shape: (batch_size, hidden_size)\n        \n        tense = self.tense_proj(tense)\n        # tense_shape: (batch_size, hidden_size)    \n        \n        context = torch.cat([nsubj_embedding, gender, tense], dim=1)\n        # context_shape: (batch_size, hidden_size * 2 + embedding_size) \n        \n        context = self.fc_out(context)\n        # context_shape: (batch_size, output_size)\n        \n        return context","metadata":{"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"class EncoderRNN(nn.Module):\n    def __init__(self,\n                 vocab_size, embedding_size, hidden_size,\n                 pad_idx, device, num_layers, dropout_p):\n        \n        super(EncoderRNN, self).__init__()\n        \n        self.device = device\n        self.num_layers = num_layers\n        \n        self.hidden_size = hidden_size\n        \n        self.embedding = nn.Sequential(\n            nn.Embedding(vocab_size, embedding_size, pad_idx),\n            nn.Dropout(dropout_p)\n        )\n        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout_p)\n        \n    def forward(self, x, hidden, cell):\n        # x_shape: (seq_len, batch_size)\n        embedding = self.embedding(x)\n        # embedding_shape: (seq_len, batch_size, embedding_size)\n        output, (hidden, cell) = self.lstm(embedding, (hidden, cell))\n        # output_shape: (seq_len, batch_size, hidden_size)\n        # hidden_shape: (num_layers, batch_size, hidden_size)\n        # cell_shape: (num_layers, batch_size, hidden_size)\n        return hidden, cell","metadata":{"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"class DecoderRNN(nn.Module):\n    def __init__(self,\n                 vocab_size, embedding_size, hidden_size, output_size,\n                 pad_idx, device, num_layers, dropout_p):\n        \n        super(DecoderRNN, self).__init__()\n        \n        self.device = device\n        \n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        \n        self.embedding = nn.Sequential(\n            nn.Embedding(vocab_size, embedding_size, pad_idx),\n            nn.Dropout(dropout_p)\n        )\n        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout_p)\n        self.fc = nn.Linear(hidden_size, output_size)\n        \n    def forward(self, x, hidden, cell):\n        x = x.unsqueeze(0)\n        # x_shape:      (seq_len=1, batch_size)\n        # hidden_shape: (num_layers, batch_size, hidden_size)\n        # cell_shape:   (num_layers, batch_size, hidden_size)\n        \n        embedding = self.embedding(x)\n        # embedding_shape: (seq_len=1, batch_size, embedding_size)\n        \n        lstm_out, (hidden, cell) = self.lstm(embedding, (hidden, cell))\n        # lstm_out_shape: (seq_len=1, batch_size, hidden_size)\n        \n        fc_out = self.fc(lstm_out)\n        # fc_out_shape: (seq_len=1, batch_size, output_size)\n        # output_shape: (seq_len=1, batch_size, output_size)\n        \n        return fc_out, hidden, cell","metadata":{"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"class Seq2SeqModel(nn.Module):\n    def __init__(self,\n                 encoder_vocab_size, decoder_vocab_size,\n                 embedding_size, hidden_size, output_size,\n                 context_input_size, context_hidden_size, context_output_size,\n                 pad_idx, device, num_layers, dropout_p):\n        \n        super(Seq2SeqModel, self).__init__()\n        \n        self.device = device\n        \n        self.num_layers = num_layers\n        self.decoder_vocab_size = decoder_vocab_size\n        \n        self.context_mem = ContextMem(context_input_size, context_hidden_size, context_output_size, embedding_size, device).to(device)\n        self.encoder = EncoderRNN(encoder_vocab_size, embedding_size, hidden_size, pad_idx, device, num_layers, dropout_p).to(device)\n        self.decoder = DecoderRNN(decoder_vocab_size, embedding_size, hidden_size, output_size, pad_idx, device, num_layers, dropout_p).to(device)\n        \n    def forward(self, input, target, context, teacher_forcing_ratio=0.5):\n        batch_size = input.shape[1]\n        target_len = target.shape[0]\n        target_vocab_size = self.decoder_vocab_size\n        \n        outputs = torch.zeros(target_len, batch_size, target_vocab_size, device=self.device)\n\n        nsubj, gender, tense = context\n        # nsubj_shape:  (batch_size)\n        # gender_shape: (batch_size, context_input_size)\n        # tense_shape:  (batch_size, context_input_size)\n        \n        nsubj_embedding = self.decoder.embedding(nsubj).squeeze(0)\n        # nsubj_embedding_shape: (batch_size, embedding_size)\n        \n        hidden = self.context_mem(nsubj_embedding, gender, tense)\n        cell = hidden.clone()\n        # hidden, cell shapes: (batch_size, context_output_size=hidden_size)\n        \n        if self.num_layers == 1:\n            hidden.unsqueeze_(0)\n            cell.unsqueeze_(0)\n            # hidden, cell shapes: (1, batch_size, context_output_size=hidden_size)\n        else:\n            hidden = torch.cat([hidden.unsqueeze(0)] * self.num_layers, 0)\n            cell = torch.cat([cell.unsqueeze(0)] * self.num_layers, 0)\n            # hidden, cell shapes: (num_layers, batch_size, context_output_size=hidden_size)\n        \n        hidden, cell = self.encoder(input, hidden, cell)\n        # hidden, cell shapes: (num_layers, batch_size, hidden_size)\n        \n        prev_token_idx = target[0]\n        # prev_token_shape: (batch_size)\n        \n        for t in range(1, target_len):\n            output, hidden, cell = self.decoder(prev_token_idx, hidden, cell)\n            outputs[t] = output.squeeze(0)\n            \n            best_prediction = outputs[t].argmax(dim=1)\n            # best_prediction_shape: (batch_size)\n            prev_token_idx = target[t] if random() < teacher_forcing_ratio else best_prediction\n        \n        return outputs","metadata":{"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"markdown","source":"## Обучение модели","metadata":{}},{"cell_type":"markdown","source":"### Функция сохранения текущего состояния модели","metadata":{}},{"cell_type":"code","source":"def save_model(model: Seq2SeqModel, optimizer, epoch, path):\n    checkpoint = {\n        'encoder_state_dict': model.state_dict(),\n        'decoder_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'criterion': criterion,\n        'epoch': epoch\n    }\n    \n    torch.save(checkpoint, path)\n#     with open(path, mode='wb') as f:\n#         pickle.dump(checkpoint, f)","metadata":{"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":"### Функция загрузки уже тренировавшейся модели","metadata":{}},{"cell_type":"code","source":"def load_model(model: Seq2SeqModel, optimizer, criterion, path, for_inference=True):\n#     with open(path, mode='rb') as f:\n#         checkpoint = pickle.load(f)\n    checkpoint = torch.load(path)\n    \n    model.encoder.load_state_dict(checkpoint['encoder_state_dict'])\n    model.decoder.load_state_dict(checkpoint['decoder_state_dict'])\n    \n    if not for_inference:\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        epoch = checkpoint['epoch']\n        criterion = checkpoint['criterion']\n\n        return epoch","metadata":{"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"markdown","source":"### Инициализация гиперпараметров","metadata":{}},{"cell_type":"code","source":"learning_rate = 0.001\nbatch_size = 256\nepochs_amount = 15\nlemm_vocab_size = 35000\norig_vocab_size = 60000\nhidden_size = 1024\nembedding_size = 300\nnum_layers = 2\nmax_norm = 1.0\ndropout_p = 0.6\ncontext_input_size = 4\ncontext_hidden_size = 512\ncontext_output_size = hidden_size\npatience = 3\noutput_size = orig_vocab_size\npad_idx = lemm_text_transformer.special_tokens_to_idx.get('<PAD>')\nmodel_path = './'\nmodel_name = 'simple_seq2seq_with_context.model'","metadata":{"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"model = Seq2SeqModel(lemm_vocab_size, orig_vocab_size, embedding_size, hidden_size, output_size,\n                     context_input_size, context_hidden_size, context_output_size,\n                     pad_idx, device, num_layers, dropout_p).to(device)","metadata":{"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)","metadata":{"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)","metadata":{"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"try:\n    epoch = load_model(model, optimizer, criterion, model_path + model_name)\n    print(f'Loaded model from {model_path}')\nexcept:\n    print(f'No models found at {model_path}')\n    epoch = 1","metadata":{"trusted":true},"execution_count":65,"outputs":[{"name":"stdout","text":"No models found at ./models/\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Урезание данных для соответствия размеру батча","metadata":{}},{"cell_type":"code","source":"train_lemm_tensor_f = cut_to_fit_batch(train_lemm_tensor, batch_size)\ntrain_orig_tensor_f = cut_to_fit_batch(train_orig_tensor, batch_size)\n\ntest_lemm_tensor_f = cut_to_fit_batch(test_lemm_tensor, batch_size)\ntest_orig_tensor_f = cut_to_fit_batch(test_orig_tensor, batch_size)\n\nval_lemm_tensor_f = cut_to_fit_batch(val_lemm_tensor, batch_size)\nval_orig_tensor_f = cut_to_fit_batch(val_orig_tensor, batch_size)","metadata":{"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"train_context_tensors_f = [cut_to_fit_batch(tensor.unsqueeze(0), batch_size).squeeze(1) for tensor in train_context_tensors]\ntest_context_tensors_f = [cut_to_fit_batch(tensor.unsqueeze(0), batch_size).squeeze(1) for tensor in test_context_tensors]\nval_context_tensors_f = [cut_to_fit_batch(tensor.unsqueeze(0), batch_size).squeeze(1) for tensor in val_context_tensors]","metadata":{"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"markdown","source":"### Инициализация данных итерируемых по батчам","metadata":{}},{"cell_type":"code","source":"train_dataset = TensorDataset(train_lemm_tensor_f, train_orig_tensor_f, *train_context_tensors_f)\ntest_dataset = TensorDataset(test_lemm_tensor_f, test_orig_tensor_f, *test_context_tensors_f)\nval_dataset = TensorDataset(val_lemm_tensor_f, val_orig_tensor_f, *val_context_tensors_f)","metadata":{"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=batch_size)\ntest_loader = DataLoader(test_dataset, batch_size=1)\nval_loader = DataLoader(val_dataset, batch_size=batch_size)","metadata":{"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"markdown","source":"### Определение функции проверки работы сети между эпохами обучения","metadata":{}},{"cell_type":"code","source":"def test_evaluate(model, input, context, target_len=8):\n    with torch.no_grad():\n        model.eval()\n        \n        input = input.to(device)\n\n        nsubj, gender, tense = context\n        nsubj_embedding = model.decoder.embedding(nsubj)\n\n        hidden = model.context_mem(nsubj_embedding, gender, tense)\n        cell = hidden.clone()\n\n        if model.num_layers == 1:\n            hidden.unsqueeze_(0)\n            cell.unsqueeze_(0)\n            # hidden, cell shapes: (1, batch_size, context_output_size=hidden_size)\n        else:\n            hidden = torch.cat([hidden.unsqueeze(0)] * model.num_layers, 0)\n            cell = torch.cat([cell.unsqueeze(0)] * model.num_layers, 0)\n            # hidden, cell shapes: (num_layers, batch_size, context_output_size=hidden_size)\n\n        sos_idx = lemm_text_transformer.special_tokens_to_idx.get('<SOS>')\n        eos_idx = lemm_text_transformer.special_tokens_to_idx.get('<EOS>')\n\n        hidden, cell = model.encoder(input, hidden, cell)\n\n        predicted_indexes = [sos_idx]\n\n        for _ in range(1, target_len):\n            prev_idx = torch.tensor([predicted_indexes[-1]], dtype=torch.long, device=device)\n\n            output, hidden, cell = model.decoder(prev_idx, hidden, cell)\n            output = output.squeeze(0)\n\n            best_prediction = output.argmax(dim=1).item()\n\n            if best_prediction == eos_idx:\n                break\n\n            predicted_indexes.append(best_prediction)\n\n\n        predicted_tokens = [orig_text_transformer.vocab.idx_to_token(idx) for idx in predicted_indexes]\n        return predicted_tokens[1:]","metadata":{"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"markdown","source":"### Определение функции обучения сети","metadata":{}},{"cell_type":"code","source":"def train(model, optimizer, criterion, train_data, val_data, test_data, epochs_amount, max_norm, context, patience=3, current_epoch=1, n_prints=5):\n    min_mean_val_loss = float('+inf')\n    initial_patiece = patience\n    print_every = len(train_data) // n_prints\n    \n    for epoch in tqdm(range(current_epoch, epochs_amount + 1), 'Epochs'):\n        print(f'\\nEpoch [{epoch} / {epochs_amount}]')\n        \n        model.train()\n        for iteration, (input, target, nsubj, gender, tense) in enumerate(tqdm(train_data, 'Epoch training iterations')):\n            optimizer.zero_grad()\n            \n            input = torch.transpose(input, 1, 0).to(device)   \n            # input_shape: (seq_len, batch_size)\n            \n            target = torch.transpose(target, 1, 0).to(device)\n            # target_shape: (seq_len, batch_size)\n            \n            context = (nsubj.to(device), gender.to(device), tense.to(device))\n            \n            output = model(input, target, context)\n            # output_shape: (seq_len, batch_size, orig_vocab_size) but need (N, orig_vocab_size)\n            \n            target = target[1:].reshape(-1)\n            # now target_shape is (seq_len * batch_size)\n            \n            orig_vocab_size = output.shape[2]\n            \n            output = output[1:].reshape(-1, orig_vocab_size)\n            # now output_shape is (seq_len * batch_size, orig_vocab_size)\n            \n            loss = criterion(output, target)\n            loss.backward()\n            \n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_norm)\n            \n            optimizer.step()\n            \n            if iteration % print_every == 0:\n                print(f'\\tIteration #{iteration}: training loss = {loss.item()}')\n            elif iteration == len(train_data):\n                print(f'\\tIteration #{iteration}: training loss = {loss.item()}')\n            \n            \n        with torch.no_grad():\n            model.eval()\n            val_loss = []\n            \n            for input, target, nsubj, gender, tense in tqdm(val_data, 'Epoch validating iterations'):\n                input = torch.transpose(input, 1, 0).to(device)\n                target = torch.transpose(target, 1, 0).to(device)\n                context = (nsubj.to(device), gender.to(device), tense.to(device))\n                output = model(input, target, context)\n                \n                orig_vocab_size = output.shape[2]\n                output = output[1:].reshape(-1, orig_vocab_size)\n                target = target[1:].reshape(-1)\n                \n                val_loss.append(criterion(output, target).item())\n            \n            mean_val_loss = sum(val_loss) / len(val_loss)\n            print(f'\\tValidation loss = {mean_val_loss}')\n            if mean_val_loss < min_mean_val_loss:\n                try:\n                    save_model(model, optimizer, epoch, model_path + model_name)\n                    min_mean_val_loss = mean_val_loss\n                    patience = initial_patiece\n                except Exception as exc:\n                    print(exc)\n            else:\n                patience -= 1\n            \n            test_data = DataLoader(test_data.dataset, batch_size=1, shuffle=True)\n            for input, target, nsubj, gender, tense in test_data:\n                target = target.squeeze(0).to(device)\n                context = (nsubj.to(device), gender.to(device), tense.to(device))\n                \n                input = torch.transpose(input, 1, 0)\n                target_len = target.shape[0]\n                \n                output = test_evaluate(model, input, context, target_len)\n                decoded_input = [lemm_text_transformer.vocab.idx_to_token(idx.item()) for idx in input]\n                decoded_target = [orig_text_transformer.vocab.idx_to_token(idx.item()) for idx in target]\n                \n                print(f'\\tInput: {decoded_input}')\n                print(f'\\tOutput: {output}')\n                print(f'\\tTarget: {decoded_target}')\n                break\n        \n        if patience == 0:\n            print(f'\\nModel learning finished due to early stopping')\n            break\n","metadata":{"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"markdown","source":"### Определение функции эксплуатации обученной модели","metadata":{}},{"cell_type":"code","source":"def evaluate(model: Seq2SeqModel, sentence: str, max_seq_len=8):\n    input_tensor = lemm_text_transformer.text_to_tensor(sentence, max_seq_len).to(device)\n    input_tensor = torch.transpose(input_tensor, 1, 0)\n    sos_idx = lemm_text_transformer.special_tokens_to_idx.get('<SOS>')\n    eos_idx = lemm_text_transformer.special_tokens_to_idx.get('<EOS>')\n    \n    with torch.no_grad():\n        model.eval()\n        hidden, cell = model.encoder(input_tensor)\n        \n        predicted_indexes = [sos_idx]\n        \n#         while True:\n#             prev_idx = torch.tensor([predicted_indexes[-1]], dtype=torch.long, device=device)\n            \n#             output, hidden, cell = model.decoder(prev_idx, hidden, cell)\n#             output = output.squeeze(0)\n            \n#             best_prediction = output.argmax(dim=1).item()\n            \n#             if best_prediction == eos_idx:\n#                 break\n            \n#             predicted_indexes.append(best_prediction)\n                       \n        \n        for _ in range(1, max_seq_len):\n            prev_idx = torch.tensor([predicted_indexes[-1]], dtype=torch.long, device=device)\n            \n            output, hidden, cell = model.decoder(prev_idx, hidden, cell)\n            output = output.squeeze(0)\n            \n            best_prediction = output.argmax(dim=1).item()\n            \n            if best_prediction == eos_idx:\n                break\n                \n            predicted_indexes.append(best_prediction)\n        \n    predicted_tokens = [orig_text_transformer.vocab.idx_to_token(idx) for idx in predicted_indexes]\n    return predicted_tokens[1:]","metadata":{"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"train(model, optimizer, criterion, train_loader, val_loader, test_loader, epochs_amount, max_norm, patience, epoch)","metadata":{"tags":[],"trusted":true},"execution_count":80,"outputs":[{"output_type":"display_data","data":{"text/plain":"Epochs:   0%|          | 0/15 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cb1ec8ed1964ee0a53cf48c44186bc8"}},"metadata":{}},{"name":"stdout","text":"\nEpoch [1 / 15]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch training iterations:   0%|          | 0/1254 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"102f4d66cbcc48b28cb0f399a811c9ab"}},"metadata":{}},{"name":"stdout","text":"\tIteration #0: training loss = 1.2785922288894653\n\tIteration #250: training loss = 1.296160340309143\n\tIteration #500: training loss = 1.1016517877578735\n\tIteration #750: training loss = 1.0643939971923828\n\tIteration #1000: training loss = 0.9494953155517578\n\tIteration #1250: training loss = 0.9913126826286316\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch validating iterations:   0%|          | 0/34 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbdbe2c6efad4846b4e8046961146a96"}},"metadata":{}},{"name":"stdout","text":"\tValidation loss = 0.9365814503501443\n\tInput: ['<SOS>', 'я', 'буквально', 'прийти', 'в', 'ярость', '.', '<EOS>', '<PAD>', '<PAD>']\n\tOutput: ['я', 'я', 'буквально', 'пришла', 'в', 'ярость', '.']\n\tTarget: ['<SOS>', 'я', 'буквально', 'пришла', 'в', 'ярость', '.', '<EOS>', '<PAD>', '<PAD>']\n\nEpoch [2 / 15]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch training iterations:   0%|          | 0/1254 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83e652be88354f15b7320898b35d2088"}},"metadata":{}},{"name":"stdout","text":"\tIteration #0: training loss = 0.8931426405906677\n\tIteration #250: training loss = 0.8857982158660889\n\tIteration #500: training loss = 0.7826675772666931\n\tIteration #750: training loss = 0.7749985456466675\n\tIteration #1000: training loss = 0.7045783400535583\n\tIteration #1250: training loss = 0.7101231217384338\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch validating iterations:   0%|          | 0/34 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbddbfc5b6f243cb845780c1b5d5979c"}},"metadata":{}},{"name":"stdout","text":"\tValidation loss = 0.8051528474863838\n\tInput: ['<SOS>', 'я', 'ездить', 'на', 'дальний', 'восток', ';', '<EOS>', '<PAD>', '<PAD>']\n\tOutput: ['я', 'я', 'ездил', 'на', 'кровавом', 'пляжи', ';']\n\tTarget: ['<SOS>', 'я', 'ездил', 'на', 'дальний', 'восток', ';', '<EOS>', '<PAD>', '<PAD>']\n\nEpoch [3 / 15]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch training iterations:   0%|          | 0/1254 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e69e60f43ba4e5d860d3d1e0d9b441f"}},"metadata":{}},{"name":"stdout","text":"\tIteration #0: training loss = 0.6158519983291626\n\tIteration #250: training loss = 0.5816569328308105\n\tIteration #500: training loss = 0.5571786165237427\n\tIteration #750: training loss = 0.5811482071876526\n\tIteration #1000: training loss = 0.517846405506134\n\tIteration #1250: training loss = 0.5300610065460205\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch validating iterations:   0%|          | 0/34 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a2d0d49a6844488b540fded7695e29d"}},"metadata":{}},{"name":"stdout","text":"\tValidation loss = 0.7367723514051998\n\tInput: ['<SOS>', 'лайон', 'привыкнуть', 'к', 'одиночество', '.', '<EOS>', '<PAD>', '<PAD>', '<PAD>']\n\tOutput: ['не', 'лайон', 'привык', 'к', 'одиночеству', '.']\n\tTarget: ['<SOS>', 'лайон', 'привык', 'к', 'одиночеству', '.', '<EOS>', '<PAD>', '<PAD>', '<PAD>']\n\nEpoch [4 / 15]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch training iterations:   0%|          | 0/1254 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d030a98dc0c49f0b377abf5c317df36"}},"metadata":{}},{"name":"stdout","text":"\tIteration #0: training loss = 0.46785449981689453\n\tIteration #250: training loss = 0.4008624851703644\n\tIteration #500: training loss = 0.4046700596809387\n\tIteration #750: training loss = 0.4499998390674591\n\tIteration #1000: training loss = 0.4141438901424408\n\tIteration #1250: training loss = 0.4048989415168762\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch validating iterations:   0%|          | 0/34 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1bea7eef98d3415ab601c68cac02f9a3"}},"metadata":{}},{"name":"stdout","text":"\tValidation loss = 0.7036590821602765\n\tInput: ['<SOS>', 'кларк', 'отправиться', 'за', 'свой', 'багаж', '.', '<EOS>', '<PAD>', '<PAD>']\n\tOutput: ['кларк', 'кларк', 'отправился', 'за', 'своим', 'билетом', '.']\n\tTarget: ['<SOS>', 'кларк', 'отправился', 'за', 'своим', 'багажом', '.', '<EOS>', '<PAD>', '<PAD>']\n\nEpoch [5 / 15]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch training iterations:   0%|          | 0/1254 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4bf5ca99907e4b0cadd65980388632c3"}},"metadata":{}},{"name":"stdout","text":"\tIteration #0: training loss = 0.3367561399936676\n\tIteration #250: training loss = 0.3033932149410248\n\tIteration #500: training loss = 0.30409717559814453\n\tIteration #750: training loss = 0.33393025398254395\n\tIteration #1000: training loss = 0.29899927973747253\n\tIteration #1250: training loss = 0.31801632046699524\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch validating iterations:   0%|          | 0/34 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59e26e45d3a9443391535ea32af35e5e"}},"metadata":{}},{"name":"stdout","text":"\tValidation loss = 0.6798321990405812\n\tInput: ['<SOS>', 'он', 'почесать', 'в', 'борода', '.', '<EOS>', '<PAD>', '<PAD>', '<PAD>']\n\tOutput: ['он', 'он', 'почесал', 'в', 'бороду', '.']\n\tTarget: ['<SOS>', 'он', 'почесал', 'в', 'бороде', '.', '<EOS>', '<PAD>', '<PAD>', '<PAD>']\n\nEpoch [6 / 15]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch training iterations:   0%|          | 0/1254 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d1ad636287c4b5a9e14fec2d7b80172"}},"metadata":{}},{"name":"stdout","text":"\tIteration #0: training loss = 0.27230215072631836\n\tIteration #250: training loss = 0.21999263763427734\n\tIteration #500: training loss = 0.2449188083410263\n\tIteration #750: training loss = 0.29884451627731323\n\tIteration #1000: training loss = 0.2604082524776459\n\tIteration #1250: training loss = 0.25481635332107544\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch validating iterations:   0%|          | 0/34 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63dcf0d73f604305aabdf7419297b5c0"}},"metadata":{}},{"name":"stdout","text":"\tValidation loss = 0.6659175858778112\n\tInput: ['<SOS>', 'хмельницкий', 'прийти', 'в', 'страшный', 'ярость', '.', '<EOS>', '<PAD>', '<PAD>']\n\tOutput: ['хмельницкий', 'хмельницкий', 'пришел', 'в', 'страшную', 'ярость', '.']\n\tTarget: ['<SOS>', 'хмельницкий', 'пришел', 'в', 'страшную', 'ярость', '.', '<EOS>', '<PAD>', '<PAD>']\n\nEpoch [7 / 15]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch training iterations:   0%|          | 0/1254 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb1ade4a78d844569ea8386a8981378e"}},"metadata":{}},{"name":"stdout","text":"\tIteration #0: training loss = 0.20434243977069855\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-80-e37acd47fd24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs_amount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-79-b5a3b1c2fab9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, criterion, train_data, val_data, test_data, epochs_amount, max_norm, context, patience, current_epoch, n_prints)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"# load_model(model, optimizer, criterion, model_path + model_name)\n# model.eval()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import gc\n# del model\n# del optimizer\n# gc.collect()\n# torch.cuda.empty_cache()\n# gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input, target, nsubj, gender, tense = test_dataset.tensors","metadata":{"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"n = 8","metadata":{"trusted":true},"execution_count":275,"outputs":[]},{"cell_type":"code","source":"test_evaluate(model, input[n].unsqueeze(1), (nsubj[n].unsqueeze(0).to(device), gender[n].unsqueeze(0).to(device), tense[n].unsqueeze(0).to(device)))","metadata":{"trusted":true},"execution_count":276,"outputs":[{"execution_count":276,"output_type":"execute_result","data":{"text/plain":"['я', 'я', 'катаюсь', 'в', 'другом', 'месте', '.']"},"metadata":{}}]},{"cell_type":"code","source":"orig_text_transformer.vocab.idx_to_token(nsubj[n].item())","metadata":{"trusted":true},"execution_count":277,"outputs":[{"execution_count":277,"output_type":"execute_result","data":{"text/plain":"'я'"},"metadata":{}}]},{"cell_type":"code","source":"gender_labels = pd.get_dummies(test_df.gender).drop_duplicates().idxmax(1).values.tolist()","metadata":{"trusted":true},"execution_count":278,"outputs":[]},{"cell_type":"code","source":"gender_vectors = []\nfor _, vec in pd.get_dummies(test_df.gender).drop_duplicates().iterrows():\n    gender_vectors.append(vec.values.tolist())","metadata":{"trusted":true},"execution_count":279,"outputs":[]},{"cell_type":"code","source":"gender_label_to_vec = {label: vec for label, vec in zip(gender_labels, gender_vectors)}","metadata":{"trusted":true},"execution_count":280,"outputs":[]},{"cell_type":"code","source":"gender_label_to_vec","metadata":{"trusted":true},"execution_count":281,"outputs":[{"execution_count":281,"output_type":"execute_result","data":{"text/plain":"{'masc': [0, 1, 0, 0],\n 'fem': [1, 0, 0, 0],\n 'undefined': [0, 0, 0, 1],\n 'neut': [0, 0, 1, 0]}"},"metadata":{}}]},{"cell_type":"code","source":"for label, vec in gender_label_to_vec.items():\n    if vec == list(map(int, gender[n])):\n        print(label)","metadata":{"trusted":true},"execution_count":282,"outputs":[{"name":"stdout","text":"undefined\n","output_type":"stream"}]},{"cell_type":"code","source":"tense_labels = pd.get_dummies(test_df.tense).drop_duplicates().idxmax(1).values.tolist()","metadata":{"trusted":true},"execution_count":283,"outputs":[]},{"cell_type":"code","source":"tense_vectors = []\nfor _, vec in pd.get_dummies(test_df.tense).drop_duplicates().iterrows():\n    tense_vectors.append(vec.values.tolist())","metadata":{"trusted":true},"execution_count":284,"outputs":[]},{"cell_type":"code","source":"tense_label_to_vec = {label: vec for label, vec in zip(tense_labels, tense_vectors)}","metadata":{"trusted":true},"execution_count":285,"outputs":[]},{"cell_type":"code","source":"tense_label_to_vec","metadata":{"trusted":true},"execution_count":286,"outputs":[{"execution_count":286,"output_type":"execute_result","data":{"text/plain":"{'past': [0, 1, 0, 0],\n 'pres': [0, 0, 1, 0],\n 'fut': [1, 0, 0, 0],\n 'undefined': [0, 0, 0, 1]}"},"metadata":{}}]},{"cell_type":"code","source":"for label, vec in tense_label_to_vec.items():\n    if vec == list(map(int, tense[n])):\n        print(label)","metadata":{"trusted":true},"execution_count":287,"outputs":[{"name":"stdout","text":"pres\n","output_type":"stream"}]},{"cell_type":"code","source":"# test_sample = train_df.sample(100)\n# test_input = test_sample.lemm_texts.to_list()\n# test_target = test_sample.orig_texts.to_list()\n# test_pair = list(zip(test_input, test_target))","metadata":{"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"# for input_sentence, target_sentence in test_pair[:10]:\n#     model_output = evaluate(model, input_sentence)\n#     print(f'Input: {input_sentence}')\n#     print(f'Output: {model_output}')\n#     print(f'Target: {target_sentence}')\n#     print('\\n')","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}