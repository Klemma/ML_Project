{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Импорт необходимых зависимостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "import time\n",
    "import pickle\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from random import random, sample\n",
    "from typing import List\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from functools import reduce\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/simple_sentences/processed/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemm_texts</th>\n",
       "      <th>orig_texts</th>\n",
       "      <th>nsubj</th>\n",
       "      <th>gender</th>\n",
       "      <th>tense</th>\n",
       "      <th>number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>я предлагать оригинальный подарок для малыш!</td>\n",
       "      <td>я предлагаю оригинальный подарок для малыша!</td>\n",
       "      <td>я</td>\n",
       "      <td>undefined</td>\n",
       "      <td>pres</td>\n",
       "      <td>sing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>я обезательный перезвонить в любой случай.</td>\n",
       "      <td>я обезательно перезвоню в любом случае.</td>\n",
       "      <td>я</td>\n",
       "      <td>undefined</td>\n",
       "      <td>fut</td>\n",
       "      <td>sing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>цена на память я не помнить.</td>\n",
       "      <td>цены на память я не помню.</td>\n",
       "      <td>я</td>\n",
       "      <td>undefined</td>\n",
       "      <td>pres</td>\n",
       "      <td>sing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>я не помнить , где находиться.</td>\n",
       "      <td>я не помню, где находились.</td>\n",
       "      <td>я</td>\n",
       "      <td>undefined</td>\n",
       "      <td>pres</td>\n",
       "      <td>sing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>я работать на высококачественный американский ...</td>\n",
       "      <td>я работаю на высококачественных американских м...</td>\n",
       "      <td>я</td>\n",
       "      <td>undefined</td>\n",
       "      <td>pres</td>\n",
       "      <td>sing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356967</th>\n",
       "      <td>другой ящерица медленно подбрести к свой товарка.</td>\n",
       "      <td>другая ящерица медленно подбрела к своей товарке.</td>\n",
       "      <td>ящерица</td>\n",
       "      <td>fem</td>\n",
       "      <td>past</td>\n",
       "      <td>sing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356968</th>\n",
       "      <td>зелёный ящерица застылый на мраморный ступень.</td>\n",
       "      <td>зеленая ящерица застыла на мраморной ступени.</td>\n",
       "      <td>ящерица</td>\n",
       "      <td>fem</td>\n",
       "      <td>past</td>\n",
       "      <td>sing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356969</th>\n",
       "      <td>больший ящерица шмыгнуть по песок.</td>\n",
       "      <td>большая ящерица шмыгнула по песку.</td>\n",
       "      <td>ящерица</td>\n",
       "      <td>fem</td>\n",
       "      <td>past</td>\n",
       "      <td>sing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356970</th>\n",
       "      <td>домашний ящерица быстро пробежать вдоль штора.</td>\n",
       "      <td>домашняя ящерица быстро пробежала вдоль штор.</td>\n",
       "      <td>ящерица</td>\n",
       "      <td>fem</td>\n",
       "      <td>past</td>\n",
       "      <td>sing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356971</th>\n",
       "      <td>крошечный ящерка сбежать с валун.</td>\n",
       "      <td>крошечная ящерка сбежала с валуна.</td>\n",
       "      <td>ящерка</td>\n",
       "      <td>fem</td>\n",
       "      <td>past</td>\n",
       "      <td>sing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>356972 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               lemm_texts  \\\n",
       "0            я предлагать оригинальный подарок для малыш!   \n",
       "1              я обезательный перезвонить в любой случай.   \n",
       "2                            цена на память я не помнить.   \n",
       "3                          я не помнить , где находиться.   \n",
       "4       я работать на высококачественный американский ...   \n",
       "...                                                   ...   \n",
       "356967  другой ящерица медленно подбрести к свой товарка.   \n",
       "356968     зелёный ящерица застылый на мраморный ступень.   \n",
       "356969                 больший ящерица шмыгнуть по песок.   \n",
       "356970     домашний ящерица быстро пробежать вдоль штора.   \n",
       "356971                  крошечный ящерка сбежать с валун.   \n",
       "\n",
       "                                               orig_texts    nsubj     gender  \\\n",
       "0            я предлагаю оригинальный подарок для малыша!        я  undefined   \n",
       "1                 я обезательно перезвоню в любом случае.        я  undefined   \n",
       "2                              цены на память я не помню.        я  undefined   \n",
       "3                             я не помню, где находились.        я  undefined   \n",
       "4       я работаю на высококачественных американских м...        я  undefined   \n",
       "...                                                   ...      ...        ...   \n",
       "356967  другая ящерица медленно подбрела к своей товарке.  ящерица        fem   \n",
       "356968      зеленая ящерица застыла на мраморной ступени.  ящерица        fem   \n",
       "356969                 большая ящерица шмыгнула по песку.  ящерица        fem   \n",
       "356970      домашняя ящерица быстро пробежала вдоль штор.  ящерица        fem   \n",
       "356971                 крошечная ящерка сбежала с валуна.   ящерка        fem   \n",
       "\n",
       "       tense number  \n",
       "0       pres   sing  \n",
       "1        fut   sing  \n",
       "2       pres   sing  \n",
       "3       pres   sing  \n",
       "4       pres   sing  \n",
       "...      ...    ...  \n",
       "356967  past   sing  \n",
       "356968  past   sing  \n",
       "356969  past   sing  \n",
       "356970  past   sing  \n",
       "356971  past   sing  \n",
       "\n",
       "[356972 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Определение классов словаря и трансформера текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, tokens: List[str], unk_idx: int):\n",
    "        self._tokens = tokens\n",
    "        self._token_to_idx = {token: idx for idx, token in enumerate(tokens)}\n",
    "        self._unk_idx = unk_idx\n",
    "        \n",
    "    def token_to_idx(self, token: str) -> int:\n",
    "        return self._token_to_idx.get(token, self._unk_idx)\n",
    "    \n",
    "    def idx_to_token(self, idx: int) -> str:\n",
    "        return self._tokens[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTransformer:\n",
    "    def __init__(self, vocab_size: int):\n",
    "        self.vocab = None\n",
    "        self.vocab_size = vocab_size\n",
    "        self.special_tokens_to_idx = {'<UNK>': 0, '<PAD>': 1, '<SOS>': 2, '<EOS>': 3}\n",
    "        self._tokenizer = nltk.tokenize.wordpunct_tokenize\n",
    "    \n",
    "    def tokenize(self, text) -> List[str]:\n",
    "        return self._tokenizer(text.lower())\n",
    "    \n",
    "    def build_vocab(self, tokens: List[str]):\n",
    "        tokens_ = [special_token for special_token in self.special_tokens_to_idx.keys()]\n",
    "        special_tokens_amount = len(self.special_tokens_to_idx)\n",
    "        \n",
    "        for token, _ in Counter(tokens).most_common(self.vocab_size - special_tokens_amount):\n",
    "            tokens_.append(token)\n",
    "        \n",
    "        unk_idx = self.special_tokens_to_idx.get('<UNK>')\n",
    "        self.vocab = Vocab(tokens_, unk_idx)\n",
    "        \n",
    "    def transform_text(self, text: str) -> List[int]:\n",
    "        tokenized_text = self.tokenize(text)\n",
    "        transformed = [self.vocab.token_to_idx(token) for token in tokenized_text]\n",
    "        return transformed\n",
    "    \n",
    "    def fit_transform(self, texts: List[str]) -> None:\n",
    "        transformed_texts = []\n",
    "        \n",
    "        tokenized_texts = [self.tokenize(text) for text in tqdm(texts, 'Tokenizing texts')]\n",
    "        tokens = chain(*tokenized_texts)\n",
    "        self.build_vocab(tokens)\n",
    "        \n",
    "        for tokenized_text in tqdm(tokenized_texts, 'Transforming texts'):\n",
    "            transformed = [self.vocab.token_to_idx(token) for token in tokenized_text]\n",
    "            transformed_texts.append(transformed)\n",
    "    \n",
    "    def transform_texts(self, texts: List[str]) -> List[List[int]]:\n",
    "        transformed_texts = [transform_text(text) for text in tqdm(texts, 'Transforming texts')]\n",
    "        return transformed_texts\n",
    "    \n",
    "    def text_to_tensor(self, text: str, max_seq_len=8) -> torch.tensor:\n",
    "        transformed_text = self.transform_text(text)\n",
    "        pad_idx = self.special_tokens_to_idx.get('<PAD>')\n",
    "        sos_idx = self.special_tokens_to_idx.get('<SOS>')\n",
    "        eos_idx = self.special_tokens_to_idx.get('<EOS>')\n",
    "        \n",
    "        pad_size = 0\n",
    "        if len(transformed_text) >= max_seq_len:\n",
    "            transformed_text = transformed_text[:max_seq_len]\n",
    "        else:\n",
    "            pad_size = max_seq_len - len(transformed_text)\n",
    "            transformed_text.extend([pad_idx] * pad_size)   \n",
    "        transformed_text.insert(0, sos_idx)\n",
    "        transformed_text.insert(len(transformed_text) - pad_size, eos_idx)\n",
    "        \n",
    "        tensor = torch.tensor(transformed_text, dtype=torch.long)\n",
    "        return tensor.unsqueeze(0)\n",
    "    \n",
    "    def texts_to_tensor(self, texts: List[str], max_seq_len=8) -> torch.tensor:\n",
    "        pad_idx = self.special_tokens_to_idx.get('<PAD>')\n",
    "        sos_idx = self.special_tokens_to_idx.get('<SOS>')\n",
    "        eos_idx = self.special_tokens_to_idx.get('<EOS>')\n",
    "        transformed_texts = []\n",
    "        \n",
    "        for text in tqdm(texts, 'Building tensor'):\n",
    "            transformed_text = self.transform_text(text)\n",
    "            pad_size = 0\n",
    "            if len(transformed_text) >= max_seq_len:\n",
    "                transformed_text = transformed_text[:max_seq_len]\n",
    "            else:\n",
    "                pad_size = max_seq_len - len(transformed_text)\n",
    "                transformed_text.extend([pad_idx] * pad_size)   \n",
    "            transformed_text.insert(0, sos_idx)\n",
    "            transformed_text.insert(len(transformed_text) - pad_size, eos_idx)\n",
    "            transformed_texts.append(transformed_text)\n",
    "        \n",
    "        tensor = torch.tensor(transformed_texts, dtype=torch.long).permute(1, 0)\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разбиение данных на обучающую, тестовую и валидационную выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = model_selection.train_test_split(df, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df, val_df = model_selection.train_test_split(test_df, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Токенизация текстов и индексация токенов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemm_vocab_size = 35000\n",
    "orig_vocab_size = 70000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemm_text_transformer = TextTransformer(lemm_vocab_size)\n",
    "orig_text_transformer = TextTransformer(orig_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03762f4be2a74bfbad08cd91d3696a6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing texts:   0%|          | 0/321274 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d46b726f4104dbf83882625b33999db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transforming texts:   0%|          | 0/321274 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lemm_text_transformer.fit_transform(train_df.lemm_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8a0253c2aed425898806d40b2b9fc14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing texts:   0%|          | 0/321274 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10709761ac624b439c35bc1d483f23df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transforming texts:   0%|          | 0/321274 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "orig_text_transformer.fit_transform(train_df.orig_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Перевод данных в тензоры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dcf78eb967d405d8ad5fb8b3f3c5f29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building tensor:   0%|          | 0/321274 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c94b9bc8b444b818e6e5b4129372a6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building tensor:   0%|          | 0/26773 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0e58b90e33545199139c2e632c24ca5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building tensor:   0%|          | 0/8925 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_lemm_tensor = lemm_text_transformer.texts_to_tensor(train_df.lemm_texts.to_list())\n",
    "test_lemm_tensor = lemm_text_transformer.texts_to_tensor(test_df.lemm_texts.to_list())\n",
    "val_lemm_tensor = lemm_text_transformer.texts_to_tensor(val_df.lemm_texts.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e8fd770f5094249a75130f2a33ca9e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building tensor:   0%|          | 0/321274 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16688bb11d024dc596c944651ce5dba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building tensor:   0%|          | 0/26773 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3bd3faa3e584116a5066f140e33e963",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building tensor:   0%|          | 0/8925 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_orig_tensor = orig_text_transformer.texts_to_tensor(train_df.orig_texts.to_list())\n",
    "test_orig_tensor = orig_text_transformer.texts_to_tensor(test_df.orig_texts.to_list())\n",
    "val_orig_tensor = orig_text_transformer.texts_to_tensor(val_df.orig_texts.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_context(df, df_type: str):\n",
    "    gender_rows = pd.get_dummies(df.gender).iterrows()\n",
    "    tense_rows = pd.get_dummies(df.tense).iterrows()\n",
    "    nsubj_to_idx = orig_text_transformer.vocab.token_to_idx\n",
    "    \n",
    "    transformed_genders = [row[1].to_list() for row in tqdm(gender_rows, f'Transforming genders ({df_type})')]\n",
    "    transformed_tenses = [row[1].to_list() for row in tqdm(tense_rows, f'Transforming tenses ({df_type})')]\n",
    "    transformed_nsubjes = [nsubj_to_idx(nsubj) for nsubj in tqdm(df.nsubj, f'Transforming nsubjes ({df_type})')]\n",
    "    \n",
    "    context = [transformed_nsubjes, transformed_genders, transformed_tenses]\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d86368e455dc4635b1c05502ca937059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transforming genders (train): 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bdb7b33baeb44979acbfd1205174f27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transforming tenses (train): 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccd45713c6344585a3cf01696afcc1e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transforming nsubjes (train):   0%|          | 0/321274 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a903ed8a155b4cf5beb7d1a0901b662b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transforming genders (test): 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "872c76a479804aa8800cf5e5fdc4d7e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transforming tenses (test): 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de8737dd73de4b959278167dcd718978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transforming nsubjes (test):   0%|          | 0/26773 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "607174dd19c64c1b80b6dffef816f6bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transforming genders (validation): 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6b5749992694d76a020d72724b7cba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transforming tenses (validation): 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60391e0c4e47473a9ac8b7c3604fa1c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transforming nsubjes (validation):   0%|          | 0/8925 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_context = transform_context(train_df, 'train')\n",
    "test_context = transform_context(test_df, 'test')\n",
    "val_context = transform_context(val_df, 'validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_to_tensors(nsubj_list, gender_list, tense_list):\n",
    "    nsubj_tensor = torch.tensor(nsubj_list)\n",
    "    gender_tensor = torch.tensor(gender_list, dtype=torch.float32)\n",
    "    tense_tensor = torch.tensor(tense_list, dtype=torch.float32)\n",
    "    \n",
    "    context_tensors = [nsubj_tensor, gender_tensor, tense_tensor]\n",
    "    return context_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_context_tensors = context_to_tensors(*train_context)\n",
    "test_context_tensors = context_to_tensors(*test_context)\n",
    "val_context_tensors = context_to_tensors(*val_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_to_fit_batch(tensor: torch.Tensor, batch_size: int):\n",
    "    n_samples = tensor.shape[1]\n",
    "    new_n_samples = (n_samples // batch_size) * batch_size\n",
    "    result, _ = tensor.split(new_n_samples, dim=1)\n",
    "    return torch.transpose(result, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ContextMem(nn.Module):\n",
    "#     def __init__(self, input_dim, output_dim, hidden_ff):\n",
    "#         super(ContextMem, self).__init__()\n",
    "#         self.fc_norm = nn.Linear(input_dim, hidden_ff)\n",
    "#         self.hff     = nn.Linear(hidden_ff, output_dim)\n",
    "#         self.fc_gate = nn.Linear(output_dim, output_dim)\n",
    "\n",
    "#     def forward(self, context):\n",
    "#         #context shape = (batch_size, input_dim=3)\n",
    "#         context = self.fc_norm(context)\n",
    "#         #context shape = (batch_size, hidden_ff)\n",
    "#         context = self.hff(context)\n",
    "#         #context shape = (batch_size, output_dim)\n",
    "#         context_norm = F.tanh(context)\n",
    "        \n",
    "#         #context shape = (batch_size, output_dim)\n",
    "#         context_gate = self.fc_gate(context)\n",
    "#         #context_gate shape = (batch_size, output_dim)\n",
    "#         context_gate = F.sigmoid(context)\n",
    "\n",
    "#         return context_norm * context_gate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Построение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextMem(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, nsubj_embedding_size, device):\n",
    "        super(ContextMem, self).__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        self.gender_proj = nn.Linear(input_size, hidden_size, bias=False)\n",
    "        self.tense_proj = nn.Linear(input_size, hidden_size, bias=False)\n",
    "        self.fc_out = nn.Linear(hidden_size * 2 + nsubj_embedding_size, output_size, bias=False)\n",
    "        \n",
    "    def forward(self, nsubj_embedding, gender, tense):\n",
    "        # nsubj_embedding_shape: (batch_size, embedding_size)\n",
    "        # gender_shape: (batch_size, input_size)\n",
    "        # tense_shape: (batch_size, input_size)\n",
    "        \n",
    "        gender = self.gender_proj(gender)\n",
    "        # gender_shape: (batch_size, hidden_size)\n",
    "        \n",
    "        tense = self.tense_proj(tense)\n",
    "        # tense_shape: (batch_size, hidden_size)    \n",
    "        \n",
    "        context = torch.cat([nsubj_embedding, gender, tense], dim=1)\n",
    "        # context_shape: (batch_size, hidden_size * 2 + embedding_size) \n",
    "        \n",
    "        context = self.fc_out(context)\n",
    "        # context_shape: (batch_size, output_size)\n",
    "        \n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size, embedding_size, hidden_size,\n",
    "                 pad_idx, device, num_layers, dropout_p):\n",
    "        \n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, embedding_size, pad_idx),\n",
    "            nn.Dropout(dropout_p)\n",
    "        )\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout_p)\n",
    "        \n",
    "    def forward(self, x, hidden, cell):\n",
    "        # x_shape: (seq_len, batch_size)\n",
    "        embedding = self.embedding(x)\n",
    "        # embedding_shape: (seq_len, batch_size, embedding_size)\n",
    "        output, (hidden, cell) = self.lstm(embedding, (hidden, cell))\n",
    "        # output_shape: (seq_len, batch_size, hidden_size)\n",
    "        # hidden_shape: (num_layers, batch_size, hidden_size)\n",
    "        # cell_shape: (num_layers, batch_size, hidden_size)\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size, embedding_size, hidden_size, output_size,\n",
    "                 pad_idx, device, num_layers, dropout_p):\n",
    "        \n",
    "        super(DecoderRNN, self).__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, embedding_size, pad_idx),\n",
    "            nn.Dropout(dropout_p)\n",
    "        )\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout_p)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x, hidden, cell):\n",
    "        x = x.unsqueeze(0)\n",
    "        # x_shape:      (seq_len=1, batch_size)\n",
    "        # hidden_shape: (num_layers, batch_size, hidden_size)\n",
    "        # cell_shape:   (num_layers, batch_size, hidden_size)\n",
    "        \n",
    "        embedding = self.embedding(x)\n",
    "        # embedding_shape: (seq_len=1, batch_size, embedding_size)\n",
    "        \n",
    "        lstm_out, (hidden, cell) = self.lstm(embedding, (hidden, cell))\n",
    "        # lstm_out_shape: (seq_len=1, batch_size, hidden_size)\n",
    "        \n",
    "        fc_out = self.fc(lstm_out)\n",
    "        # fc_out_shape: (seq_len=1, batch_size, output_size)\n",
    "        # output_shape: (seq_len=1, batch_size, output_size)\n",
    "        \n",
    "        return fc_out, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 encoder_vocab_size, decoder_vocab_size,\n",
    "                 embedding_size, hidden_size, output_size,\n",
    "                 context_input_size, context_hidden_size, context_output_size,\n",
    "                 pad_idx, device, num_layers, dropout_p):\n",
    "        \n",
    "        super(Seq2SeqModel, self).__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.decoder_vocab_size = decoder_vocab_size\n",
    "        \n",
    "        self.context_mem = ContextMem(context_input_size, context_hidden_size, context_output_size, embedding_size, device).to(device)\n",
    "        self.encoder = EncoderRNN(encoder_vocab_size, embedding_size, hidden_size, pad_idx, device, num_layers, dropout_p).to(device)\n",
    "        self.decoder = DecoderRNN(decoder_vocab_size, embedding_size, hidden_size, output_size, pad_idx, device, num_layers, dropout_p).to(device)\n",
    "        \n",
    "    def forward(self, input, target, context, teacher_forcing_ratio=0.5):\n",
    "        batch_size = input.shape[1]\n",
    "        target_len = target.shape[0]\n",
    "        target_vocab_size = self.decoder_vocab_size\n",
    "        \n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size, device=self.device)\n",
    "\n",
    "        nsubj, gender, tense = context\n",
    "        # nsubj_shape:  (batch_size)\n",
    "        # gender_shape: (batch_size, context_input_size)\n",
    "        # tense_shape:  (batch_size, context_input_size)\n",
    "        \n",
    "        nsubj_embedding = self.decoder.embedding(nsubj).squeeze(0)\n",
    "        # nsubj_embedding_shape: (batch_size, embedding_size)\n",
    "        \n",
    "        hidden = self.context_mem(nsubj_embedding, gender, tense)\n",
    "        cell = hidden.clone()\n",
    "        # hidden, cell shapes: (batch_size, context_output_size=hidden_size)\n",
    "        \n",
    "        if self.num_layers == 1:\n",
    "            hidden.unsqueeze_(0)\n",
    "            cell.unsqueeze_(0)\n",
    "            # hidden, cell shapes: (1, batch_size, context_output_size=hidden_size)\n",
    "        else:\n",
    "            hidden = torch.cat([hidden.unsqueeze(0)] * self.num_layers, 0)\n",
    "            cell = torch.cat([cell.unsqueeze(0)] * self.num_layers, 0)\n",
    "            # hidden, cell shapes: (num_layers, batch_size, context_output_size=hidden_size)\n",
    "        \n",
    "        hidden, cell = self.encoder(input, hidden, cell)\n",
    "        # hidden, cell shapes: (num_layers, batch_size, hidden_size)\n",
    "        \n",
    "        prev_token_idx = target[0]\n",
    "        # prev_token_shape: (batch_size)\n",
    "        \n",
    "        for t in range(1, target_len):\n",
    "            output, hidden, cell = self.decoder(prev_token_idx, hidden, cell)\n",
    "            outputs[t] = output.squeeze(0)\n",
    "            \n",
    "            best_prediction = outputs[t].argmax(dim=1)\n",
    "            # best_prediction_shape: (batch_size)\n",
    "            prev_token_idx = target[t] if random() < teacher_forcing_ratio else best_prediction\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция сохранения текущего состояния модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model: Seq2SeqModel, optimizer, epoch, path):\n",
    "    checkpoint = {\n",
    "        'context_mem_state_dict': model.context_mem.state_dict(),\n",
    "        'encoder_state_dict': model.encoder.state_dict(),\n",
    "        'decoder_state_dict': model.decoder.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'criterion': criterion,\n",
    "        'epoch': epoch\n",
    "    }\n",
    "    \n",
    "    torch.save(checkpoint, path)\n",
    "#     with open(path, mode='wb') as f:\n",
    "#         pickle.dump(checkpoint, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция загрузки уже тренировавшейся модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model: Seq2SeqModel, optimizer, criterion, path, device=torch.device('cpu'), for_inference=True):\n",
    "#     with open(path, mode='rb') as f:\n",
    "#         checkpoint = pickle.load(f)\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    \n",
    "    model.context_mem.load_state_dict(checkpoint['context_mem_state_dict'])\n",
    "    model.encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "    model.decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "    \n",
    "    if not for_inference:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        epoch = checkpoint['epoch']\n",
    "        criterion = checkpoint['criterion']\n",
    "\n",
    "        return epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Инициализация гиперпараметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "batch_size = 256\n",
    "epochs_amount = 8\n",
    "lemm_vocab_size = 35000\n",
    "orig_vocab_size = 70000\n",
    "hidden_size = 512\n",
    "embedding_size = 300\n",
    "num_layers = 2\n",
    "max_norm = 1.0\n",
    "dropout_p = 0.5\n",
    "context_input_size = 4\n",
    "context_hidden_size = 256\n",
    "context_output_size = hidden_size\n",
    "patience = 2\n",
    "output_size = orig_vocab_size\n",
    "pad_idx = lemm_text_transformer.special_tokens_to_idx.get('<PAD>')\n",
    "model_path = './models/'\n",
    "model_name = 'simple_seq2seq_with_context.model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2SeqModel(lemm_vocab_size, orig_vocab_size, embedding_size, hidden_size, output_size,\n",
    "                     context_input_size, context_hidden_size, context_output_size,\n",
    "                     pad_idx, device, num_layers, dropout_p).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from ./models/\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    epoch = load_model(model, optimizer, criterion, model_path + model_name, device)\n",
    "    print(f'Loaded model from {model_path}')\n",
    "except:\n",
    "    print(f'No models found at {model_path}')\n",
    "    epoch = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Урезание данных для соответствия размеру батча"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lemm_tensor_f = cut_to_fit_batch(train_lemm_tensor, batch_size)\n",
    "train_orig_tensor_f = cut_to_fit_batch(train_orig_tensor, batch_size)\n",
    "\n",
    "test_lemm_tensor_f = cut_to_fit_batch(test_lemm_tensor, batch_size)\n",
    "test_orig_tensor_f = cut_to_fit_batch(test_orig_tensor, batch_size)\n",
    "\n",
    "val_lemm_tensor_f = cut_to_fit_batch(val_lemm_tensor, batch_size)\n",
    "val_orig_tensor_f = cut_to_fit_batch(val_orig_tensor, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_context_tensors_f = [cut_to_fit_batch(tensor.unsqueeze(0), batch_size).squeeze(1) for tensor in train_context_tensors]\n",
    "test_context_tensors_f = [cut_to_fit_batch(tensor.unsqueeze(0), batch_size).squeeze(1) for tensor in test_context_tensors]\n",
    "val_context_tensors_f = [cut_to_fit_batch(tensor.unsqueeze(0), batch_size).squeeze(1) for tensor in val_context_tensors]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Инициализация данных итерируемых по батчам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(train_lemm_tensor_f, train_orig_tensor_f, *train_context_tensors_f)\n",
    "test_dataset = TensorDataset(test_lemm_tensor_f, test_orig_tensor_f, *test_context_tensors_f)\n",
    "val_dataset = TensorDataset(val_lemm_tensor_f, val_orig_tensor_f, *val_context_tensors_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Определение функции проверки работы сети между эпохами обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_evaluate(model, input, context, target_len=8):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        \n",
    "        input = input.to(device)\n",
    "\n",
    "        nsubj, gender, tense = context\n",
    "        nsubj_embedding = model.decoder.embedding(nsubj)\n",
    "\n",
    "        hidden = model.context_mem(nsubj_embedding, gender, tense)\n",
    "        cell = hidden.clone()\n",
    "\n",
    "        if model.num_layers == 1:\n",
    "            hidden.unsqueeze_(0)\n",
    "            cell.unsqueeze_(0)\n",
    "            # hidden, cell shapes: (1, batch_size, context_output_size=hidden_size)\n",
    "        else:\n",
    "            hidden = torch.cat([hidden.unsqueeze(0)] * model.num_layers, 0)\n",
    "            cell = torch.cat([cell.unsqueeze(0)] * model.num_layers, 0)\n",
    "            # hidden, cell shapes: (num_layers, batch_size, context_output_size=hidden_size)\n",
    "\n",
    "        sos_idx = lemm_text_transformer.special_tokens_to_idx.get('<SOS>')\n",
    "        eos_idx = lemm_text_transformer.special_tokens_to_idx.get('<EOS>')\n",
    "\n",
    "        hidden, cell = model.encoder(input, hidden, cell)\n",
    "\n",
    "        predicted_indexes = [sos_idx]\n",
    "\n",
    "        for _ in range(1, target_len):\n",
    "            prev_idx = torch.tensor([predicted_indexes[-1]], dtype=torch.long, device=device)\n",
    "\n",
    "            output, hidden, cell = model.decoder(prev_idx, hidden, cell)\n",
    "            output = output.squeeze(0)\n",
    "\n",
    "            best_prediction = output.argmax(dim=1).item()\n",
    "\n",
    "            if best_prediction == eos_idx:\n",
    "                break\n",
    "\n",
    "            predicted_indexes.append(best_prediction)\n",
    "\n",
    "\n",
    "        predicted_tokens = [orig_text_transformer.vocab.idx_to_token(idx) for idx in predicted_indexes]\n",
    "        return predicted_tokens[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Определение функции обучения сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, train_data, val_data, test_data, epochs_amount, max_norm, context, patience=3, current_epoch=1, n_prints=5):\n",
    "    min_mean_val_loss = float('+inf')\n",
    "    initial_patiece = patience\n",
    "    print_every = len(train_data) // n_prints\n",
    "    \n",
    "    for epoch in tqdm(range(current_epoch, epochs_amount + 1), 'Epochs'):\n",
    "        print(f'\\nEpoch [{epoch} / {epochs_amount}]')\n",
    "        \n",
    "        model.train()\n",
    "        for iteration, (input, target, nsubj, gender, tense) in enumerate(tqdm(train_data, 'Epoch training iterations')):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            input = torch.transpose(input, 1, 0).to(device)   \n",
    "            # input_shape: (seq_len, batch_size)\n",
    "            \n",
    "            target = torch.transpose(target, 1, 0).to(device)\n",
    "            # target_shape: (seq_len, batch_size)\n",
    "            \n",
    "            context = (nsubj.to(device), gender.to(device), tense.to(device))\n",
    "            \n",
    "            output = model(input, target, context)\n",
    "            # output_shape: (seq_len, batch_size, orig_vocab_size) but need (N, orig_vocab_size)\n",
    "            \n",
    "            target = target[1:].reshape(-1)\n",
    "            # now target_shape is (seq_len * batch_size)\n",
    "            \n",
    "            orig_vocab_size = output.shape[2]\n",
    "            \n",
    "            output = output[1:].reshape(-1, orig_vocab_size)\n",
    "            # now output_shape is (seq_len * batch_size, orig_vocab_size)\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            if iteration % print_every == 0:\n",
    "                print(f'\\tIteration #{iteration}: training loss = {loss.item()}')\n",
    "            elif iteration == len(train_data):\n",
    "                print(f'\\tIteration #{iteration}: training loss = {loss.item()}')\n",
    "            \n",
    "            \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            val_loss = []\n",
    "            \n",
    "            for input, target, nsubj, gender, tense in tqdm(val_data, 'Epoch validating iterations'):\n",
    "                input = torch.transpose(input, 1, 0).to(device)\n",
    "                target = torch.transpose(target, 1, 0).to(device)\n",
    "                context = (nsubj.to(device), gender.to(device), tense.to(device))\n",
    "                output = model(input, target, context)\n",
    "                \n",
    "                orig_vocab_size = output.shape[2]\n",
    "                output = output[1:].reshape(-1, orig_vocab_size)\n",
    "                target = target[1:].reshape(-1)\n",
    "                \n",
    "                val_loss.append(criterion(output, target).item())\n",
    "            \n",
    "            mean_val_loss = sum(val_loss) / len(val_loss)\n",
    "            print(f'\\tValidation loss = {mean_val_loss}')\n",
    "            if mean_val_loss < min_mean_val_loss:\n",
    "                try:\n",
    "                    save_model(model, optimizer, epoch, model_path + model_name)\n",
    "                    min_mean_val_loss = mean_val_loss\n",
    "                    patience = initial_patiece\n",
    "                except Exception as exc:\n",
    "                    print(exc)\n",
    "            else:\n",
    "                patience -= 1\n",
    "            \n",
    "            test_data = DataLoader(test_data.dataset, batch_size=1, shuffle=True)\n",
    "            for input, target, nsubj, gender, tense in test_data:\n",
    "                target = target.squeeze(0).to(device)\n",
    "                context = (nsubj.to(device), gender.to(device), tense.to(device))\n",
    "                \n",
    "                input = torch.transpose(input, 1, 0)\n",
    "                target_len = target.shape[0]\n",
    "                \n",
    "                output = test_evaluate(model, input, context, target_len)\n",
    "                decoded_input = [lemm_text_transformer.vocab.idx_to_token(idx.item()) for idx in input]\n",
    "                decoded_target = [orig_text_transformer.vocab.idx_to_token(idx.item()) for idx in target]\n",
    "                \n",
    "                print(f'\\tInput: {decoded_input}')\n",
    "                print(f'\\tOutput: {output}')\n",
    "                print(f'\\tTarget: {decoded_target}')\n",
    "                break\n",
    "        \n",
    "        if patience == 0:\n",
    "            print(f'\\nModel learning finished due to early stopping')\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Определение функции эксплуатации обученной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, sentence: str, context, max_seq_len=8):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        \n",
    "        nsubj, gender, tense = context\n",
    "        \n",
    "        nsubj = torch.tensor([orig_text_transformer.vocab.token_to_idx(nsubj)], device=device).unsqueeze(0)\n",
    "        gender = torch.tensor([gender_label_to_vec[gender]], dtype=torch.float32, device=device)\n",
    "        tense = torch.tensor([tense_label_to_vec[tense]], dtype=torch.float32, device=device)\n",
    "        \n",
    "        nsubj_embedding = model.decoder.embedding(nsubj).squeeze(0)\n",
    "\n",
    "        hidden = model.context_mem(nsubj_embedding, gender, tense)\n",
    "        cell = hidden.clone()\n",
    "\n",
    "        if model.num_layers == 1:\n",
    "            hidden.unsqueeze_(0)\n",
    "            cell.unsqueeze_(0)\n",
    "            # hidden, cell shapes: (1, batch_size, context_output_size=hidden_size)\n",
    "        else:\n",
    "            hidden = torch.cat([hidden.unsqueeze(0)] * model.num_layers, 0)\n",
    "            cell = torch.cat([cell.unsqueeze(0)] * model.num_layers, 0)\n",
    "            # hidden, cell shapes: (num_layers, batch_size, context_output_size=hidden_size)\n",
    "        \n",
    "        input_tensor = lemm_text_transformer.text_to_tensor(sentence, max_seq_len).to(device)\n",
    "        input_tensor = torch.transpose(input_tensor, 1, 0)\n",
    "        sos_idx = lemm_text_transformer.special_tokens_to_idx.get('<SOS>')\n",
    "        eos_idx = lemm_text_transformer.special_tokens_to_idx.get('<EOS>')\n",
    "\n",
    "    \n",
    "        hidden, cell = model.encoder(input_tensor, hidden, cell)\n",
    "        \n",
    "        predicted_indexes = [sos_idx]\n",
    "        \n",
    "#         while True:\n",
    "#             prev_idx = torch.tensor([predicted_indexes[-1]], dtype=torch.long, device=device)\n",
    "            \n",
    "#             output, hidden, cell = model.decoder(prev_idx, hidden, cell)\n",
    "#             output = output.squeeze(0)\n",
    "            \n",
    "#             best_prediction = output.argmax(dim=1).item()\n",
    "            \n",
    "#             if best_prediction == eos_idx:\n",
    "#                 break\n",
    "            \n",
    "#             predicted_indexes.append(best_prediction)\n",
    "                       \n",
    "        \n",
    "        for _ in range(1, max_seq_len):\n",
    "            prev_idx = torch.tensor([predicted_indexes[-1]], dtype=torch.long, device=device)\n",
    "            \n",
    "            output, hidden, cell = model.decoder(prev_idx, hidden, cell)\n",
    "            output = output.squeeze(0)\n",
    "            \n",
    "            best_prediction = output.argmax(dim=1).item()\n",
    "            \n",
    "            if best_prediction == eos_idx:\n",
    "                break\n",
    "                \n",
    "            predicted_indexes.append(best_prediction)\n",
    "        \n",
    "    predicted_tokens = [orig_text_transformer.vocab.idx_to_token(idx) for idx in predicted_indexes]\n",
    "    return predicted_tokens[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train(model, optimizer, criterion, train_loader, val_loader, test_loader, epochs_amount, max_norm, patience, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_model(model, optimizer, criterion, model_path + model_name)\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# del model\n",
    "# del optimizer\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_label_to_vec = {\n",
    "    'masc'     : [0, 1, 0, 0],\n",
    "    'fem'      : [1, 0, 0, 0],\n",
    "    'undefined': [0, 0, 0, 1],\n",
    "    'neut'     : [0, 0, 1, 0]\n",
    "}\n",
    "\n",
    "tense_label_to_vec = {\n",
    "    'past'     : [0, 1, 0, 0],\n",
    "    'pres'     : [0, 0, 1, 0],\n",
    "    'fut'      : [1, 0, 0, 0],\n",
    "    'undefined': [0, 0, 0, 1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_to_label(label_type: str, input_vec):\n",
    "    if label_type == 'gender':\n",
    "        for label, vec in gender_label_to_vec.items():\n",
    "            if vec == input_vec:\n",
    "                return label\n",
    "        \n",
    "    elif label_type == 'tense':\n",
    "        for label, vec in tense_label_to_vec.items():\n",
    "            if vec == input_vec:\n",
    "                return label\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = train_df.sample(50)\n",
    "\n",
    "test_input = test_sample.lemm_texts.to_list()\n",
    "test_target = test_sample.orig_texts.to_list()\n",
    "\n",
    "test_nsubj = test_sample.nsubj.to_list()\n",
    "test_gender = test_sample.gender.to_list()\n",
    "test_tense = test_sample.tense.to_list()\n",
    "\n",
    "test_zipped = list(zip(test_input, test_target, test_nsubj, test_gender, test_tense))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load('./models/params.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('context_mem.gender_proj.weight',\n",
       "              tensor([[-0.4963,  0.0215, -0.7154, -0.2743],\n",
       "                      [-0.2054,  0.0732, -0.3534,  0.0271],\n",
       "                      [ 0.1722,  0.1258,  0.0351, -0.1378],\n",
       "                      ...,\n",
       "                      [-0.2794, -0.2679, -0.5680, -0.0552],\n",
       "                      [ 0.0308,  0.2176, -0.1868,  0.1538],\n",
       "                      [-0.3556,  0.2144,  0.3736,  0.1786]], device='cuda:0')),\n",
       "             ('context_mem.tense_proj.weight',\n",
       "              tensor([[-0.6928, -0.0233,  0.4835, -0.2829],\n",
       "                      [ 0.5565, -0.0669, -0.0504,  0.6564],\n",
       "                      [ 0.4251,  0.0579,  0.4597, -0.3516],\n",
       "                      ...,\n",
       "                      [-0.2581,  0.1273, -0.1396, -0.1242],\n",
       "                      [ 0.6218, -0.0855,  0.4336,  0.4036],\n",
       "                      [ 0.1899,  0.0572,  0.1556,  0.0835]], device='cuda:0')),\n",
       "             ('context_mem.fc_out.weight',\n",
       "              tensor([[-0.0824, -0.0193,  0.0476,  ...,  0.2887, -0.2129,  0.0775],\n",
       "                      [-0.0245,  0.1134, -0.0609,  ...,  0.0296, -0.0332,  0.0497],\n",
       "                      [-0.0423, -0.0091,  0.0660,  ..., -0.2158,  0.2755, -0.0611],\n",
       "                      ...,\n",
       "                      [ 0.0680,  0.1477, -0.0930,  ...,  0.0599,  0.0333,  0.1235],\n",
       "                      [ 0.0437, -0.0149, -0.0200,  ..., -0.0090, -0.0159,  0.0566],\n",
       "                      [ 0.0258,  0.0601,  0.1090,  ...,  0.0072,  0.0052,  0.0712]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.embedding.0.weight',\n",
       "              tensor([[-0.3582, -0.8379, -0.4137,  ..., -0.5417,  0.3878, -0.3078],\n",
       "                      [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "                      [ 0.3996,  0.8071,  1.0383,  ..., -2.7568, -1.0897,  0.1215],\n",
       "                      ...,\n",
       "                      [-1.2192, -0.0591,  0.6706,  ..., -2.1571,  0.0258, -0.0365],\n",
       "                      [-0.5924, -0.1338,  0.4419,  ...,  1.0075,  0.3169, -1.6589],\n",
       "                      [ 1.9608,  0.3091, -0.0853,  ..., -2.6473, -1.9691, -0.1310]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.lstm.weight_ih_l0',\n",
       "              tensor([[ 0.1017,  0.0098, -0.0301,  ..., -0.1488, -0.0583,  0.1017],\n",
       "                      [-0.1050, -0.1951, -0.1992,  ..., -0.0042,  0.0937,  0.0092],\n",
       "                      [-0.0231, -0.0238, -0.0762,  ..., -0.1363,  0.1596,  0.0646],\n",
       "                      ...,\n",
       "                      [-0.0780,  0.2873,  0.0677,  ...,  0.0461,  0.2988, -0.0633],\n",
       "                      [ 0.1829, -0.1137, -0.0264,  ...,  0.1549,  0.3055, -0.0338],\n",
       "                      [-0.0797,  0.0031,  0.1416,  ..., -0.1051,  0.0908,  0.1103]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.lstm.weight_hh_l0',\n",
       "              tensor([[-0.0741,  0.0199, -0.0900,  ..., -0.0663, -0.0274,  0.0128],\n",
       "                      [-0.0496,  1.0122, -0.0528,  ..., -0.0373,  0.0221,  0.0637],\n",
       "                      [ 0.1959,  0.0741, -0.0162,  ...,  0.0805,  0.0074, -0.0238],\n",
       "                      ...,\n",
       "                      [-0.1113,  0.1491, -0.0772,  ...,  0.0668,  0.0412, -0.0803],\n",
       "                      [ 0.0277,  0.1311, -0.0483,  ..., -0.0420, -0.1428, -0.1038],\n",
       "                      [-0.0848,  0.0047,  0.0946,  ..., -0.0274, -0.1442,  0.2219]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.lstm.bias_ih_l0',\n",
       "              tensor([-0.0643,  0.0705,  0.1061,  ..., -0.1198, -0.0180,  0.0210],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.lstm.bias_hh_l0',\n",
       "              tensor([-0.0304,  0.1034,  0.0617,  ..., -0.1592, -0.0205, -0.0030],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.lstm.weight_ih_l1',\n",
       "              tensor([[-0.0379,  0.0868, -0.0035,  ..., -0.1119, -0.0856, -0.0983],\n",
       "                      [-0.0554, -0.0558,  0.0858,  ...,  0.0586, -0.1399,  0.0896],\n",
       "                      [ 0.0174,  0.2164, -0.0651,  ...,  0.1004,  0.0398, -0.0958],\n",
       "                      ...,\n",
       "                      [ 0.1537,  0.0406,  0.0709,  ...,  0.0834,  0.0114, -0.1763],\n",
       "                      [-0.0097, -0.0448,  0.0276,  ..., -0.0584, -0.0786, -0.1844],\n",
       "                      [-0.2132,  0.1353, -0.0159,  ..., -0.2478,  0.0424,  0.0140]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.lstm.weight_hh_l1',\n",
       "              tensor([[ 0.0169, -0.1221, -0.0028,  ...,  0.1345, -0.1292,  0.0287],\n",
       "                      [-0.0746,  0.1304,  0.0137,  ..., -0.1474,  0.0629, -0.0773],\n",
       "                      [-0.0532, -0.0033, -0.0658,  ...,  0.0150, -0.0183,  0.0167],\n",
       "                      ...,\n",
       "                      [-0.1385, -0.2295,  0.1470,  ...,  0.3214, -0.2507,  0.2276],\n",
       "                      [-0.0781,  0.0107,  0.2869,  ..., -0.0726,  0.1622,  0.0674],\n",
       "                      [ 0.0053,  0.1066,  0.0839,  ...,  0.0128, -0.1160, -0.0951]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.lstm.bias_ih_l1',\n",
       "              tensor([-0.0525,  0.0679, -0.0129,  ..., -0.0949, -0.1192, -0.0635],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.lstm.bias_hh_l1',\n",
       "              tensor([-0.0991,  0.0672,  0.0453,  ..., -0.0947, -0.1528, -0.0574],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.embedding.0.weight',\n",
       "              tensor([[-0.8467, -0.0548, -0.4611,  ...,  1.3390,  0.5208,  0.0398],\n",
       "                      [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "                      [ 0.0929, -0.1938, -0.2699,  ...,  0.2157,  0.2773, -0.2577],\n",
       "                      ...,\n",
       "                      [-0.2801, -1.1834,  0.1093,  ..., -1.3160, -0.4758,  0.4005],\n",
       "                      [ 0.3841,  0.4512, -0.4495,  ..., -0.6827,  1.0016,  0.4429],\n",
       "                      [-0.1216,  0.6113, -0.6068,  ...,  0.7444,  2.8123, -0.9894]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.lstm.weight_ih_l0',\n",
       "              tensor([[-0.0567,  0.1130, -0.0721,  ..., -0.0218, -0.0381,  0.0127],\n",
       "                      [ 0.0394, -0.0757, -0.0326,  ...,  0.0120,  0.0729, -0.0578],\n",
       "                      [-0.0010, -0.1314,  0.0254,  ..., -0.1075, -0.0083, -0.1081],\n",
       "                      ...,\n",
       "                      [-0.0251,  0.0172, -0.0441,  ...,  0.0939, -0.1326, -0.0712],\n",
       "                      [-0.0220,  0.0664, -0.0062,  ...,  0.0075,  0.0219,  0.0021],\n",
       "                      [-0.0074, -0.0482, -0.0416,  ..., -0.0408, -0.0098,  0.1494]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.lstm.weight_hh_l0',\n",
       "              tensor([[ 0.1128, -0.1229, -0.1072,  ..., -0.1642, -0.0742, -0.2827],\n",
       "                      [ 0.0590,  0.3307,  0.0590,  ..., -0.1409,  0.0479,  0.0329],\n",
       "                      [-0.1093,  0.0021,  0.1002,  ...,  0.0844, -0.0306,  0.0077],\n",
       "                      ...,\n",
       "                      [ 0.1256,  0.0319,  0.2442,  ..., -0.1345,  0.0179,  0.0980],\n",
       "                      [-0.1712,  0.0590,  0.2567,  ...,  0.1211,  0.1022,  0.1611],\n",
       "                      [-0.0219, -0.3101,  0.2229,  ...,  0.0979,  0.1076,  0.1894]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.lstm.bias_ih_l0',\n",
       "              tensor([ 0.0427, -0.0304,  0.0371,  ..., -0.1967, -0.0712,  0.0206],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.lstm.bias_hh_l0',\n",
       "              tensor([-0.0107, -0.0300, -0.0022,  ..., -0.1423, -0.0667,  0.0149],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.lstm.weight_ih_l1',\n",
       "              tensor([[ 0.0787,  0.0507, -0.2610,  ...,  0.0796, -0.0419, -0.1510],\n",
       "                      [ 0.0740, -0.3939, -0.1704,  ...,  0.0425,  0.1305, -0.0380],\n",
       "                      [-0.0581, -0.0467, -0.1136,  ...,  0.0981,  0.0126, -0.0742],\n",
       "                      ...,\n",
       "                      [ 0.2320, -0.2301,  0.4687,  ...,  0.0634,  0.0435,  0.3992],\n",
       "                      [ 0.3414,  0.0927, -0.0136,  ...,  0.0415, -0.0330,  0.0654],\n",
       "                      [ 0.1682,  0.0106, -0.1128,  ...,  0.1324,  0.0153, -0.0339]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.lstm.weight_hh_l1',\n",
       "              tensor([[-0.2010,  0.0962, -0.1735,  ...,  0.0234, -0.0337, -0.1272],\n",
       "                      [ 0.1119,  0.2823, -0.0226,  ...,  0.0046,  0.0267,  0.1774],\n",
       "                      [ 0.1435,  0.2019, -0.2105,  ...,  0.1822,  0.4251,  0.0696],\n",
       "                      ...,\n",
       "                      [ 0.3664, -0.0499,  0.0150,  ..., -0.1366, -0.1655,  0.0724],\n",
       "                      [-0.0875,  0.1171, -0.2452,  ...,  0.3368,  0.6202, -0.3059],\n",
       "                      [-0.1221, -0.0010,  0.1273,  ..., -0.0068,  0.0743,  0.0833]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.lstm.bias_ih_l1',\n",
       "              tensor([-0.0368,  0.1987,  0.1839,  ..., -0.1031, -0.0079,  0.1903],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.lstm.bias_hh_l1',\n",
       "              tensor([-0.0142,  0.2217,  0.1764,  ..., -0.1301, -0.0433,  0.2001],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.fc.weight',\n",
       "              tensor([[ 1.1492e-01,  5.7501e-02, -4.5786e-01,  ..., -5.5748e-02,\n",
       "                       -1.2415e-01, -2.8015e-02],\n",
       "                      [-3.3016e-02, -4.0134e-02,  4.4097e-01,  ..., -3.1409e-01,\n",
       "                       -1.9528e-01,  7.3931e-05],\n",
       "                      [-3.1179e-02, -2.4025e-02,  4.0410e-01,  ..., -3.1205e-01,\n",
       "                       -2.1832e-01, -5.7736e-02],\n",
       "                      ...,\n",
       "                      [ 5.8439e-02,  2.1536e-01, -1.1822e-01,  ..., -6.7560e-01,\n",
       "                       -6.6403e-01,  9.2001e-02],\n",
       "                      [-9.0550e-02,  1.4247e-01, -1.7950e-01,  ..., -1.0687e-01,\n",
       "                        5.8394e-02, -4.1711e-02],\n",
       "                      [ 4.5333e-02,  2.6930e-01, -1.4887e-01,  ..., -4.6431e-01,\n",
       "                       -4.5188e-01,  9.0820e-02]], device='cuda:0')),\n",
       "             ('decoder.fc.bias',\n",
       "              tensor([ 0.5598, -0.6188, -0.4786,  ..., -0.1806, -0.1305, -0.1520],\n",
       "                     device='cuda:0'))])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_mem.gender_proj.weight True\n",
      "context_mem.tense_proj.weight True\n",
      "context_mem.fc_out.weight True\n",
      "encoder.embedding.0.weight True\n",
      "encoder.lstm.weight_ih_l0 True\n",
      "encoder.lstm.weight_hh_l0 True\n",
      "encoder.lstm.bias_ih_l0 True\n",
      "encoder.lstm.bias_hh_l0 True\n",
      "encoder.lstm.weight_ih_l1 True\n",
      "encoder.lstm.weight_hh_l1 True\n",
      "encoder.lstm.bias_ih_l1 True\n",
      "encoder.lstm.bias_hh_l1 True\n",
      "decoder.embedding.0.weight True\n",
      "decoder.lstm.weight_ih_l0 True\n",
      "decoder.lstm.weight_hh_l0 True\n",
      "decoder.lstm.bias_ih_l0 True\n",
      "decoder.lstm.bias_hh_l0 True\n",
      "decoder.lstm.weight_ih_l1 True\n",
      "decoder.lstm.weight_hh_l1 True\n",
      "decoder.lstm.bias_ih_l1 True\n",
      "decoder.lstm.bias_hh_l1 True\n",
      "decoder.fc.weight True\n",
      "decoder.fc.bias True\n"
     ]
    }
   ],
   "source": [
    "for k, v in state_dict.items():\n",
    "    res = torch.all(v == model.state_dict()[k]).item()\n",
    "    print(k, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: я работать на россия.\n",
      "Nsubj: я\n",
      "gender: fem\n",
      "tense: past\n",
      "Output: ['я', 'уехал', 'на', '<UNK>', '.']\n",
      "Target: я работала на россию.\n",
      "\n",
      "\n",
      "Input: я часто ездить к мать.\n",
      "Nsubj: я\n",
      "gender: undefined\n",
      "tense: pres\n",
      "Output: ['я', 'официант', 'роль', 'к', 'первый', '.']\n",
      "Target: я часто езжу к матери.\n",
      "\n",
      "\n",
      "Input: ленка всегда спасть с какой - нибыть железяка.\n",
      "Nsubj: ленка\n",
      "gender: fem\n",
      "tense: past\n",
      "Output: ['<UNK>', 'удивленно', 'ника', 'с', 'ногу', '-', '-']\n",
      "Target: ленка всегда спала с какой-нибудь железякой.\n",
      "\n",
      "\n",
      "Input: христос стоять на колено ...\n",
      "Nsubj: христос\n",
      "gender: masc\n",
      "tense: past\n",
      "Output: ['христос', 'шел', 'на', 'досады', '...']\n",
      "Target: христос стоял на коленях...\n",
      "\n",
      "\n",
      "Input: как я скучать , как скучать!\n",
      "Nsubj: я\n",
      "gender: fem\n",
      "tense: past\n",
      "Output: ['оказался', 'я', 'вздохнул', ',', 'оказался', 'правилам', '!']\n",
      "Target: как я скучала, как скучала!\n",
      "\n",
      "\n",
      "Input: кэти нажать на курок.\n",
      "Nsubj: кэти\n",
      "gender: fem\n",
      "tense: past\n",
      "Output: ['вивиана', '<UNK>', 'на', 'рукам', '.']\n",
      "Target: кэти нажала на курок.\n",
      "\n",
      "\n",
      "Input: я выделить для это время.\n",
      "Nsubj: я\n",
      "gender: undefined\n",
      "tense: fut\n",
      "Output: ['я', '<UNK>', 'считал', 'эту', 'вдвоем', '.']\n",
      "Target: я выделю для этого время.\n",
      "\n",
      "\n",
      "Input: кто - то идти между скала.\n",
      "Nsubj: кто\n",
      "gender: masc\n",
      "tense: past\n",
      "Output: ['кто', '-', 'кресла', 'стоял', 'у', 'арестованному', '.']\n",
      "Target: кто-то шел между скал.\n",
      "\n",
      "\n",
      "Input: она сказать о это дональд.\n",
      "Nsubj: она\n",
      "gender: fem\n",
      "tense: past\n",
      "Output: ['она', 'справлялась', 'об', 'этом', '<UNK>', '.']\n",
      "Target: она сказала об этом дональду.\n",
      "\n",
      "\n",
      "Input: я направиться к эскалатор.\n",
      "Nsubj: я\n",
      "gender: masc\n",
      "tense: past\n",
      "Output: ['я', 'направился', 'к', '<UNK>', '.']\n",
      "Target: я направился к эскалатору.\n",
      "\n",
      "\n",
      "Input: лей задыхаться от волнение.\n",
      "Nsubj: лея\n",
      "gender: fem\n",
      "tense: past\n",
      "Output: ['<UNK>', '<UNK>', 'от', 'считал', '.']\n",
      "Target: лея задыхалась от волнения.\n",
      "\n",
      "\n",
      "Input: противник быстро продвигаться в южный направление.\n",
      "Nsubj: противник\n",
      "gender: masc\n",
      "tense: past\n",
      "Output: ['противник', 'быстро', 'натану', 'в', '<UNK>', 'блаженстве', '.']\n",
      "Target: противник быстро продвигался в южном направлении.\n",
      "\n",
      "\n",
      "Input: она взглянуть на свой рука.\n",
      "Nsubj: она\n",
      "gender: fem\n",
      "tense: past\n",
      "Output: ['она', 'взглянула', 'на', 'человек', 'домик', '.']\n",
      "Target: она взглянула на свои руки.\n",
      "\n",
      "\n",
      "Input: сегодня я видеть вы в бой.\n",
      "Nsubj: я\n",
      "gender: masc\n",
      "tense: past\n",
      "Output: ['следовала', 'я', 'шли', 'шли', 'в', 'востоку', '.']\n",
      "Target: сегодня я видел вас в бою.\n",
      "\n",
      "\n",
      "Input: они тоже быть под ваш контроль?\n",
      "Nsubj: они\n",
      "gender: masc\n",
      "tense: undefined\n",
      "Output: ['они', 'тоже', 'душ', 'под', '<UNK>', '<UNK>', '?']\n",
      "Target: они тоже будут под вашим контролем?\n",
      "\n",
      "\n",
      "Input: они встать под вишня.\n",
      "Nsubj: они\n",
      "gender: undefined\n",
      "tense: past\n",
      "Output: ['они', 'богомаз', 'под', '<UNK>', '.']\n",
      "Target: они встали под вишню.\n",
      "\n",
      "\n",
      "Input: сергей нехотя пойти навстречу судьба.\n",
      "Nsubj: сергей\n",
      "gender: masc\n",
      "tense: past\n",
      "Output: ['<UNK>', '<UNK>', 'пребывал', 'лифта', 'мере', '.']\n",
      "Target: сергей нехотя пошел навстречу судьбе.\n",
      "\n",
      "\n",
      "Input: я представить в лицо ...\n",
      "Nsubj: я\n",
      "gender: undefined\n",
      "tense: fut\n",
      "Output: ['я', '<UNK>', 'в', '<UNK>', '...']\n",
      "Target: я представлю в лицах...\n",
      "\n",
      "\n",
      "Input: я говорить о папаша.\n",
      "Nsubj: я\n",
      "gender: undefined\n",
      "tense: pres\n",
      "Output: ['я', 'только', 'об', '<UNK>', '.']\n",
      "Target: я говорю о папаше.\n",
      "\n",
      "\n",
      "Input: террорист преподавать в университет.\n",
      "Nsubj: террорист\n",
      "gender: masc\n",
      "tense: past\n",
      "Output: ['эмоциональной', '<UNK>', 'в', 'участвовала', '.']\n",
      "Target: террорист преподавал в университете.\n",
      "\n",
      "\n",
      "Input: алексей пропасть на весь ночь.\n",
      "Nsubj: алексей\n",
      "gender: masc\n",
      "tense: past\n",
      "Output: ['исхудала', 'желания', 'на', 'прихлебывал', 'лодке', '.']\n",
      "Target: алексей пропал на всю ночь.\n",
      "\n",
      "\n",
      "Input: я не верить в истолкование.\n",
      "Nsubj: я\n",
      "gender: undefined\n",
      "tense: pres\n",
      "Output: ['я', 'сбежали', 'тоже', 'в', '<UNK>', '.']\n",
      "Target: я не верю в истолкования.\n",
      "\n",
      "\n",
      "Input: они подняться на поверхность.\n",
      "Nsubj: они\n",
      "gender: undefined\n",
      "tense: past\n",
      "Output: ['они', 'жила', 'на', 'маршал', '.']\n",
      "Target: они поднялись на поверхность.\n",
      "\n",
      "\n",
      "Input: я он ещё пятка еврик добавить.\n",
      "Nsubj: я\n",
      "gender: masc\n",
      "tense: past\n",
      "Output: ['я', 'остановились', 'так', '<UNK>', '<UNK>', 'маркус', '.']\n",
      "Target: я ему ещё пяток евриков добавил.\n",
      "\n",
      "\n",
      "Input: сука рычать из темнота.\n",
      "Nsubj: сука\n",
      "gender: fem\n",
      "tense: past\n",
      "Output: ['сука', 'окоп', 'из', 'сыном', '.']\n",
      "Target: сука рычала из темноты.\n",
      "\n",
      "\n",
      "Input: гарри взглянуть на великан.\n",
      "Nsubj: гарри\n",
      "gender: masc\n",
      "tense: past\n",
      "Output: ['гарри', 'взглянул', 'на', 'ирвинг', '.']\n",
      "Target: гарри взглянул на великана.\n",
      "\n",
      "\n",
      "Input: он также отказаться от охрана.\n",
      "Nsubj: он\n",
      "gender: masc\n",
      "tense: past\n",
      "Output: ['он', 'их', 'андрей', 'андрей', 'валера', '.']\n",
      "Target: он также отказался от охраны.\n",
      "\n",
      "\n",
      "Input: дэймона встать под струя.\n",
      "Nsubj: дэймон\n",
      "gender: masc\n",
      "tense: past\n",
      "Output: ['дэймон', 'изменились', 'под', 'таможне', '.']\n",
      "Target: дэймон встал под струю.\n",
      "\n",
      "\n",
      "Input: толстяк наконец справиться с виноградинка.\n",
      "Nsubj: толстяк\n",
      "gender: masc\n",
      "tense: past\n",
      "Output: ['<UNK>', 'проходим', 'неуверенно', 'с', '<UNK>', '.']\n",
      "Target: толстяк наконец справился с виноградинкой.\n",
      "\n",
      "\n",
      "Input: он находиться в задумчивый настроение.\n",
      "Nsubj: он\n",
      "gender: masc\n",
      "tense: past\n",
      "Output: ['он', 'обернулся', 'в', 'юле', 'врага', '.']\n",
      "Target: он находился в задумчивом настроении.\n",
      "\n",
      "\n",
      "Input: каждый девочка думать о подобный игрушка.\n",
      "Nsubj: девочка\n",
      "gender: fem\n",
      "tense: past\n",
      "Output: ['<UNK>', 'недоверчиво', 'работал', 'о', 'возница', 'свежий', '.']\n",
      "Target: каждая девочка думала о подобной игрушке.\n",
      "\n",
      "\n",
      "Input: мы замереть от волнение.\n",
      "Nsubj: мы\n",
      "gender: undefined\n",
      "tense: past\n",
      "Output: ['мы', '<UNK>', 'тимур', 'сени', '.']\n",
      "Target: мы замерли от волнения.\n",
      "\n",
      "\n",
      "Input: я выползти из каюта.\n",
      "Nsubj: я\n",
      "gender: masc\n",
      "tense: past\n",
      "Output: ['я', 'лужам', 'из', 'табурета', '.']\n",
      "Target: я выполз из каюты.\n",
      "\n",
      "\n",
      "Input: мы войти в ванная.\n",
      "Nsubj: мы\n",
      "gender: undefined\n",
      "tense: past\n",
      "Output: ['мы', 'при', 'в', '<UNK>', '.']\n",
      "Target: мы вошли в ванную.\n",
      "\n",
      "\n",
      "Input: аким молча двинуться к люк.\n",
      "Nsubj: аким\n",
      "gender: masc\n",
      "tense: past\n",
      "Output: ['алеке', 'сел', 'час', 'к', 'видимо', '.']\n",
      "Target: аким молча двинулся к люку.\n",
      "\n",
      "\n",
      "Input: я уже не говорить о юбк!\n",
      "Nsubj: я\n",
      "gender: undefined\n",
      "tense: pres\n",
      "Output: ['я', 'жилу', 'не', 'целью', 'не', '<UNK>', '!']\n",
      "Target: я уже не говорю о юбк!\n",
      "\n",
      "\n",
      "Input: он чуть не сойти с ум.\n",
      "Nsubj: он\n",
      "gender: masc\n",
      "tense: past\n",
      "Output: ['он', 'сквозь', 'сквозь', 'с', 'с', '<UNK>', '.']\n",
      "Target: он чуть не сошел с ума.\n",
      "\n",
      "\n",
      "Input: виктор постучать в дверь.\n",
      "Nsubj: виктор\n",
      "gender: masc\n",
      "tense: past\n",
      "Output: ['колесами', 'привык', 'в', 'дверь', '.']\n",
      "Target: виктор постучал в дверь.\n",
      "\n",
      "\n",
      "Input: я отскочить на безопасный расстояние.\n",
      "Nsubj: я\n",
      "gender: masc\n",
      "tense: past\n",
      "Output: ['я', 'клубам', 'на', '<UNK>', 'тряслась', '.']\n",
      "Target: я отскочил на безопасное расстояние.\n",
      "\n",
      "\n",
      "Input: священник поскрести в затылок.\n",
      "Nsubj: священник\n",
      "gender: masc\n",
      "tense: undefined\n",
      "Output: ['журнале', 'струнку', 'в', 'купались', '.']\n",
      "Target: священник поскреб в затылке.\n",
      "\n",
      "\n",
      "Input: я же вы дать честной слово.\n",
      "Nsubj: я\n",
      "gender: fem\n",
      "tense: past\n",
      "Output: ['я', 'дрожал', 'шли', '<UNK>', '<UNK>', 'гарри', '.']\n",
      "Target: я же вам дала честное слово.\n",
      "\n",
      "\n",
      "Input: сапёр тоже не привыкнуть к подобный приключение.\n",
      "Nsubj: сапер\n",
      "gender: masc\n",
      "tense: past\n",
      "Output: ['покатила', 'тоже', 'заре', 'к', 'отеля', '<UNK>', 'развернутой']\n",
      "Target: сапер тоже не привык к подобным приключениям.\n",
      "\n",
      "\n",
      "Input: кудесник вступить в сад.\n",
      "Nsubj: кудесник\n",
      "gender: masc\n",
      "tense: past\n",
      "Output: ['кудесник', 'положении', 'в', 'нуждалась', '.']\n",
      "Target: кудесник вступил в сад.\n",
      "\n",
      "\n",
      "Input: рыцарь молча отъехать в сторона.\n",
      "Nsubj: рыцарь\n",
      "gender: masc\n",
      "tense: past\n",
      "Output: ['предка', 'сел', 'виктора', 'в', 'сторону', '.']\n",
      "Target: рыцарь молча отъехал в сторону.\n",
      "\n",
      "\n",
      "Input: он указать на алиса.\n",
      "Nsubj: он\n",
      "gender: masc\n",
      "tense: past\n",
      "Output: ['он', 'группой', 'на', '<UNK>', '.']\n",
      "Target: он указал на алису.\n",
      "\n",
      "\n",
      "Input: кассандра подняться на нога.\n",
      "Nsubj: кассандра\n",
      "gender: fem\n",
      "tense: past\n",
      "Output: ['утвердился', 'направилась', 'на', 'смысле', '.']\n",
      "Target: кассандра поднялась на ноги.\n",
      "\n",
      "\n",
      "Input: она тихо пойти к выход.\n",
      "Nsubj: она\n",
      "gender: fem\n",
      "tense: past\n",
      "Output: ['она', 'устремился', 'сплюнул', 'к', 'сил', '.']\n",
      "Target: она тихо пошла к выходу.\n",
      "\n",
      "\n",
      "Input: они долго стучать в закрытый ворота.\n",
      "Nsubj: они\n",
      "gender: undefined\n",
      "tense: past\n",
      "Output: ['они', 'шахту', '<UNK>', 'в', '<UNK>', '<UNK>', '.']\n",
      "Target: они долго стучали в закрытые ворота.\n",
      "\n",
      "\n",
      "Input: ты возиться с ребёнок.\n",
      "Nsubj: ты\n",
      "gender: undefined\n",
      "tense: pres\n",
      "Output: ['ты', 'термометр', 'с', 'лилиан', '.']\n",
      "Target: ты возишься с детьми.\n",
      "\n",
      "\n",
      "Input: он присесть на повалить дерево.\n",
      "Nsubj: он\n",
      "gender: masc\n",
      "tense: past\n",
      "Output: ['он', 'еврейском', 'на', '<UNK>', '<UNK>', '.']\n",
      "Target: он присел на поваленное дерево.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for input_sent, target_sent, nsubj, gender, tense in test_zipped:\n",
    "    output = evaluate(model, input_sent, (nsubj, gender, tense))\n",
    "    \n",
    "    print(f'Input: {input_sent}')\n",
    "    print(f'Nsubj: {nsubj}\\ngender: {gender}\\ntense: {tense}')\n",
    "    print(f'Output: {output}')\n",
    "    print(f'Target: {target_sent}')\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
