{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Импорт необходимых зависимостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "import time\n",
    "import pickle\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from random import random, sample\n",
    "from typing import List\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from functools import reduce\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/simple_sentences/processed/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemm_texts</th>\n",
       "      <th>orig_texts</th>\n",
       "      <th>nsubj</th>\n",
       "      <th>gender</th>\n",
       "      <th>tense</th>\n",
       "      <th>number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>я предлагать оригинальный подарок для малыш!</td>\n",
       "      <td>я предлагаю оригинальный подарок для малыша!</td>\n",
       "      <td>я</td>\n",
       "      <td>undefined</td>\n",
       "      <td>pres</td>\n",
       "      <td>sing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>я обезательный перезвонить в любой случай.</td>\n",
       "      <td>я обезательно перезвоню в любом случае.</td>\n",
       "      <td>я</td>\n",
       "      <td>undefined</td>\n",
       "      <td>fut</td>\n",
       "      <td>sing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>цена на память я не помнить.</td>\n",
       "      <td>цены на память я не помню.</td>\n",
       "      <td>я</td>\n",
       "      <td>undefined</td>\n",
       "      <td>pres</td>\n",
       "      <td>sing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>я не помнить , где находиться.</td>\n",
       "      <td>я не помню, где находились.</td>\n",
       "      <td>я</td>\n",
       "      <td>undefined</td>\n",
       "      <td>pres</td>\n",
       "      <td>sing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>я работать на высококачественный американский ...</td>\n",
       "      <td>я работаю на высококачественных американских м...</td>\n",
       "      <td>я</td>\n",
       "      <td>undefined</td>\n",
       "      <td>pres</td>\n",
       "      <td>sing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356967</th>\n",
       "      <td>другой ящерица медленно подбрести к свой товарка.</td>\n",
       "      <td>другая ящерица медленно подбрела к своей товарке.</td>\n",
       "      <td>ящерица</td>\n",
       "      <td>fem</td>\n",
       "      <td>past</td>\n",
       "      <td>sing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356968</th>\n",
       "      <td>зелёный ящерица застылый на мраморный ступень.</td>\n",
       "      <td>зеленая ящерица застыла на мраморной ступени.</td>\n",
       "      <td>ящерица</td>\n",
       "      <td>fem</td>\n",
       "      <td>past</td>\n",
       "      <td>sing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356969</th>\n",
       "      <td>больший ящерица шмыгнуть по песок.</td>\n",
       "      <td>большая ящерица шмыгнула по песку.</td>\n",
       "      <td>ящерица</td>\n",
       "      <td>fem</td>\n",
       "      <td>past</td>\n",
       "      <td>sing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356970</th>\n",
       "      <td>домашний ящерица быстро пробежать вдоль штора.</td>\n",
       "      <td>домашняя ящерица быстро пробежала вдоль штор.</td>\n",
       "      <td>ящерица</td>\n",
       "      <td>fem</td>\n",
       "      <td>past</td>\n",
       "      <td>sing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356971</th>\n",
       "      <td>крошечный ящерка сбежать с валун.</td>\n",
       "      <td>крошечная ящерка сбежала с валуна.</td>\n",
       "      <td>ящерка</td>\n",
       "      <td>fem</td>\n",
       "      <td>past</td>\n",
       "      <td>sing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>356972 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               lemm_texts  \\\n",
       "0            я предлагать оригинальный подарок для малыш!   \n",
       "1              я обезательный перезвонить в любой случай.   \n",
       "2                            цена на память я не помнить.   \n",
       "3                          я не помнить , где находиться.   \n",
       "4       я работать на высококачественный американский ...   \n",
       "...                                                   ...   \n",
       "356967  другой ящерица медленно подбрести к свой товарка.   \n",
       "356968     зелёный ящерица застылый на мраморный ступень.   \n",
       "356969                 больший ящерица шмыгнуть по песок.   \n",
       "356970     домашний ящерица быстро пробежать вдоль штора.   \n",
       "356971                  крошечный ящерка сбежать с валун.   \n",
       "\n",
       "                                               orig_texts    nsubj     gender  \\\n",
       "0            я предлагаю оригинальный подарок для малыша!        я  undefined   \n",
       "1                 я обезательно перезвоню в любом случае.        я  undefined   \n",
       "2                              цены на память я не помню.        я  undefined   \n",
       "3                             я не помню, где находились.        я  undefined   \n",
       "4       я работаю на высококачественных американских м...        я  undefined   \n",
       "...                                                   ...      ...        ...   \n",
       "356967  другая ящерица медленно подбрела к своей товарке.  ящерица        fem   \n",
       "356968      зеленая ящерица застыла на мраморной ступени.  ящерица        fem   \n",
       "356969                 большая ящерица шмыгнула по песку.  ящерица        fem   \n",
       "356970      домашняя ящерица быстро пробежала вдоль штор.  ящерица        fem   \n",
       "356971                 крошечная ящерка сбежала с валуна.   ящерка        fem   \n",
       "\n",
       "       tense number  \n",
       "0       pres   sing  \n",
       "1        fut   sing  \n",
       "2       pres   sing  \n",
       "3       pres   sing  \n",
       "4       pres   sing  \n",
       "...      ...    ...  \n",
       "356967  past   sing  \n",
       "356968  past   sing  \n",
       "356969  past   sing  \n",
       "356970  past   sing  \n",
       "356971  past   sing  \n",
       "\n",
       "[356972 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fit_contexts(df):\n",
    "#     contexts = []\n",
    "#     for nsubj, gender, tense in zip(df['nsubj'], df['gender'], df['tense']):\n",
    "#         context = []\n",
    "#         context.append(nsubj)\n",
    "\n",
    "#         if gender == 'undefined': context.append(0)\n",
    "#         elif gender == 'masc': context.append(1)\n",
    "#         elif gender == 'fem': context.append(2)\n",
    "#         else: context.append(3)\n",
    "\n",
    "#         if tense == 'past': context.append(0)\n",
    "#         elif tense == 'pres': context.append(1)\n",
    "#         elif tense == 'fut': context.append(2)    \n",
    "#         else: context.append(3)\n",
    "#         contexts.append(context)\n",
    "        \n",
    "#     return contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Определение классов словаря и трансформера текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, tokens: List[str], unk_idx: int):\n",
    "        self._tokens = tokens\n",
    "        self._token_to_idx = {token: idx for idx, token in enumerate(tokens)}\n",
    "        self._unk_idx = unk_idx\n",
    "        \n",
    "    def token_to_idx(self, token: str) -> int:\n",
    "        return self._token_to_idx.get(token, self._unk_idx)\n",
    "    \n",
    "    def idx_to_token(self, idx: int) -> str:\n",
    "        return self._tokens[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTransformer:\n",
    "    def __init__(self, vocab_size: int):\n",
    "        self.vocab = None\n",
    "        self.vocab_size = vocab_size\n",
    "        self.special_tokens_to_idx = {'<UNK>': 0, '<PAD>': 1, '<SOS>': 2, '<EOS>': 3}\n",
    "        self._tokenizer = nltk.tokenize.wordpunct_tokenize\n",
    "    \n",
    "    def tokenize(self, text) -> List[str]:\n",
    "        return self._tokenizer(text.lower())\n",
    "    \n",
    "    def build_vocab(self, tokens: List[str]):\n",
    "        tokens_ = [special_token for special_token in self.special_tokens_to_idx.keys()]\n",
    "        special_tokens_amount = len(self.special_tokens_to_idx)\n",
    "        \n",
    "        for token, _ in Counter(tokens).most_common(self.vocab_size - special_tokens_amount):\n",
    "            tokens_.append(token)\n",
    "        \n",
    "        unk_idx = self.special_tokens_to_idx.get('<UNK>')\n",
    "        self.vocab = Vocab(tokens_, unk_idx)\n",
    "        \n",
    "    def transform_text(self, text: str) -> List[int]:\n",
    "        tokenized_text = self.tokenize(text)\n",
    "        transformed = [self.vocab.token_to_idx(token) for token in tokenized_text]\n",
    "        return transformed\n",
    "    \n",
    "    def fit_transform(self, texts: List[str]) -> None:\n",
    "        transformed_texts = []\n",
    "        \n",
    "        tokenized_texts = [self.tokenize(text) for text in tqdm(texts, 'Tokenizing texts')]\n",
    "        tokens = chain(*tokenized_texts)\n",
    "        self.build_vocab(tokens)\n",
    "        \n",
    "        for tokenized_text in tqdm(tokenized_texts, 'Transforming texts'):\n",
    "            transformed = [self.vocab.token_to_idx(token) for token in tokenized_text]\n",
    "            transformed_texts.append(transformed)\n",
    "    \n",
    "    def transform_texts(self, texts: List[str]) -> List[List[int]]:\n",
    "        transformed_texts = [transform_text(text) for text in tqdm(texts, 'Transforming texts')]\n",
    "        return transformed_texts\n",
    "    \n",
    "    def text_to_tensor(self, text: str, max_seq_len=8) -> torch.tensor:\n",
    "        transformed_text = self.transform_text(text)\n",
    "        pad_idx = self.special_tokens_to_idx.get('<PAD>')\n",
    "        sos_idx = self.special_tokens_to_idx.get('<SOS>')\n",
    "        eos_idx = self.special_tokens_to_idx.get('<EOS>')\n",
    "        \n",
    "        pad_size = 0\n",
    "        if len(transformed_text) >= max_seq_len:\n",
    "            transformed_text = transformed_text[:max_seq_len]\n",
    "        else:\n",
    "            pad_size = max_seq_len - len(transformed_text)\n",
    "            transformed_text.extend([pad_idx] * pad_size)   \n",
    "        transformed_text.insert(0, sos_idx)\n",
    "        transformed_text.insert(len(transformed_text) - pad_size, eos_idx)\n",
    "        \n",
    "        tensor = torch.tensor(transformed_text, dtype=torch.long)\n",
    "        return tensor.unsqueeze(0)\n",
    "    \n",
    "    def texts_to_tensor(self, texts: List[str], max_seq_len=8) -> torch.tensor:\n",
    "        pad_idx = self.special_tokens_to_idx.get('<PAD>')\n",
    "        sos_idx = self.special_tokens_to_idx.get('<SOS>')\n",
    "        eos_idx = self.special_tokens_to_idx.get('<EOS>')\n",
    "        transformed_texts = []\n",
    "        \n",
    "        for text in tqdm(texts, 'Building tensor'):\n",
    "            transformed_text = self.transform_text(text)\n",
    "            pad_size = 0\n",
    "            if len(transformed_text) >= max_seq_len:\n",
    "                transformed_text = transformed_text[:max_seq_len]\n",
    "            else:\n",
    "                pad_size = max_seq_len - len(transformed_text)\n",
    "                transformed_text.extend([pad_idx] * pad_size)   \n",
    "            transformed_text.insert(0, sos_idx)\n",
    "            transformed_text.insert(len(transformed_text) - pad_size, eos_idx)\n",
    "            transformed_texts.append(transformed_text)\n",
    "        \n",
    "        tensor = torch.tensor(transformed_texts, dtype=torch.long).permute(1, 0)\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разбиение данных на обучающую, тестовую и валидационную выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = model_selection.train_test_split(df, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df, val_df = model_selection.train_test_split(test_df, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Токенизация текстов и индексация токенов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemm_vocab_size = 35000\n",
    "orig_vocab_size = 60000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemm_text_transformer = TextTransformer(lemm_vocab_size)\n",
    "orig_text_transformer = TextTransformer(orig_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2d894454cdf43dda5e68490e3c1bc0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing texts:   0%|          | 0/321274 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e00c91e6f704abf885a5b58eb2a6eb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transforming texts:   0%|          | 0/321274 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lemm_text_transformer.fit_transform(train_df.lemm_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c07d57906247482c8bf57454c4c9c05f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing texts:   0%|          | 0/321274 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cf69d1e11d04c77819bfe5b5639c496",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transforming texts:   0%|          | 0/321274 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "orig_text_transformer.fit_transform(train_df.orig_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Перевод данных в тензоры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "594d251130a24fd98900e8c70a12fd82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building tensor:   0%|          | 0/321274 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d06ecac39b0a4fa980fbeb0a8f518f3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building tensor:   0%|          | 0/26773 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f510e6d1ac3e490fb5ffa421b2d109b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building tensor:   0%|          | 0/8925 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_lemm_tensor = lemm_text_transformer.texts_to_tensor(train_df.lemm_texts.to_list())\n",
    "test_lemm_tensor = lemm_text_transformer.texts_to_tensor(test_df.lemm_texts.to_list())\n",
    "val_lemm_tensor = lemm_text_transformer.texts_to_tensor(val_df.lemm_texts.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3993eb4ad5b4a889f58b887ace71355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building tensor:   0%|          | 0/321274 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6b5a3182d3349e288b848ee32f1df13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building tensor:   0%|          | 0/26773 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74509106d0c5490d83fdd23a1a451fc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building tensor:   0%|          | 0/8925 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_orig_tensor = orig_text_transformer.texts_to_tensor(train_df.orig_texts.to_list())\n",
    "test_orig_tensor = orig_text_transformer.texts_to_tensor(test_df.orig_texts.to_list())\n",
    "val_orig_tensor = orig_text_transformer.texts_to_tensor(val_df.orig_texts.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_context(df, df_type: str):\n",
    "    gender_rows = pd.get_dummies(df.gender).iterrows()\n",
    "    tense_rows = pd.get_dummies(df.tense).iterrows()\n",
    "    nsubj_to_idx = orig_text_transformer.vocab.token_to_idx\n",
    "    \n",
    "    transformed_genders = [row[1].to_list() for row in tqdm(gender_rows, f'Transforming genders ({df_type})')]\n",
    "    transformed_tenses = [row[1].to_list() for row in tqdm(tense_rows, f'Transforming tenses ({df_type})')]\n",
    "    transformed_nsubjes = [nsubj_to_idx(nsubj) for nsubj in tqdm(df.nsubj, f'Transforming nsubjes ({df_type})')]\n",
    "    \n",
    "    context = [transformed_nsubjes, transformed_genders, transformed_tenses]\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab8a2f8ee39b45ab9e778b2e1cb57ecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transforming genders (train): 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cda0d47aafd4431a7ffc83db8322776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transforming tenses (train): 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef8ddda31c564aa7b63a1e451088826e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transforming nsubjes (train):   0%|          | 0/321274 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d6e7e440ada436eb3541e5e3f20aac6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transforming genders (test): 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e0b3f65a5e44c88ac4dcb52345d0447",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transforming tenses (test): 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce628f18dd8d423baba4afc01b8c4d95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transforming nsubjes (test):   0%|          | 0/26773 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2970dca59c174742b2e893f7dccf9c53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transforming genders (validation): 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d48464f9ab8c4a17b0c79b5b9cf3d782",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transforming tenses (validation): 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8b1c1dbac464e30a4b3bac7aea9cd5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transforming nsubjes (validation):   0%|          | 0/8925 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_context = transform_context(train_df, 'train')\n",
    "test_context = transform_context(test_df, 'test')\n",
    "val_context = transform_context(val_df, 'validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_to_tensors(nsubj_list, gender_list, tense_list):\n",
    "    nsubj_tensor = torch.tensor(nsubj_list)\n",
    "    gender_tensor = torch.tensor(gender_list, dtype=torch.float32)\n",
    "    tense_tensor = torch.tensor(tense_list, dtype=torch.float32)\n",
    "    \n",
    "    context_tensors = [nsubj_tensor, gender_tensor, tense_tensor]\n",
    "    return context_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_context_tensors = context_to_tensors(*train_context)\n",
    "test_context_tensors = context_to_tensors(*test_context)\n",
    "val_context_tensors = context_to_tensors(*val_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformed_train_contexts = transform_contexts(train_contexts)\n",
    "# transformed_train_contexts_tensor = torch.tensor(transformed_train_contexts, dtype=torch.float32)\n",
    "\n",
    "# transformed_test_contexts = transform_contexts(test_contexts)\n",
    "# transformed_test_contexts_tensor = torch.tensor(transformed_test_contexts, dtype=torch.long)\n",
    "\n",
    "# transformed_val_contexts = transform_contexts(val_contexts)\n",
    "# transformed_val_contexts_tensor = torch.tensor(transformed_val_contexts, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformed_train_contexts_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_to_fit_batch(tensor: torch.Tensor, batch_size: int):\n",
    "    n_samples = tensor.shape[1]\n",
    "    new_n_samples = (n_samples // batch_size) * batch_size\n",
    "    result, _ = tensor.split(new_n_samples, dim=1)\n",
    "    return torch.transpose(result, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ContextMem(nn.Module):\n",
    "#     def __init__(self, input_dim, output_dim, hidden_ff):\n",
    "#         super(ContextMem, self).__init__()\n",
    "#         self.fc_norm = nn.Linear(input_dim, hidden_ff)\n",
    "#         self.hff     = nn.Linear(hidden_ff, output_dim)\n",
    "#         self.fc_gate = nn.Linear(output_dim, output_dim)\n",
    "\n",
    "#     def forward(self, context):\n",
    "#         #context shape = (batch_size, input_dim=3)\n",
    "#         context = self.fc_norm(context)\n",
    "#         #context shape = (batch_size, hidden_ff)\n",
    "#         context = self.hff(context)\n",
    "#         #context shape = (batch_size, output_dim)\n",
    "#         context_norm = F.tanh(context)\n",
    "        \n",
    "#         #context shape = (batch_size, output_dim)\n",
    "#         context_gate = self.fc_gate(context)\n",
    "#         #context_gate shape = (batch_size, output_dim)\n",
    "#         context_gate = F.sigmoid(context)\n",
    "\n",
    "#         return context_norm * context_gate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Построение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextMem(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, nsubj_embedding_size, device):\n",
    "        super(ContextMem, self).__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        self.gender_proj = nn.Linear(input_size, hidden_size, bias=False)\n",
    "        self.tense_proj = nn.Linear(input_size, hidden_size, bias=False)\n",
    "        self.fc_out = nn.Linear(hidden_size * 2 + nsubj_embedding_size, output_size, bias=False)\n",
    "#         self.nsubj_proj  = nn.Linear(embedding_size, hidden_size, bias=False)\n",
    "        \n",
    "        \n",
    "    def forward(self, nsubj_embedding, gender, tense):\n",
    "        # nsubj_embedding_shape: (batch_size, embedding_size)\n",
    "        # gender_shape: (batch_size, input_size)\n",
    "        # tense_shape: (batch_size, input_size)\n",
    "        \n",
    "        gender = self.gender_proj(gender)\n",
    "        # gender_shape: (batch_size, hidden_size)\n",
    "        \n",
    "        tense = self.tense_proj(tense)\n",
    "        # tense_shape: (batch_size, hidden_size)    \n",
    "        \n",
    "        context = torch.cat([nsubj_embedding, gender, tense], dim=1)\n",
    "        # context_shape: (batch_size, hidden_size * 2 + embedding_size) \n",
    "        \n",
    "        context = self.fc_out(context)\n",
    "        # context_shape: (batch_size, output_size)\n",
    "        \n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fc1 = nn.Linear(4, 256, bias=False)\n",
    "# fc2 = nn.Linear(4, 256, bias=False)\n",
    "# fc3 = nn.Linear(256 * 2 + 300, 1024, bias=False)\n",
    "# emb = torch.rand(32, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inp1 = torch.rand(32, 4)\n",
    "# inp2 = torch.rand(32, 4)\n",
    "\n",
    "# out1 = fc1(inp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out3 = torch.cat([emb, out1, out2], dim=1)\n",
    "# out4 = fc3(out3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cat([out4.unsqueeze(0), out4.unsqueeze(0)], 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size, embedding_size, hidden_size,\n",
    "                 pad_idx, device, num_layers, dropout_p):\n",
    "        \n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, embedding_size, pad_idx),\n",
    "            nn.Dropout(dropout_p)\n",
    "        )\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout_p)\n",
    "        \n",
    "    def forward(self, x, hidden, cell):\n",
    "        # x_shape: (seq_len, batch_size)\n",
    "        embedding = self.embedding(x)\n",
    "        # embedding_shape: (seq_len, batch_size, embedding_size)\n",
    "        output, (hidden, cell) = self.lstm(embedding, (hidden, cell))\n",
    "        # output_shape: (seq_len, batch_size, hidden_size)\n",
    "        # hidden_shape: (num_layers, batch_size, hidden_size)\n",
    "        # cell_shape: (num_layers, batch_size, hidden_size)\n",
    "        return hidden, cell\n",
    "    \n",
    "#     def init_hidden_state(self, context, batch_size: int):\n",
    "#         hidden = context_mem(*context)\n",
    "#         cell = context_mem(*context)\n",
    "#         return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size, embedding_size, hidden_size, output_size,\n",
    "                 pad_idx, device, num_layers, dropout_p):\n",
    "        \n",
    "        super(DecoderRNN, self).__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, embedding_size, pad_idx),\n",
    "            nn.Dropout(dropout_p)\n",
    "        )\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout_p)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x, hidden, cell):\n",
    "        x = x.unsqueeze(0)\n",
    "        # x_shape:      (seq_len=1, batch_size)\n",
    "        # hidden_shape: (num_layers, batch_size, hidden_size)\n",
    "        # cell_shape:   (num_layers, batch_size, hidden_size)\n",
    "        \n",
    "        embedding = self.embedding(x)\n",
    "        # embedding_shape: (seq_len=1, batch_size, embedding_size)\n",
    "        \n",
    "        lstm_out, (hidden, cell) = self.lstm(embedding, (hidden, cell))\n",
    "        # lstm_out_shape: (seq_len=1, batch_size, hidden_size)\n",
    "        \n",
    "        fc_out = self.fc(lstm_out)\n",
    "        # fc_out_shape: (seq_len=1, batch_size, output_size)\n",
    "        # output_shape: (seq_len=1, batch_size, output_size)\n",
    "        \n",
    "        return fc_out, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 encoder_vocab_size, decoder_vocab_size,\n",
    "                 embedding_size, hidden_size, output_size,\n",
    "                 context_input_size, context_hidden_size, context_output_size,\n",
    "                 pad_idx, device, num_layers, dropout_p):\n",
    "        \n",
    "        super(Seq2SeqModel, self).__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.decoder_vocab_size = decoder_vocab_size\n",
    "        \n",
    "        self.context_mem = ContextMem(context_input_size, context_hidden_size, context_output_size, embedding_size, device).to(device)\n",
    "        self.encoder = EncoderRNN(encoder_vocab_size, embedding_size, hidden_size, pad_idx, device, num_layers, dropout_p).to(device)\n",
    "        self.decoder = DecoderRNN(decoder_vocab_size, embedding_size, hidden_size, output_size, pad_idx, device, num_layers, dropout_p).to(device)\n",
    "        \n",
    "    def forward(self, input, target, context, teacher_forcing_ratio=0.5):\n",
    "        batch_size = input.shape[1]\n",
    "        target_len = target.shape[0]\n",
    "        target_vocab_size = self.decoder_vocab_size\n",
    "        \n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size, device=self.device)\n",
    "\n",
    "        nsubj, gender, tense = context\n",
    "        # nsubj_shape:  (batch_size)\n",
    "        # gender_shape: (batch_size, context_input_size)\n",
    "        # tense_shape:  (batch_size, context_input_size)\n",
    "        \n",
    "        nsubj_embedding = self.decoder.embedding(nsubj).squeeze(0)\n",
    "        # nsubj_embedding_shape: (batch_size, embedding_size)\n",
    "        \n",
    "        hidden = self.context_mem(nsubj_embedding, gender, tense)\n",
    "        cell = hidden.clone()\n",
    "        # hidden, cell shapes: (batch_size, context_output_size=hidden_size)\n",
    "        \n",
    "        if self.num_layers == 1:\n",
    "            hidden.unsqueeze_(0)\n",
    "            cell.unsqueeze_(0)\n",
    "            # hidden, cell shapes: (1, batch_size, context_output_size=hidden_size)\n",
    "        else:\n",
    "            hidden = torch.cat([hidden.unsqueeze(0)] * self.num_layers, 0)\n",
    "            cell = torch.cat([cell.unsqueeze(0)] * self.num_layers, 0)\n",
    "            # hidden, cell shapes: (num_layers, batch_size, context_output_size=hidden_size)\n",
    "        \n",
    "        hidden, cell = self.encoder(input, hidden, cell)\n",
    "        # hidden, cell shapes: (num_layers, batch_size, hidden_size)\n",
    "        \n",
    "        prev_token_idx = target[0]\n",
    "        # prev_token_shape: (batch_size)\n",
    "        \n",
    "        for t in range(target_len):\n",
    "            output, hidden, cell = self.decoder(prev_token_idx, hidden, cell)\n",
    "            outputs[t] = output.squeeze(0)\n",
    "            \n",
    "            best_prediction = outputs[t].argmax(dim=1)\n",
    "            # best_prediction_shape: (batch_size)\n",
    "            prev_token_idx = target[t] if random() < teacher_forcing_ratio else best_prediction\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция сохранения текущего состояния модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model: Seq2SeqModel, optimizer, epoch, path):\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'criterion': criterion,\n",
    "        'epoch': epoch\n",
    "    }\n",
    "    \n",
    "    torch.save(checkpoint, path)\n",
    "#     with open(path, mode='wb') as f:\n",
    "#         pickle.dump(checkpoint, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция загрузки уже тренировавшейся модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model: Seq2SeqModel, optimizer, criterion, path):\n",
    "#     with open(path, mode='rb') as f:\n",
    "#         checkpoint = pickle.load(f)\n",
    "    checkpoint = torch.load(path)\n",
    "        \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    criterion = checkpoint['criterion']\n",
    "    \n",
    "    return epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Инициализация гиперпараметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "epochs_amount = 15\n",
    "lemm_vocab_size = 35000\n",
    "orig_vocab_size = 60000\n",
    "hidden_size = 1024\n",
    "embedding_size = 300\n",
    "num_layers = 2\n",
    "max_norm = 1.0\n",
    "dropout_p = 0.4\n",
    "context_input_size = 4\n",
    "context_hidden_size = 512\n",
    "context_output_size = hidden_size\n",
    "patience = 3\n",
    "output_size = orig_vocab_size\n",
    "pad_idx = lemm_text_transformer.special_tokens_to_idx.get('<PAD>')\n",
    "model_path = './models/'\n",
    "model_name = 'simple_seq2seq_with_context.model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2SeqModel(lemm_vocab_size, orig_vocab_size, embedding_size, hidden_size, output_size,\n",
    "                     context_input_size, context_hidden_size, context_output_size,\n",
    "                     pad_idx, device, num_layers, dropout_p).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No models found at ./models/\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    epoch = load_model(model, optimizer, criterion, model_path + model_name)\n",
    "    print(f'Loaded model from {model_path}')\n",
    "except:\n",
    "    print(f'No models found at {model_path}')\n",
    "    epoch = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Урезание данных для соответствия размеру батча"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lemm_tensor_f = cut_to_fit_batch(train_lemm_tensor, batch_size)\n",
    "train_orig_tensor_f = cut_to_fit_batch(train_orig_tensor, batch_size)\n",
    "\n",
    "test_lemm_tensor_f = cut_to_fit_batch(test_lemm_tensor, batch_size)\n",
    "test_orig_tensor_f = cut_to_fit_batch(test_orig_tensor, batch_size)\n",
    "\n",
    "val_lemm_tensor_f = cut_to_fit_batch(val_lemm_tensor, batch_size)\n",
    "val_orig_tensor_f = cut_to_fit_batch(val_orig_tensor, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_context_tensors_f = [cut_to_fit_batch(tensor.unsqueeze(0), batch_size).squeeze(1) for tensor in train_context_tensors]\n",
    "test_context_tensors_f = [cut_to_fit_batch(tensor.unsqueeze(0), batch_size).squeeze(1) for tensor in test_context_tensors]\n",
    "val_context_tensors_f = [cut_to_fit_batch(tensor.unsqueeze(0), batch_size).squeeze(1) for tensor in val_context_tensors]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Инициализация данных итерируемых по батчам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(train_lemm_tensor_f, train_orig_tensor_f, *train_context_tensors_f)\n",
    "test_dataset = TensorDataset(test_lemm_tensor_f, test_orig_tensor_f, *test_context_tensors_f)\n",
    "val_dataset = TensorDataset(val_lemm_tensor_f, val_orig_tensor_f, *val_context_tensors_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Определение функции проверки работы сети между эпохами обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_evaluate(model, input, target_len=8):\n",
    "    input = input.to(device)\n",
    "    sos_idx = lemm_text_transformer.special_tokens_to_idx.get('<SOS>')\n",
    "    eos_idx = lemm_text_transformer.special_tokens_to_idx.get('<EOS>')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        hidden, cell = model.encoder(input)\n",
    "        \n",
    "        predicted_indexes = [sos_idx]\n",
    "        \n",
    "        for _ in range(1, target_len):\n",
    "            prev_idx = torch.tensor([predicted_indexes[-1]], dtype=torch.long, device=device)\n",
    "            \n",
    "            output, hidden, cell = model.decoder(prev_idx, hidden, cell)\n",
    "            output = output.squeeze(0)\n",
    "            \n",
    "            best_prediction = output.argmax(dim=1).item()\n",
    "            \n",
    "            if best_prediction == eos_idx:\n",
    "                break\n",
    "                \n",
    "            predicted_indexes.append(best_prediction)\n",
    "                        \n",
    "        \n",
    "    predicted_tokens = [orig_text_transformer.vocab.idx_to_token(idx) for idx in predicted_indexes]\n",
    "    return predicted_tokens[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Определение функции обучения сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, train_data, val_data, test_data, epochs_amount, max_norm, context, patience=3, current_epoch=1, n_prints=5):\n",
    "    min_mean_val_loss = float('+inf')\n",
    "    initial_patiece = patience\n",
    "    print_every = len(train_data) // n_prints\n",
    "    \n",
    "    for epoch in tqdm(range(current_epoch, epochs_amount + 1), 'Epochs'):\n",
    "        print(f'\\nEpoch [{epoch} / {epochs_amount}]')\n",
    "        \n",
    "        model.train()\n",
    "        for iteration, (input, target, nsubj, gender, tense) in enumerate(tqdm(train_data, 'Epoch training iterations')):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            input = torch.transpose(input, 1, 0).to(device)   \n",
    "            # input_shape: (seq_len, batch_size)\n",
    "            \n",
    "            target = torch.transpose(target, 1, 0).to(device)\n",
    "            # target_shape: (seq_len, batch_size)\n",
    "            \n",
    "            context = (nsubj.to(device), gender.to(device), tense.to(device))\n",
    "            \n",
    "            output = model(input, target, context)\n",
    "            # output_shape: (seq_len, batch_size, orig_vocab_size) but need (N, orig_vocab_size)\n",
    "            \n",
    "            target = target[1:].reshape(-1)\n",
    "            # now target_shape is (seq_len * batch_size)\n",
    "            \n",
    "            orig_vocab_size = output.shape[2]\n",
    "            \n",
    "            output = output[1:].reshape(-1, orig_vocab_size)\n",
    "            # now output_shape is (seq_len * batch_size, orig_vocab_size)\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            if iteration % print_every == 0:\n",
    "                print(f'\\tIteration #{iteration}: training loss = {loss.item()}')\n",
    "            elif iteration == len(train_data):\n",
    "                print(f'\\tIteration #{iteration}: training loss = {loss.item()}')\n",
    "            \n",
    "            \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            val_loss = []\n",
    "            \n",
    "            for input, target, nsubj, gender, tense in tqdm(val_data, 'Epoch validating iterations'):\n",
    "                input = torch.transpose(input, 1, 0).to(device)\n",
    "                target = torch.transpose(target, 1, 0).to(device)\n",
    "                context = (nsubj.to(device), gender.to(device), tense.to(device))\n",
    "                output = model(input, target, context)\n",
    "                \n",
    "                orig_vocab_size = output.shape[2]\n",
    "                output = output[1:].reshape(-1, orig_vocab_size)\n",
    "                target = target[1:].reshape(-1)\n",
    "                \n",
    "                val_loss.append(criterion(output, target).item())\n",
    "            \n",
    "            mean_val_loss = sum(val_loss) / len(val_loss)\n",
    "            print(f'\\tValidation loss = {mean_val_loss}')\n",
    "            if mean_val_loss < min_mean_val_loss:\n",
    "                try:\n",
    "                    save_model(model, optimizer, epoch, model_path)\n",
    "                    min_mean_val_loss = mean_val_loss\n",
    "                    patience = initial_patiece\n",
    "                except Exception as exc:\n",
    "                    print(exc)\n",
    "            else:\n",
    "                patience -= 1\n",
    "            \n",
    "#             test_data = DataLoader(test_data.dataset, batch_size=1, shuffle=True)\n",
    "#             for input, target in test_data:\n",
    "#                 target = target.squeeze(0).to(device)\n",
    "                \n",
    "#                 input = torch.transpose(input, 1, 0)\n",
    "#                 target_len = target.shape[0]\n",
    "                \n",
    "#                 output = test_evaluate(model, input, target_len)\n",
    "#                 decoded_input = [lemm_text_transformer.vocab.idx_to_token(idx.item()) for idx in input]\n",
    "#                 decoded_target = [orig_text_transformer.vocab.idx_to_token(idx.item()) for idx in target]\n",
    "                \n",
    "#                 print(f'\\tInput: {decoded_input}')\n",
    "#                 print(f'\\tOutput: {output}')\n",
    "#                 print(f'\\tTarget: {decoded_target}')\n",
    "#                 break\n",
    "        \n",
    "        if patience == 0:\n",
    "            print(f'\\nModel learning finished due to early stopping')\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Определение функции эксплуатации обученной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: Seq2SeqModel, sentence: str, max_seq_len=8):\n",
    "    input_tensor = lemm_text_transformer.text_to_tensor(sentence, max_seq_len).to(device)\n",
    "    input_tensor = torch.transpose(input_tensor, 1, 0)\n",
    "    sos_idx = lemm_text_transformer.special_tokens_to_idx.get('<SOS>')\n",
    "    eos_idx = lemm_text_transformer.special_tokens_to_idx.get('<EOS>')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        hidden, cell = model.encoder(input_tensor)\n",
    "        \n",
    "        predicted_indexes = [sos_idx]\n",
    "        \n",
    "#         while True:\n",
    "#             prev_idx = torch.tensor([predicted_indexes[-1]], dtype=torch.long, device=device)\n",
    "            \n",
    "#             output, hidden, cell = model.decoder(prev_idx, hidden, cell)\n",
    "#             output = output.squeeze(0)\n",
    "            \n",
    "#             best_prediction = output.argmax(dim=1).item()\n",
    "            \n",
    "#             if best_prediction == eos_idx:\n",
    "#                 break\n",
    "            \n",
    "#             predicted_indexes.append(best_prediction)\n",
    "                       \n",
    "        \n",
    "        for _ in range(1, max_seq_len):\n",
    "            prev_idx = torch.tensor([predicted_indexes[-1]], dtype=torch.long, device=device)\n",
    "            \n",
    "            output, hidden, cell = model.decoder(prev_idx, hidden, cell)\n",
    "            output = output.squeeze(0)\n",
    "            \n",
    "            best_prediction = output.argmax(dim=1).item()\n",
    "            \n",
    "            if best_prediction == eos_idx:\n",
    "                break\n",
    "                \n",
    "            predicted_indexes.append(best_prediction)\n",
    "        \n",
    "    predicted_tokens = [orig_text_transformer.vocab.idx_to_token(idx) for idx in predicted_indexes]\n",
    "    return predicted_tokens[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4293fd91c3f64f5faf96bbfc16e8f596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [1 / 15]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "498fc37024a74000a0a5a05eb03d8172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch training iterations:   0%|          | 0/10039 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tIteration #0: training loss = 11.00257682800293\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-e37acd47fd24>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs_amount\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-44-e26657ebf176>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, criterion, train_data, val_data, test_data, epochs_amount, max_norm, context, patience, current_epoch, n_prints)\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m             \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\iluxa\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\utils\\clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[1;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mclip_coef\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_norm\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtotal_norm\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1e-6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mclip_coef\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclip_coef\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, optimizer, criterion, train_loader, val_loader, test_loader, epochs_amount, max_norm, patience, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_model(model, optimizer, criterion, model_path + model_name)\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# del model\n",
    "# del optimizer\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_sample = train_df.sample(100)\n",
    "# test_input = test_sample.lemm_texts.to_list()\n",
    "# test_target = test_sample.orig_texts.to_list()\n",
    "# test_pair = list(zip(test_input, test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for input_sentence, target_sentence in test_pair[:10]:\n",
    "#     model_output = evaluate(model, input_sentence)\n",
    "#     print(f'Input: {input_sentence}')\n",
    "#     print(f'Output: {model_output}')\n",
    "#     print(f'Target: {target_sentence}')\n",
    "#     print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
