{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Импорт необходимых зависимостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from random import random, sample\n",
    "from typing import List\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from functools import reduce\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn import model_selection\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchtext.data.metrics import bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RANDOM_STATE = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/lenta/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orig_texts</th>\n",
       "      <th>lemm_texts</th>\n",
       "      <th>nsubj</th>\n",
       "      <th>gender</th>\n",
       "      <th>tense</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1245806</th>\n",
       "      <td>об этом сообщает риа новости со ссылкой на мат...</td>\n",
       "      <td>о это сообщать риа новость с ссылка на мать по...</td>\n",
       "      <td>риа</td>\n",
       "      <td>neut</td>\n",
       "      <td>pres</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594042</th>\n",
       "      <td>генеральный прокурор рф владимир устинов счита...</td>\n",
       "      <td>генеральный прокурор рф владимир устинов счита...</td>\n",
       "      <td>прокурор</td>\n",
       "      <td>masc</td>\n",
       "      <td>pres</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705659</th>\n",
       "      <td>телеканал «дождь» восстановил вещание, прерван...</td>\n",
       "      <td>телеканал « дождь » восстановить вещание, прер...</td>\n",
       "      <td>телеканал</td>\n",
       "      <td>masc</td>\n",
       "      <td>past</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603796</th>\n",
       "      <td>соответствующее требование прозвучало во время...</td>\n",
       "      <td>соответствующий требование прозвучать в время ...</td>\n",
       "      <td>требование</td>\n",
       "      <td>neut</td>\n",
       "      <td>past</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1430273</th>\n",
       "      <td>в пятницу вечером на сайтах \"единой россии\", \"...</td>\n",
       "      <td>в пятница вечером на сайт \"единый россия\", \"гр...</td>\n",
       "      <td>заявления</td>\n",
       "      <td>neut</td>\n",
       "      <td>past</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1331395</th>\n",
       "      <td>об этом заявил президент - председатель правле...</td>\n",
       "      <td>о это заявить президент - председатель правлен...</td>\n",
       "      <td>президент</td>\n",
       "      <td>masc</td>\n",
       "      <td>past</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1710395</th>\n",
       "      <td>\"мы думаем, что они (страны-члены совбеза) пре...</td>\n",
       "      <td>\\\" мы думать, что они (страна-член совбез) пре...</td>\n",
       "      <td>мы</td>\n",
       "      <td>undefined</td>\n",
       "      <td>pres</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1434398</th>\n",
       "      <td>социологи \"росгосстраха\" оценили сознательност...</td>\n",
       "      <td>социолог \"росгосстрах\"оценить сознательность р...</td>\n",
       "      <td>социологи</td>\n",
       "      <td>masc</td>\n",
       "      <td>past</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1832873</th>\n",
       "      <td>российская сборная сохранила за собой 24 строчку.</td>\n",
       "      <td>российский сборная сохранить за себя 24 строчка.</td>\n",
       "      <td>сборная</td>\n",
       "      <td>fem</td>\n",
       "      <td>past</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537488</th>\n",
       "      <td>эксперты компании связывают это с прекращением...</td>\n",
       "      <td>эксперт компания связывать это с прекращение и...</td>\n",
       "      <td>эксперты</td>\n",
       "      <td>masc</td>\n",
       "      <td>pres</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>926362 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                orig_texts  \\\n",
       "1245806  об этом сообщает риа новости со ссылкой на мат...   \n",
       "1594042  генеральный прокурор рф владимир устинов счита...   \n",
       "705659   телеканал «дождь» восстановил вещание, прерван...   \n",
       "603796   соответствующее требование прозвучало во время...   \n",
       "1430273  в пятницу вечером на сайтах \"единой россии\", \"...   \n",
       "...                                                    ...   \n",
       "1331395  об этом заявил президент - председатель правле...   \n",
       "1710395  \"мы думаем, что они (страны-члены совбеза) пре...   \n",
       "1434398  социологи \"росгосстраха\" оценили сознательност...   \n",
       "1832873  российская сборная сохранила за собой 24 строчку.   \n",
       "1537488  эксперты компании связывают это с прекращением...   \n",
       "\n",
       "                                                lemm_texts       nsubj  \\\n",
       "1245806  о это сообщать риа новость с ссылка на мать по...         риа   \n",
       "1594042  генеральный прокурор рф владимир устинов счита...    прокурор   \n",
       "705659   телеканал « дождь » восстановить вещание, прер...   телеканал   \n",
       "603796   соответствующий требование прозвучать в время ...  требование   \n",
       "1430273  в пятница вечером на сайт \"единый россия\", \"гр...   заявления   \n",
       "...                                                    ...         ...   \n",
       "1331395  о это заявить президент - председатель правлен...   президент   \n",
       "1710395  \\\" мы думать, что они (страна-член совбез) пре...          мы   \n",
       "1434398  социолог \"росгосстрах\"оценить сознательность р...   социологи   \n",
       "1832873   российский сборная сохранить за себя 24 строчка.     сборная   \n",
       "1537488  эксперт компания связывать это с прекращение и...    эксперты   \n",
       "\n",
       "            gender tense  \n",
       "1245806       neut  pres  \n",
       "1594042       masc  pres  \n",
       "705659        masc  past  \n",
       "603796        neut  past  \n",
       "1430273       neut  past  \n",
       "...            ...   ...  \n",
       "1331395       masc  past  \n",
       "1710395  undefined  pres  \n",
       "1434398       masc  past  \n",
       "1832873        fem  past  \n",
       "1537488       masc  pres  \n",
       "\n",
       "[926362 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(frac=0.5, random_state=RANDOM_STATE)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Определение классов словаря и трансформера текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, tokens: List[str], unk_idx: int):\n",
    "        self._tokens = tokens\n",
    "        self._token_to_idx = {token: idx for idx, token in enumerate(tqdm(tokens, 'Transforming tokens'))}\n",
    "        self._unk_idx = unk_idx\n",
    "        \n",
    "    def token_to_idx(self, token: str) -> int:\n",
    "        return self._token_to_idx.get(token, self._unk_idx)\n",
    "    \n",
    "    def idx_to_token(self, idx: int) -> str:\n",
    "        return self._tokens[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TextTransformer:\n",
    "    def __init__(self, vocab_size: int = 250000):\n",
    "        self.vocab = None\n",
    "        self.vocab_size = vocab_size\n",
    "        self.special_tokens_to_idx = {'<unk>': 0, '<pad>': 1, '<sos>': 2, '<eos>': 3}\n",
    "#         self.special_tokens_to_idx = None\n",
    "        self._tokenizer = nltk.tokenize.word_tokenize\n",
    "    \n",
    "    def tokenize(self, text, language='russian') -> List[str]:\n",
    "        return self._tokenizer(text.lower(), language)\n",
    "    \n",
    "    def save_vocab(self, path='./vocab.vcb'):\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(self.vocab, f)\n",
    "            \n",
    "    def load_vocab(self, path):\n",
    "        with open(path, 'rb') as f:\n",
    "            self.vocab = pickle.load(f)\n",
    "    \n",
    "    def build_vocab(self, tokens: List[str], unk_idx: int = 0, pad_idx: int = 1):\n",
    "#         self.special_tokens_to_idx = {'<unk>': unk_idx, '<pad>': pad_idx, '<sos>': unk_idx + 1, '<eos>': unk_idx + 2}\n",
    "#         tokens.extend(list(self.special_tokens_to_idx.keys()))\n",
    "#         self.vocab = Vocab(tokens, unk_idx)\n",
    "        tokens_ = [special_token for special_token in self.special_tokens_to_idx.keys()]\n",
    "        special_tokens_amount = len(self.special_tokens_to_idx)\n",
    "        \n",
    "        for token, _ in Counter(tokens).most_common(self.vocab_size - special_tokens_amount):\n",
    "            tokens_.append(token)\n",
    "        \n",
    "        unk_idx = self.special_tokens_to_idx.get('<unk>')\n",
    "        self.vocab = Vocab(tokens_, unk_idx)\n",
    "        \n",
    "    def transform_text(self, text: str) -> List[int]:\n",
    "        tokenized_text = self.tokenize(text)\n",
    "        transformed = [self.vocab.token_to_idx(token) for token in tokenized_text]\n",
    "        return transformed\n",
    "    \n",
    "    def fit(self, texts: List[str]) -> None:\n",
    "        transformed_texts = []\n",
    "        \n",
    "        tokenized_texts = [self.tokenize(text) for text in tqdm(texts, 'Tokenizing texts')]\n",
    "        tokens = chain(*tokenized_texts)\n",
    "        self.build_vocab(tokens)\n",
    "        \n",
    "#         for tokenized_text in tqdm(tokenized_texts, 'Transforming texts'):\n",
    "#             transformed = [self.vocab.token_to_idx(token) for token in tokenized_text]\n",
    "#             transformed_texts.append(transformed)\n",
    "    \n",
    "    def transform_texts(self, texts: List[str]) -> List[List[int]]:\n",
    "        transformed_texts = [transform_text(text) for text in tqdm(texts, 'Transforming texts')]\n",
    "        return transformed_texts\n",
    "    \n",
    "    def text_to_tensor(self, text: str, max_seq_len) -> torch.tensor:\n",
    "        transformed_text = self.transform_text(text)\n",
    "        pad_idx = self.special_tokens_to_idx.get('<pad>')\n",
    "        sos_idx = self.special_tokens_to_idx.get('<sos>')\n",
    "        eos_idx = self.special_tokens_to_idx.get('<eos>')\n",
    "        \n",
    "        pad_size = 0\n",
    "        if len(transformed_text) >= max_seq_len:\n",
    "            transformed_text = transformed_text[:max_seq_len]\n",
    "        else:\n",
    "            pad_size = max_seq_len - len(transformed_text)\n",
    "            transformed_text.extend([pad_idx] * pad_size)   \n",
    "        transformed_text.insert(0, sos_idx)\n",
    "        transformed_text.insert(len(transformed_text) - pad_size, eos_idx)\n",
    "        \n",
    "        tensor = torch.tensor(transformed_text, dtype=torch.long)\n",
    "        return tensor.unsqueeze(0)\n",
    "    \n",
    "    def texts_to_tensor(self, texts: List[str], max_seq_len) -> torch.tensor:\n",
    "        pad_idx = self.special_tokens_to_idx.get('<pad>')\n",
    "        sos_idx = self.special_tokens_to_idx.get('<sos>')\n",
    "        eos_idx = self.special_tokens_to_idx.get('<eos>')\n",
    "        transformed_texts = []\n",
    "        \n",
    "        for text in tqdm(texts, 'Building tensor'):\n",
    "            transformed_text = self.transform_text(text)\n",
    "            pad_size = 0\n",
    "            if len(transformed_text) >= max_seq_len:\n",
    "                transformed_text = transformed_text[:max_seq_len]\n",
    "            else:\n",
    "                pad_size = max_seq_len - len(transformed_text)\n",
    "                transformed_text.extend([pad_idx] * pad_size)   \n",
    "            transformed_text.insert(0, sos_idx)\n",
    "            transformed_text.insert(len(transformed_text) - pad_size, eos_idx)\n",
    "            transformed_texts.append(transformed_text)\n",
    "        \n",
    "        tensor = torch.tensor(transformed_texts, dtype=torch.long).permute(1, 0)\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разбиение данных на обучающую, тестовую и валидационную выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df, test_df = model_selection.train_test_split(df, test_size=0.1, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df, val_df = model_selection.train_test_split(test_df, test_size=0.5, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Токенизация текстов и индексация токенов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab_size = 125000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_seq_len = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_transformer = TextTransformer(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_transformer.load_vocab('./data/cached/vocab.vcb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_transformer.build_vocab(embedding.vocab.words[0:-2], embedding.vocab.unk_id, embedding.vocab.pad_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemm_vocab_size = 23000\n",
    "# orig_vocab_size = 65000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemm_text_transformer = TextTransformer(lemm_vocab_size)\n",
    "# orig_text_transformer = TextTransformer(orig_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_transformer.fit(train_df.orig_texts.to_list() + train_df.lemm_texts.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# with open('./data/cached/tokens.list', 'rb') as f:\n",
    "#     tokens = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# text_transformer.build_vocab(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens = [text_transformer.vocab.idx_to_token(idx) for idx in range(4, 124999)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./tokens.list', 'wb') as f:\n",
    "#     pickle.dump(tokens, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# orig_text_transformer.fit(train_df.orig_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Перевод данных в тензоры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensors = {\n",
    "#     'train_lemm_tensor': train_lemm_tensor,\n",
    "#     'test_lemm_tensor': test_lemm_tensor,\n",
    "#     'val_lemm_tensor': val_lemm_tensor,\n",
    "#     'train_orig_tensor': train_orig_tensor,\n",
    "#     'test_orig_tensor': test_orig_tensor,\n",
    "#     'val_orig_tensor': val_orig_tensor\n",
    "# }\n",
    "\n",
    "# with open('./data_tensors.data', 'wb') as f:\n",
    "#     pickle.dump(tensors, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('./data/cached/data_tensors.data', 'rb') as f:\n",
    "    tensors = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_lemm_tensor, test_lemm_tensor, val_lemm_tensor,\\\n",
    "train_orig_tensor, test_orig_tensor, val_orig_tensor = tensors.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_lemm_tensor = text_transformer.texts_to_tensor(train_df.lemm_texts.to_list(), max_seq_len)\n",
    "# test_lemm_tensor = text_transformer.texts_to_tensor(test_df.lemm_texts.to_list(), max_seq_len)\n",
    "# val_lemm_tensor = text_transformer.texts_to_tensor(val_df.lemm_texts.to_list(), max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_orig_tensor = text_transformer.texts_to_tensor(train_df.orig_texts.to_list(), max_seq_len)\n",
    "# test_orig_tensor = text_transformer.texts_to_tensor(test_df.orig_texts.to_list(), max_seq_len)\n",
    "# val_orig_tensor = text_transformer.texts_to_tensor(val_df.orig_texts.to_list(), max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gender_to_vec = {\n",
    "    'masc': [1, 0, 0, 0],\n",
    "    'fem': [0, 1, 0, 0],\n",
    "    'neut': [0, 0, 1, 0],\n",
    "    'undefined': [0, 0, 0, 1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tense_to_vec = {\n",
    "    'pres': [1, 0, 0],\n",
    "    'past': [0, 1, 0],\n",
    "    'fut': [0, 0, 1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transform_context(df, df_type: str):\n",
    "    transformed_gender = [gender_to_vec.get(gender) for gender in tqdm(df.gender, f'Transforming gender ({df_type})')]\n",
    "    transformed_tense = [tense_to_vec.get(tense) for tense in tqdm(df.tense, f'Transforming tense ({df_type})')]\n",
    "    transformed_nsubj = [text_transformer.vocab.token_to_idx(nsubj) for nsubj in tqdm(df.nsubj, f'Transforming nsubj ({df_type})')]\n",
    "    \n",
    "    context = [transformed_nsubj, transformed_gender, transformed_tense]\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def context_to_tensors(context):\n",
    "    nsubj, gender, tense = context\n",
    "    \n",
    "    nsubj_tensor = torch.tensor(nsubj)\n",
    "    gender_tensor = torch.tensor(gender, dtype=torch.float32)\n",
    "    tense_tensor = torch.tensor(tense, dtype=torch.float32)\n",
    "    \n",
    "    context_tensors = [nsubj_tensor, gender_tensor, tense_tensor]\n",
    "    return context_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1987d9bd361c4f94a2830e3c1638ff0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Transforming gender (train)', max=833725.0, style=Progres…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14c00c242bc9474cb70226fd06f40900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Transforming tense (train)', max=833725.0, style=Progress…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5760593db41046e6a6d7d689de6fb54c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Transforming nsubj (train)', max=833725.0, style=Progress…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "414225ea6cf94f8ab74b10c652067b55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Transforming gender (test)', max=46318.0, style=ProgressS…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8c9ab272dac427d98d3c48afec0872c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Transforming tense (test)', max=46318.0, style=ProgressSt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e104d1f35344e148c4b8e3b84a9347d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Transforming nsubj (test)', max=46318.0, style=ProgressSt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00a273b3341c4c8facd0d4bb30777e1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Transforming gender (validation)', max=46319.0, style=Pro…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31d6d23c261d468bbd3f0861e5c90e6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Transforming tense (validation)', max=46319.0, style=Prog…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59b369656bf0488899955395911af991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Transforming nsubj (validation)', max=46319.0, style=Prog…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_context = transform_context(train_df, 'train')\n",
    "test_context = transform_context(test_df, 'test')\n",
    "val_context = transform_context(val_df, 'validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_context_tensors = context_to_tensors(train_context)\n",
    "test_context_tensors = context_to_tensors(test_context)\n",
    "val_context_tensors = context_to_tensors(val_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cut_to_fit_batch(tensor: torch.Tensor, batch_size: int):\n",
    "    n_samples = tensor.shape[1]\n",
    "    new_n_samples = (n_samples // batch_size) * batch_size\n",
    "    result = tensor.split(new_n_samples, dim=1)[0]\n",
    "    return torch.transpose(result, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Построение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ContextMem(nn.Module):\n",
    "    def __init__(self, gender_input_size, tense_input_size, hidden_size, output_size, nsubj_embedding_size, device):\n",
    "        super(ContextMem, self).__init__()\n",
    "        \n",
    "        self.device = device\n",
    "\n",
    "        self.gender_proj = nn.Linear(gender_input_size, hidden_size, bias=False)\n",
    "        self.tense_proj = nn.Linear(tense_input_size, hidden_size, bias=False)\n",
    "        self.fc_out = nn.Linear(hidden_size * 2 + nsubj_embedding_size, output_size, bias=False)\n",
    "        \n",
    "    def forward(self, nsubj_embedding, gender, tense):\n",
    "        # nsubj_embedding_shape: (batch_size, embedding_size)\n",
    "        # gender_shape: (batch_size, input_size)\n",
    "        # tense_shape: (batch_size, input_size)\n",
    "        \n",
    "        gender = self.gender_proj(gender)\n",
    "        # gender_shape: (batch_size, hidden_size)\n",
    "        \n",
    "        tense = self.tense_proj(tense)\n",
    "        # tense_shape: (batch_size, hidden_size)    \n",
    "        \n",
    "        context = torch.cat([nsubj_embedding, gender, tense], dim=-1)\n",
    "        # context_shape: (batch_size, hidden_size * 2 + embedding_size)\n",
    "        \n",
    "        context = self.fc_out(context)\n",
    "        # context_shape: (batch_size, output_size)\n",
    "        \n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_size: int, hidden_size: int, pad_idx: int,\n",
    "                 device, dropout_p: float, embedding=None, pretrained_embedding_loaded=False):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = embedding\n",
    "        self.pretrained_embedding_loaded = pretrained_embedding_loaded\n",
    "        \n",
    "        self.rnn = nn.GRU(embedding_size, hidden_size, dropout=0.0, bidirectional=True)\n",
    "        \n",
    "    def forward(self, sequence, hidden):\n",
    "        # sequence_shape: (seq_len, batch_size)\n",
    "        # hidden_shape: (num_layers=1 * 2, batch_size, hidden_size)\n",
    "        # cell_shape: (num_layers=1 * 2, batch_size, hidden_size)\n",
    "        \n",
    "        if self.pretrained_embedding_loaded:\n",
    "            with torch.no_grad():\n",
    "                embedding = self.embedding(sequence)\n",
    "        else:\n",
    "            embedding = self.embedding(sequence)\n",
    "        # embedding_shape: (seq_len, batch_size, embedding_size)\n",
    "        encoder_states = self.rnn(embedding, hidden)[0]\n",
    "        # encoder_states: (seq_len, batch_size, hidden_size * 2)\n",
    "        \n",
    "        return encoder_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_size: int, hidden_size: int, output_size: int, pad_idx: int,\n",
    "                 device, dropout_p: float, embedding=None, pretrained_embedding_loaded=False):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = embedding\n",
    "        self.pretrained_embedding_loaded = pretrained_embedding_loaded\n",
    "        \n",
    "        self.attn_weights = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size, bias=False),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 1, bias=False),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        self.rnn = nn.GRU(embedding_size + 2 * hidden_size, hidden_size, dropout=0.0)\n",
    "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, token, encoder_states):\n",
    "        token.unsqueeze_(0)\n",
    "        # token_shape: (seq_len=1, batch_size)\n",
    "        \n",
    "        encoder_states = torch.transpose(encoder_states, 1, 0)\n",
    "        # encoder_states_shape: (batch_size, seq_len, hidden_size * 2)\n",
    "        \n",
    "        if self.pretrained_embedding_loaded:\n",
    "            with torch.no_grad():\n",
    "                embedding = self.embedding(token)\n",
    "        else:\n",
    "            embedding = self.embedding(token)\n",
    "        # embedding_shape: (seq_len=1, batch_size, embedding_size)\n",
    "        \n",
    "        seq_len = encoder_states.shape[1]\n",
    "        \n",
    "        attn_weights = self.attn_weights(encoder_states)\n",
    "        # attn_weights_shape: (batch_size, seq_len, 1)\n",
    "        \n",
    "        context_vec = torch.bmm(attn_weights.permute(0, 2, 1), encoder_states).permute(1, 0, 2)\n",
    "        # context_vec_shape: (1, batch_size, hidden_size * 2)\n",
    "        \n",
    "        combined = torch.cat((embedding, context_vec), dim=2)\n",
    "        # combined_shape: (1, batch_size, embedding_size + 2 * hidden_size)\n",
    "        \n",
    "        rnn_out = self.rnn(combined)[0]\n",
    "        # rnn_out_shape: (seq_len=1, batch_size, hidden_size)\n",
    "        \n",
    "        fc_out = self.fc_out(rnn_out)\n",
    "        # fc_out_shape: (seq_len=1, batch_size, output_size)\n",
    "        \n",
    "        return fc_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Seq2SeqModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_size, embedding_size, hidden_size, output_size,\n",
    "                 gender_input_size, tense_input_size, context_hidden_size, context_output_size,\n",
    "                 pad_idx, device, dropout_p, pretrained_embedding=None):\n",
    "        super(Seq2SeqModel, self).__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        if pretrained_embedding is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(pretrained_embedding, padding_idx=pad_idx)\n",
    "            self.pretrained_embedding_loaded = True\n",
    "        else:\n",
    "            self.embedding = nn.Sequential(\n",
    "                nn.Embedding(vocab_size, embedding_size, padding_idx=pad_idx),\n",
    "                nn.Dropout(dropout_p)\n",
    "            )\n",
    "            self.pretrained_embedding_loaded = False\n",
    "        \n",
    "        self.context_mem = ContextMem(gender_input_size, tense_input_size, context_hidden_size, context_output_size, embedding_size, device).to(device)\n",
    "        self.encoder = EncoderRNN(vocab_size, embedding_size, hidden_size,\n",
    "                                  pad_idx, device, dropout_p,\n",
    "                                  self.embedding, self.pretrained_embedding_loaded).to(device)\n",
    "        self.decoder = DecoderRNN(vocab_size, embedding_size, hidden_size, output_size,\n",
    "                                  pad_idx, device, dropout_p,\n",
    "                                  self.embedding, self.pretrained_embedding_loaded).to(device)\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "    def forward(self, input, target, context, teacher_forcing_ratio=0.0):\n",
    "        batch_size = input.shape[1]\n",
    "        target_len = target.shape[0]\n",
    "        target_vocab_size = self.vocab_size\n",
    "        \n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size, device=self.device)\n",
    "        \n",
    "        nsubj, gender, tense = context\n",
    "        # nsubj_shape:  (batch_size)\n",
    "        # gender_shape: (batch_size, gender_input_size)\n",
    "        # tense_shape:  (batch_size, tense_input_size)\n",
    "        \n",
    "        if self.pretrained_embedding_loaded:\n",
    "            with torch.no_grad():\n",
    "                nsubj_embedding = self.embedding(nsubj).squeeze(0)\n",
    "        else:\n",
    "            nsubj_embedding = self.embedding(nsubj).squeeze(0)\n",
    "            # nsubj_embedding_shape: (batch_size, embedding_size)\n",
    "        \n",
    "        hidden = self.context_mem(nsubj_embedding, gender, tense)\n",
    "        # hidden_shape: (batch_size, context_output_size=hidden_size)\n",
    "        \n",
    "        hidden = torch.cat([hidden.unsqueeze(0)] * 2, 0)\n",
    "        # hidden_shape: (2, batch_size, context_output_size=hidden_size)\n",
    "        \n",
    "        encoder_states = self.encoder(input, hidden)\n",
    "        # encoder_states_shape: (seq_len, batch_size, hidden_size * 2)\n",
    "        \n",
    "        prev_token_idx = target[0] # sos_token\n",
    "        # prev_token_shape: (batch_size)\n",
    "        \n",
    "        for t in range(1, target_len):\n",
    "            output = self.decoder(prev_token_idx, encoder_states)\n",
    "            # output_shape: (1, batch_size, output_size)\n",
    "            \n",
    "            outputs[t] = output.squeeze(0)\n",
    "            \n",
    "            best_prediction = outputs[t].argmax(dim=1)\n",
    "            # best_prediction_shape: (batch_size)\n",
    "            \n",
    "            prev_token_idx = target[t] if random() < teacher_forcing_ratio else best_prediction\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция сохранения текущего состояния модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, epoch, path):\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'criterion': criterion,\n",
    "        'epoch': epoch\n",
    "    }\n",
    "    \n",
    "    torch.save(checkpoint, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция загрузки уже тренировавшейся модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_model(model, optimizer, criterion, path, for_inference=True, device=torch.device('cpu')):\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    if not for_inference:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        epoch = checkpoint['epoch']\n",
    "        criterion = checkpoint['criterion']\n",
    "\n",
    "        return epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Инициализация гиперпараметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "batch_size = 4\n",
    "epochs_amount = 50\n",
    "hidden_size = 768\n",
    "embedding_size = 300\n",
    "max_norm = 1.0\n",
    "dropout_p = 0.5\n",
    "gender_input_size = 4\n",
    "tense_input_size = 3\n",
    "context_hidden_size = hidden_size // 2\n",
    "context_output_size = hidden_size\n",
    "patience = 3\n",
    "output_size = vocab_size\n",
    "pad_idx = text_transformer.special_tokens_to_idx.get('<pad>')\n",
    "model_path = './models/'\n",
    "model_name = 'seq2seq_attention_fixed.model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Seq2SeqModel(vocab_size, embedding_size, hidden_size, output_size,\n",
    "                     gender_input_size, tense_input_size, context_hidden_size, context_output_size, \n",
    "                     pad_idx, device, dropout_p).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No models found at ./models/\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    epoch = load_model(model, optimizer, criterion, model_path + model_name, for_inference=True)\n",
    "    print(f'Loaded model from {model_path}')\n",
    "except:\n",
    "    print(f'No models found at {model_path}')\n",
    "    epoch = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Урезание данных для соответствия размеру батча"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_lemm_tensor_f = cut_to_fit_batch(train_lemm_tensor, batch_size)\n",
    "train_orig_tensor_f = cut_to_fit_batch(train_orig_tensor, batch_size)\n",
    "\n",
    "test_lemm_tensor_f = cut_to_fit_batch(test_lemm_tensor, batch_size)\n",
    "test_orig_tensor_f = cut_to_fit_batch(test_orig_tensor, batch_size)\n",
    "\n",
    "val_lemm_tensor_f = cut_to_fit_batch(val_lemm_tensor, batch_size)\n",
    "val_orig_tensor_f = cut_to_fit_batch(val_orig_tensor, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_context_tensors_f = [cut_to_fit_batch(tensor.unsqueeze(0), batch_size).squeeze(1) for tensor in train_context_tensors]\n",
    "test_context_tensors_f = [cut_to_fit_batch(tensor.unsqueeze(0), batch_size).squeeze(1) for tensor in test_context_tensors]\n",
    "val_context_tensors_f = [cut_to_fit_batch(tensor.unsqueeze(0), batch_size).squeeze(1) for tensor in val_context_tensors]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Инициализация данных итерируемых по батчам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(train_lemm_tensor_f, train_orig_tensor_f, *train_context_tensors_f)\n",
    "test_dataset = TensorDataset(test_lemm_tensor_f, test_orig_tensor_f, *test_context_tensors_f)\n",
    "val_dataset = TensorDataset(val_lemm_tensor_f, val_orig_tensor_f, *val_context_tensors_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Определение функции проверки работы сети между эпохами обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_evaluate(model, input, context, target_len=40):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        \n",
    "        input = input.to(device)\n",
    "\n",
    "        nsubj, gender, tense = context\n",
    "        nsubj_embedding = model.embedding(nsubj)\n",
    "\n",
    "        hidden = model.context_mem(nsubj_embedding, gender, tense)\n",
    "        hidden = torch.cat([hidden.unsqueeze(0)] * 2, 0)\n",
    "\n",
    "        sos_idx = text_transformer.special_tokens_to_idx.get('<sos>')\n",
    "        eos_idx = text_transformer.special_tokens_to_idx.get('<eos>')\n",
    "    \n",
    "        encoder_states = model.encoder(input, hidden)\n",
    "        \n",
    "        predicted_indexes = [sos_idx]\n",
    "        \n",
    "        for _ in range(1, target_len):\n",
    "            prev_idx = torch.tensor([predicted_indexes[-1]], dtype=torch.long, device=device)\n",
    "            \n",
    "            output = model.decoder(prev_idx, encoder_states)\n",
    "            output = output.squeeze(0)\n",
    "            \n",
    "            best_prediction = output.argmax(dim=1).item()\n",
    "            \n",
    "            if best_prediction == eos_idx:\n",
    "                break\n",
    "                \n",
    "            predicted_indexes.append(best_prediction)\n",
    "                        \n",
    "        \n",
    "    predicted_tokens = [text_transformer.vocab.idx_to_token(idx) for idx in predicted_indexes]\n",
    "    return predicted_tokens[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Определение функции обучения сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, train_data, val_data, test_data, epochs_amount, max_norm, patience=3, current_epoch=1, n_prints=5):\n",
    "    min_mean_val_loss = float('+inf')\n",
    "    initial_patiece = patience\n",
    "    print_every = len(train_data) // n_prints\n",
    "    \n",
    "    for epoch in tqdm(range(current_epoch, epochs_amount + 1), 'Epochs'):\n",
    "        print(f'\\nEpoch [{epoch} / {epochs_amount}]')\n",
    "        \n",
    "        model.train()\n",
    "        for iteration, (input, target, nsubj, gender, tense) in enumerate(tqdm(train_data, 'Epoch training iterations')):\n",
    "            optimizer.zero_grad()\n",
    "            # input = lemm_texts, target = orig_texts\n",
    "            \n",
    "            input = torch.transpose(input, 1, 0).to(device)\n",
    "            # input_shape: (seq_len, batch_size)\n",
    "            \n",
    "            target = torch.transpose(target, 1, 0).to(device)\n",
    "            # target_shape: (seq_len, batch_size)\n",
    "            \n",
    "            context = (nsubj.to(device), gender.to(device), tense.to(device))\n",
    "            \n",
    "            output = model(input, target, context)\n",
    "            # output_shape: (seq_len, batch_size, vocab_size) but need (N, vocab_size)\n",
    "            \n",
    "            target = target[1:].reshape(-1)\n",
    "            # now target_shape is (seq_len * batch_size)\n",
    "            \n",
    "            vocab_size = output.shape[2]\n",
    "            \n",
    "            output = output[1:].reshape(-1, vocab_size)\n",
    "            # now output_shape is (seq_len * batch_size, vocab_size)\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            if iteration % print_every == 0:\n",
    "                print(f'\\tIteration #{iteration}: training loss = {loss.item()}')\n",
    "            elif iteration == len(train_data):\n",
    "                print(f'\\tIteration #{iteration}: training loss = {loss.item()}')\n",
    "            \n",
    "            \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            val_loss = []\n",
    "            \n",
    "            for input, target, nsubj, gender, tense in tqdm(val_data, 'Epoch validating iterations'):\n",
    "                input = torch.transpose(input, 1, 0).to(device)\n",
    "                target = torch.transpose(target, 1, 0).to(device)\n",
    "                context = (nsubj.to(device), gender.to(device), tense.to(device))\n",
    "                \n",
    "                output = model(input, target, context)\n",
    "                vocab_size = output.shape[2]\n",
    "                output = output[1:].reshape(-1, orig_vocab_size)\n",
    "                target = target[1:].reshape(-1)\n",
    "                \n",
    "                val_loss.append(criterion(output, target).item())\n",
    "            \n",
    "            mean_val_loss = sum(val_loss) / len(val_loss)\n",
    "            print(f'\\tValidation loss = {mean_val_loss}')\n",
    "            if mean_val_loss < min_mean_val_loss:\n",
    "                try:\n",
    "                    save_model(model, optimizer, epoch, model_path + model_name)\n",
    "                    min_mean_val_loss = mean_val_loss\n",
    "                    patience = initial_patiece\n",
    "                except Exception as exc:\n",
    "                    print(exc)\n",
    "            else:\n",
    "                patience -= 1\n",
    "            \n",
    "            test_data = DataLoader(test_data.dataset, batch_size=1, shuffle=True)\n",
    "            for input, target, nsubj, gender, tense in test_data:\n",
    "                target = target.squeeze(0).to(device)\n",
    "                context = (nsubj.to(device), gender.to(device), tense.to(device))\n",
    "                \n",
    "                input = torch.transpose(input, 1, 0)\n",
    "                target_len = target.shape[0]\n",
    "                \n",
    "                output = test_evaluate(model, input, context, target_len)\n",
    "                decoded_input = [text_transformer.vocab.idx_to_token(idx.item()) for idx in input]\n",
    "                decoded_target = [text_transformer.vocab.idx_to_token(idx.item()) for idx in target]\n",
    "                \n",
    "                print(f'\\tInput: {decoded_input}')\n",
    "                print(f'\\tOutput: {output}')\n",
    "                print(f'\\tTarget: {decoded_target}')\n",
    "                break\n",
    "        \n",
    "        if patience == 0:\n",
    "            print(f'\\nModel learning finished due to early stopping')\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Определение функции эксплуатации обученной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(model: Seq2SeqModel, sentence: str, context, max_seq_len=45):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        \n",
    "        nsubj, gender, tense = context\n",
    "        \n",
    "        nsubj = torch.tensor([text_transformer.vocab.token_to_idx(nsubj)], device=device).unsqueeze(0)\n",
    "        gender = torch.tensor([gender_to_vec[gender]], dtype=torch.float32, device=device)\n",
    "        tense = torch.tensor([tense_to_vec[tense]], dtype=torch.float32, device=device)\n",
    "        \n",
    "        nsubj_embedding = model.embedding(nsubj).squeeze(0)\n",
    "\n",
    "        hidden = model.context_mem(nsubj_embedding, gender, tense)\n",
    "        hidden = torch.cat([hidden.unsqueeze(0)] * 2, 0)\n",
    "        # hidden_shape: (2, batch_size, context_output_size=hidden_size)\n",
    "        \n",
    "        input_tensor = text_transformer.text_to_tensor(sentence, max_seq_len).to(device)\n",
    "        input_tensor = torch.transpose(input_tensor, 1, 0)\n",
    "        \n",
    "        sos_idx = text_transformer.special_tokens_to_idx.get('<sos>')\n",
    "        eos_idx = text_transformer.special_tokens_to_idx.get('<eos>')\n",
    "    \n",
    "    \n",
    "        encoder_states = model.encoder(input_tensor, hidden)\n",
    "        \n",
    "        predicted_indexes = [sos_idx]                       \n",
    "        \n",
    "        for _ in range(1, max_seq_len):\n",
    "            prev_idx = torch.tensor([predicted_indexes[-1]], dtype=torch.long, device=device)\n",
    "            \n",
    "            output = model.decoder(prev_idx, encoder_states)\n",
    "            output = output.squeeze(0)\n",
    "            \n",
    "            best_prediction = output.argmax(dim=1).item()\n",
    "            \n",
    "            if best_prediction == eos_idx:\n",
    "                break\n",
    "                \n",
    "            predicted_indexes.append(best_prediction)\n",
    "        \n",
    "    predicted_tokens = [text_transformer.vocab.idx_to_token(idx) for idx in predicted_indexes]\n",
    "    return predicted_tokens[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2608bd2c6ff347968cad6ba5754e5784",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epochs', max=50.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [1 / 50]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f2dc4aad76844eb9a552fcc4faa4146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch training iterations', max=208431.0, style=ProgressS…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tIteration #0: training loss = 11.751469612121582\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-e37acd47fd24>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs_amount\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-52-bc2bf5b8a58f>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, criterion, train_data, val_data, test_data, epochs_amount, max_norm, patience, current_epoch, n_prints)\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m             \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\iluxa\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\utils\\clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[1;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mclip_coef\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_norm\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtotal_norm\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1e-6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mclip_coef\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclip_coef\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, optimizer, criterion, train_loader, val_loader, test_loader, epochs_amount, max_norm, patience, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# del model\n",
    "# del optimizer\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = test_df.sample(10)\n",
    "test_lemm_sents = test_sample.lemm_texts.to_list()\n",
    "test_target_sents = test_sample.orig_texts.to_list()\n",
    "test_nsubj = test_sample.nsubj.to_list()\n",
    "test_gender = test_sample.gender.to_list()\n",
    "test_tense = test_sample.tense.to_list()\n",
    "test_input = zip(test_lemm_sents, test_target_sents, test_nsubj, test_gender, test_tense) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for lemm_sent, target_sent, nsubj, gender, tense in test_input:\n",
    "    model_output = evaluate(model, lemm_sent, (nsubj, gender, tense))\n",
    "    print(f'Input: {lemm_sent}')\n",
    "    print(f'Output: {model_output}')\n",
    "    print(f'Target: {target_sent}')\n",
    "    print(f'Nsubj: {nsubj}')\n",
    "    print(f'Gender: {gender}')\n",
    "    print(f'Tense: {tense}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_samples = 15000\n",
    "\n",
    "test_lemm_sent = test_df.lemm_texts.to_list()[:max_samples]\n",
    "test_orig_sent = test_df.orig_texts.to_list()[:max_samples]\n",
    "test_nsubj = test_df.nsubj.to_list()[:max_samples]\n",
    "test_gender = test_df.gender.to_list()[:max_samples]\n",
    "test_tense = test_df.tense.to_list()[:max_samples]\n",
    "test_input = zip(test_lemm_sent, test_nsubj, test_gender, test_tense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = [evaluate(model, lemm_sent, (nsubj, gender, tense)) for lemm_sent, nsubj, gender, tense in tqdm(test_input)]\n",
    "targets = [[text_transformer.tokenize(target)] for target in tqdm(test_orig_sent)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = round(bleu_score(outputs, targets, max_n=1, weights=[1]), 3)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
