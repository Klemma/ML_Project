{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2fb41fd-c50f-4033-9cbf-273df7637194",
   "metadata": {},
   "source": [
    "## Импорт необходимых зависимостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "973dc9dc-9e2e-4d2d-8c9d-98fa61b38d55",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "\n",
    "import pickle\n",
    "import pathlib\n",
    "\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset, IterableDataset\n",
    "\n",
    "from sklearn import model_selection\n",
    "\n",
    "from pprint import pprint\n",
    "from random import choice\n",
    "from typing import List, Union\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformers import BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a8e1272-bf97-4890-945c-6615ba132abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt', quiet=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6e4800-d29f-4ac2-88a3-e5ee010d9901",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91d515aa-7400-443e-9e9a-5c3c34d3ef65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RANDOM_STATE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c2c1321-83cd-4e5d-afaa-9ae0992153b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/lenta/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e460b5b5-8655-47e6-9ea8-4e9e2c2c2ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.gender.replace('undefined', 'undefined_g', inplace=True)\n",
    "df.number.replace('undefined', 'undefined_n', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c448591d-3ea9-44d1-bfc0-26048460a7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.sample(n=5000, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f03839d-c4c4-4e58-b1c6-cc5d8eb69f14",
   "metadata": {},
   "source": [
    "### Разбиение данных на обучающие, тестовые и валидационные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6445806b-5852-45de-8faf-f68a4ff2132f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = model_selection.train_test_split(df, train_size=0.9)\n",
    "test_df, val_df = model_selection.train_test_split(test_df, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e176ed0-02ae-46eb-9cf5-739472febefe",
   "metadata": {},
   "source": [
    "### Подготовка словаря"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "649b0f77-c913-4cfb-abe2-f25bb50cff3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a46956a-b483-40fb-a373-38f3b2a4586e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('DeepPavlov/rubert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27eb72d2-27b9-46b1-aef1-21e72d5bb838",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = {\n",
    "    'bos_token': '[BOS]',\n",
    "    'eos_token': '[EOS]',\n",
    "    'masc': '[MASC_G]',\n",
    "    'fem': '[FEM_G]',\n",
    "    'neut': '[NEUT_G]',\n",
    "    'undefined_g': '[UNDEF_G]',\n",
    "    'past': '[PAST_T]',\n",
    "    'pres': '[PRES_T]',\n",
    "    'fut': '[FUT_T]',\n",
    "    'sing': '[SING_N]',\n",
    "    'plur': '[PLUR_N]',\n",
    "    'undefined_n': '[UNDEF_N]'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e610137d-a216-4873-8ef1-567674f00a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({'bos_token': special_tokens['bos_token'],\n",
    "                              'eos_token': special_tokens['eos_token'],\n",
    "                              'additional_special_tokens': list(special_tokens.values())[2:]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cce9a7-d49e-495e-8448-08b3898dc5d9",
   "metadata": {},
   "source": [
    "### Разбиение данных на батчи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1bc5c99-5ba5-4794-8c75-22cbb05b9ef3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_batched_dataset(df, tokenizer=tokenizer, batch_size=batch_size):\n",
    "    n_batches = len(df) // batch_size\n",
    "    \n",
    "    for n_batch in range(n_batches):\n",
    "        \n",
    "        orig_texts   = df.orig_texts.to_list()[batch_size * n_batch:batch_size * (n_batch + 1)]\n",
    "        lemm_texts   = df.part_lemm_texts.to_list()[batch_size * n_batch:batch_size * (n_batch + 1)]\n",
    "        nsubj_list   = df.nsubj.to_list()[batch_size * n_batch:batch_size * (n_batch + 1)]\n",
    "        gender_list  = df.gender.to_list()[batch_size * n_batch:batch_size * (n_batch + 1)]\n",
    "        tense_list   = df.tense.to_list()[batch_size * n_batch:batch_size * (n_batch + 1)]\n",
    "        number_list  = df.number.to_list()[batch_size * n_batch:batch_size * (n_batch + 1)]\n",
    "        \n",
    "        bos_token = tokenizer.bos_token\n",
    "        eos_token = tokenizer.eos_token\n",
    "        \n",
    "        inputs = zip(lemm_texts, nsubj_list, gender_list, tense_list, number_list)\n",
    "        \n",
    "        inputs = [f'{nsubj} {special_tokens[gender]} {special_tokens[tense]} {special_tokens[number]} {bos_token} {lemm} {eos_token}'\n",
    "                  for lemm, nsubj, gender, tense, number in inputs]\n",
    "        \n",
    "        inputs = tokenizer(inputs, add_special_tokens=False, padding='longest',\n",
    "                           return_tensors='pt', return_attention_mask=False, return_token_type_ids=False).input_ids\n",
    "        \n",
    "        targets = [f'{bos_token} {orig} {eos_token}' for orig in orig_texts]\n",
    "        \n",
    "        targets = tokenizer(targets, add_special_tokens=False, padding='longest',\n",
    "                            return_tensors='pt', return_attention_mask=False, return_token_type_ids=False).input_ids\n",
    "        \n",
    "        yield inputs.permute(1, 0), targets.permute(1, 0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "deef4354-7d0f-4544-9e0b-954ae786c48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_n_batches = len(train_df) // batch_size\n",
    "val_n_batches = len(val_df) // batch_size\n",
    "test_n_batches = len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48dd5b26-8f9a-4c57-9dd2-85a69d02a7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_processed_data(train_data, val_data, test_data):\n",
    "    path = {\n",
    "        'dir': './data/cached',\n",
    "        'name': 'processed_data.pkl'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        pathlib.Path(path['dir']).mkdir(exist_ok=True)\n",
    "        file_path = path['dir'] + '/' + path['name']\n",
    "\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump((train_data, val_data, test_data), f)\n",
    "\n",
    "        print(f'Data is saved successfully at {file_path}')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Failed to save data due to:\\n{e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba42694e-6b07-4406-8e2c-d04a9d8a9c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_processed_data(path='./data/cached/processed_data.pkl'):\n",
    "    try:\n",
    "        with open(path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "\n",
    "        print(f'Data is loaded successfully from {path}')\n",
    "\n",
    "        return data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Failed to load data due to:\\n{e}')\n",
    "\n",
    "        return [None] * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2c404e2-2788-403f-bef6-c1c81ee8baa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe7ce4a0be7c45eebb521a88c6f3596d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unpacking test batches:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data = make_batched_dataset(train_df)\n",
    "val_data = make_batched_dataset(val_df)\n",
    "test_data = []\n",
    "for i, batch in enumerate(tqdm(make_batched_dataset(test_df, batch_size=1), desc='Unpacking test batches', total=1000)):\n",
    "    test_data.append(batch)\n",
    "    if i == 1000:\n",
    "        break\n",
    "# test_data = [batch for batch in tqdm(make_batched_dataset(test_df, batch_size=1), desc='Unpacking test batches', total=test_n_batches)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b94794e-adbc-4e8f-ab39-ba1a2cd47cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_data = False\n",
    "# save_data = False\n",
    "\n",
    "# if load_data:\n",
    "#     train_data, val_data, test_data = load_processed_data()\n",
    "\n",
    "# if not load_data or train_data is None:\n",
    "#     train_data = [batch for batch in tqdm(make_batched_dataset(train_df), desc='Unpacking train batches', total=train_n_batches)]\n",
    "#     val_data = [batch for batch in tqdm(make_batched_dataset(val_df), desc='Unpacking validation batches', total=val_n_batches)]\n",
    "#     test_data = [batch for batch in tqdm(make_batched_dataset(test_df, batch_size=1), desc='Unpacking test batches', total=test_n_batches)]\n",
    "\n",
    "# if save_data:\n",
    "#     save_processed_data(train_data, val_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aec61dc1-4e24-4dc7-acab-0fb3c785e198",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchedDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "821cf2e2-3e19-4604-a525-d0f23687087b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = DataLoader(BatchedDataset(train_data), batch_size=None, shuffle=True, num_workers=0)\n",
    "# val_data = DataLoader(BatchedDataset(val_data), batch_size=None, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b0b913-ea3a-44c0-b457-6f2842fd6166",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a064933c-35cb-4339-93b9-453058b2017b",
   "metadata": {},
   "source": [
    "## Определение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def4e99a-5a2c-413d-9fe4-109fae20d624",
   "metadata": {},
   "source": [
    "### Определение класса модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd302268-7e66-435c-93e3-cebdafc7f9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, embedding_size, nhead,\n",
    "                 num_encoder_layers, num_decoder_layers, \n",
    "                 dim_feedforward, dropout, vocab_size,\n",
    "                 max_seq_len, pad_token_id, device):\n",
    "        \n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        \n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        self.to(device)\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        self.word_embedding = nn.Embedding(vocab_size, embedding_size, pad_token_id)\n",
    "        self.input_pos_encoding = nn.Embedding(max_seq_len, embedding_size)\n",
    "        self.target_pos_encoding = nn.Embedding(max_seq_len, embedding_size)\n",
    "        \n",
    "        self.transformer = nn.Transformer(embedding_size, nhead, num_encoder_layers,\n",
    "                                          num_decoder_layers, dim_feedforward, dropout)\n",
    "        \n",
    "        self.fc_out = nn.Linear(embedding_size, vocab_size)\n",
    "        \n",
    "    def get_padding_mask(self, input):\n",
    "        # input shape: (seq_len, batch_size)\n",
    "        padding_mask = input.permute(1, 0) == self.pad_token_id\n",
    "        return padding_mask.to(self.device)\n",
    "    \n",
    "    def forward(self, input, target):\n",
    "        # input shape: (input_seq_len, batch_size)\n",
    "        # target shape: (target_seq_len, batch_size)\n",
    "    \n",
    "        embedded_input = self.word_embedding(input)\n",
    "        embedded_target = self.word_embedding(target)\n",
    "        # embedded_input shape: (input_seq_len, batch_size, embedding_size)\n",
    "        # embedded_target shape: (target_seq_len, batch_size, embedding_size)\n",
    "        \n",
    "        batch_size = input.shape[1]\n",
    "        \n",
    "        input_seq_len = input.shape[0]\n",
    "        target_seq_len = target.shape[0]\n",
    "    \n",
    "        input_positions = torch.arange(0, input_seq_len).unsqueeze(1).expand(input_seq_len, batch_size).to(self.device)\n",
    "        target_positions = torch.arange(0, target_seq_len).unsqueeze(1).expand(target_seq_len, batch_size).to(self.device)\n",
    "        # input_positions shape: (input_seq_len, batch_size)\n",
    "        # target_positions shape: (target_seq_len, batch_size)\n",
    "        \n",
    "        input_positions = self.input_pos_encoding(input_positions)\n",
    "        target_positions = self.target_pos_encoding(target_positions)\n",
    "        # input_positions shape: (input_seq_len, batch_size, embedding_size)\n",
    "        # target_positions shape: (target_seq_len, batch_size, embedding_size)\n",
    "        \n",
    "        embedded_input += input_positions\n",
    "        embedded_target += target_positions\n",
    "        \n",
    "        input_padding_mask = self.get_padding_mask(input)\n",
    "        target_padding_mask = self.get_padding_mask(target)\n",
    "        # input_padding_mask shape: (batch_size, input_seq_len)\n",
    "        \n",
    "        target_mask = self.transformer.generate_square_subsequent_mask(target_seq_len).to(self.device)\n",
    "        # target_mask shape: (target_seq_len, target_seq_len)\n",
    "        \n",
    "        output = self.transformer(embedded_input, embedded_target,\n",
    "                                  tgt_mask=target_mask,\n",
    "                                  src_key_padding_mask=input_padding_mask,\n",
    "                                  tgt_key_padding_mask=target_padding_mask)\n",
    "        # output shape: (target_seq_len, batch_size, embedding_size)\n",
    "        \n",
    "        output = self.fc_out(output)\n",
    "        # output shape: (target_seq_len, batch_size, vocab_size)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f444e6-837f-4bda-ba0f-0ca350ac9d6a",
   "metadata": {},
   "source": [
    "## Определение функций-утилит"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1237f0-267b-4b76-b02c-eb43c09c5fda",
   "metadata": {},
   "source": [
    "### Сохранение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42ef352d-2d48-4253-ae38-6bd6e66fa44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, epoch, val_loss, train_loss, path='./models/seq2seq_transformer.model'):\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'val_loss': val_loss,\n",
    "        'train_loss': train_loss\n",
    "    }\n",
    "    \n",
    "    torch.save(checkpoint, path)\n",
    "    print(f'\\n\\tModel saved successfully at {path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267bb350-d049-49ef-9e6c-aaeda118da03",
   "metadata": {},
   "source": [
    "### Загрузка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b36d6ed-f3c1-459c-b2d7-7d123d09be66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, optimizer=None, path='./model/seq2seq_transformer.model', device=torch.device('cpu')):\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    if optimizer is not None:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        \n",
    "    epoch      = checkpoint['epoch']\n",
    "    val_loss   = checkpoint['val_loss']\n",
    "    train_loss = checkpoint['train_loss']\n",
    "\n",
    "    return {'epoch': epoch, 'val_loss': val_loss, 'train_loss': train_loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee772c3-9c5d-418e-855d-4664a8648e29",
   "metadata": {},
   "source": [
    "## Место хранения модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3863bdb5-fa7f-4902-95a0-ecf2d434403a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = {\n",
    "    'dir': './models/',\n",
    "    'name': 'seq2seq_transformer.model'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d62843a-1792-4baf-a43d-c8fb375d3939",
   "metadata": {},
   "source": [
    "## Обучение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db7ee35-94fe-403c-af6c-a23114bb8335",
   "metadata": {},
   "source": [
    "### Определение параметров обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df74d24e-eff1-4f04-96b1-8c42c69d0381",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_params = {\n",
    "    'learning_rate': 5e-05,\n",
    "    'epochs': 10,\n",
    "    'max_norm': 1.0,\n",
    "    'patience': 3\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11598852-e1f4-44ef-ab75-62eca9d6f88b",
   "metadata": {},
   "source": [
    "### Определение параметров сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4be2183-3f5f-4c04-b828-373e545eb03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'embedding_size': 512,\n",
    "    'nhead': 8,\n",
    "    'num_encoder_layers': 6,\n",
    "    'num_decoder_layers': 6,\n",
    "    'dim_feedforward': 2048,\n",
    "    'dropout': 0.1,\n",
    "    'vocab_size': len(tokenizer.get_vocab()),\n",
    "    'max_seq_len': 100,\n",
    "    'pad_token_id': tokenizer.pad_token_id,\n",
    "    'device': torch.device('cuda')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841711eb-3ac5-499d-997a-1053ac80b226",
   "metadata": {},
   "source": [
    "### Инициализация модели, оптимизатора и функции потерь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b96307a-a84a-43f3-9013-d48396eb493a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2SeqTransformer(**params).to(params['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e23044da-4d9d-46ba-9870-9c298cbd2beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_params['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5b4de224-4a7e-41f9-9989-46331d41de01",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=params['pad_token_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b78a97c9-c080-4c72-ab09-da1bc969b3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_pretrained_model = False\n",
    "train_state = None\n",
    "\n",
    "if load_pretrained_model:\n",
    "    try:\n",
    "        train_state = load_model(\n",
    "            model, optimizer,\n",
    "            model_path['dir'] + model_path['name'],\n",
    "            params['device']\n",
    "        )\n",
    "        print(f\"Model loaded successfully from {model_path.get('dir') + model_path.get('name')}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'Load failed due to:\\n{e}')\n",
    "\n",
    "epoch = train_state['epoch'] if train_state is not None else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2d24b033-d3d2-422e-9bb7-13af77a36aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loss_writer = SummaryWriter('./runs/loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc781b75-6033-4132-87ee-0dce6bbfcc58",
   "metadata": {},
   "source": [
    "### Train-скрипт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8a49626b-708f-4afd-906f-7b952771afcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model, optimizer, criterion,\n",
    "    train_data, val_data, test_data,\n",
    "    epochs, max_norm, patience, current_epoch,\n",
    "    device, tokenizer, model_path, max_seq_len, n_prints=10\n",
    "):\n",
    "    \n",
    "    min_mean_val_loss = float('+inf')\n",
    "    initial_patience = patience\n",
    "    print_every = train_n_batches // n_prints\n",
    "    \n",
    "    train_samples = []\n",
    "    val_samples = []\n",
    "    data_extracted = False\n",
    "    \n",
    "    for epoch in tqdm(range(current_epoch, epochs), 'Epochs'):\n",
    "        running_train_loss = 0.0\n",
    "        print(f'\\nEpoch [{epoch} / {epochs}]')\n",
    "        \n",
    "        model.train()\n",
    "        tqdm_iter_batch = tqdm(train_data, desc='Training iterations', total=train_n_batches)\n",
    "        for iteration, (input, target) in enumerate(tqdm_iter_batch):\n",
    "            train_samples.append((input, target))\n",
    "            \n",
    "            input  = input.to(device)\n",
    "            target = target.to(device)\n",
    "            # input shape : (input_seq_len, batch_size)\n",
    "            # target shape: (target_seq_len, batch_size)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(input, target[:-1])\n",
    "            # output shape: (target_seq_len, batch_size, vocab_size)\n",
    "            \n",
    "            vocab_size = output.shape[2]\n",
    "            \n",
    "            output = output.reshape(-1, vocab_size)\n",
    "            # output shape: (target_seq_len * batch_size, vocab_size)\n",
    "            \n",
    "            target = target[1:].reshape(-1)\n",
    "            # target shape: (target_seq_len * batch_size)\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            running_train_loss += loss.item()\n",
    "            \n",
    "            tqdm_iter_batch.set_postfix({'train_loss': loss.item()})\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "#             global_step = epoch * (len(train_data) + 1) + iteration\n",
    "#             train_loss_writer.add_scalar('Training loss', loss, global_step=global_step)\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            if iteration % print_every == 0:\n",
    "                mean_train_loss = running_train_loss / print_every if iteration != 0 else running_train_loss\n",
    "                running_train_loss = 0\n",
    "                print(f'\\tIteration #{iteration}: training loss = {mean_train_loss}')\n",
    "            \n",
    "            \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            \n",
    "            val_loss = []\n",
    "            \n",
    "            for iteration, (input, target) in enumerate(tqdm(val_data, desc='Validating iterations', total=val_n_batches)):\n",
    "                val_samples.append((input, target))\n",
    "                \n",
    "                input  = input.to(device)\n",
    "                target = target.to(device)\n",
    "                \n",
    "                output = model(input, target[:-1])\n",
    "                vocab_size = output.shape[2]\n",
    "                output = output.reshape(-1, vocab_size)\n",
    "                \n",
    "                target = target[1:].reshape(-1)\n",
    "                \n",
    "                local_val_loss = criterion(output, target)\n",
    "                val_loss.append(local_val_loss.item())\n",
    "                \n",
    "            mean_val_loss = sum(val_loss) / len(val_loss)\n",
    "            print(f'\\tValidation loss = {mean_val_loss}')\n",
    "            \n",
    "            if mean_val_loss < min_mean_val_loss:\n",
    "                try:\n",
    "                    save_model(model, optimizer, epoch, mean_val_loss, loss)\n",
    "                    min_mean_val_loss = mean_val_loss\n",
    "                    patience = initial_patience\n",
    "                except Exception as e:\n",
    "                    print(f'Model saving failed due to unhandled exception:\\n{e}')\n",
    "            else:\n",
    "                patience -= 1\n",
    "                \n",
    "                \n",
    "            data_extracted = True\n",
    "            if data_extracted:\n",
    "                train_data = DataLoader(BatchedDataset(train_samples), batch_size=None, shuffle=True)\n",
    "                val_data = DataLoader(BatchedDataset(val_samples), batch_size=None, shuffle=False)\n",
    "                \n",
    "            \n",
    "            test_sample = choice(test_data)\n",
    "            \n",
    "            predictions = [tokenizer.bos_token_id]\n",
    "            for i in range(max_seq_len):\n",
    "                target = torch.tensor(predictions, device=device).unsqueeze(1)\n",
    "                \n",
    "                output = model(test_sample[0].to(device), target)\n",
    "                best_prediction = output.argmax(2)[-1].item()\n",
    "                predictions.append(best_prediction)\n",
    "                \n",
    "                if best_prediction == tokenizer.eos_token_id:\n",
    "                    break\n",
    "            \n",
    "            decoded_output = tokenizer.decode(predictions)\n",
    "            decoded_input  = tokenizer.decode(test_sample[0].squeeze(1))\n",
    "            decoded_target = tokenizer.decode(test_sample[1].squeeze(1))\n",
    "            \n",
    "            print(f'\\tInput : {decoded_input}')\n",
    "            print(f'\\tOutput: {decoded_output}')\n",
    "            print(f'\\tTarget: {decoded_target}')\n",
    "            \n",
    "        if patience == 0:\n",
    "            print(f'\\nModel learning finished due to early stopping')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0d3549-7aed-485d-8b49-d742ef811f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    model, optimizer, criterion,\n",
    "    train_data, val_data, test_data,\n",
    "    learning_params['epochs'], learning_params['max_norm'],\n",
    "    learning_params['patience'], epoch, params['device'], tokenizer,\n",
    "    model_path['dir'] + model_path['name'], params['max_seq_len']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733e205e-fc80-4462-b9d8-0a2e8e7376b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
