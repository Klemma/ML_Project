{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1b022f1-0105-47f1-8666-b01a49f67d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from pprint import pprint\n",
    "from random import random, sample\n",
    "from typing import List\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from functools import reduce\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn import model_selection\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from torchtext.data.metrics import bleu_score\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5c0deb7-b7ca-49c4-9982-71fe24b48578",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b7ed95-0365-4cb9-9c04-b0fc196f66b2",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c88853f-ffc6-4e8d-83b9-ded0866091e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/lenta/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90edc82f-a9e9-4a05-975a-81f15a714c1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orig_texts</th>\n",
       "      <th>lemm_texts</th>\n",
       "      <th>nsubj</th>\n",
       "      <th>gender</th>\n",
       "      <th>tense</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1245806</th>\n",
       "      <td>об этом сообщает риа новости со ссылкой на мат...</td>\n",
       "      <td>о это сообщать риа новость с ссылка на мать по...</td>\n",
       "      <td>риа</td>\n",
       "      <td>neut</td>\n",
       "      <td>pres</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594042</th>\n",
       "      <td>генеральный прокурор рф владимир устинов счита...</td>\n",
       "      <td>генеральный прокурор рф владимир устинов счита...</td>\n",
       "      <td>прокурор</td>\n",
       "      <td>masc</td>\n",
       "      <td>pres</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705659</th>\n",
       "      <td>телеканал «дождь» восстановил вещание, прерван...</td>\n",
       "      <td>телеканал « дождь » восстановить вещание, прер...</td>\n",
       "      <td>телеканал</td>\n",
       "      <td>masc</td>\n",
       "      <td>past</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603796</th>\n",
       "      <td>соответствующее требование прозвучало во время...</td>\n",
       "      <td>соответствующий требование прозвучать в время ...</td>\n",
       "      <td>требование</td>\n",
       "      <td>neut</td>\n",
       "      <td>past</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1430273</th>\n",
       "      <td>в пятницу вечером на сайтах \"единой россии\", \"...</td>\n",
       "      <td>в пятница вечером на сайт \"единый россия\", \"гр...</td>\n",
       "      <td>заявления</td>\n",
       "      <td>neut</td>\n",
       "      <td>past</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1331395</th>\n",
       "      <td>об этом заявил президент - председатель правле...</td>\n",
       "      <td>о это заявить президент - председатель правлен...</td>\n",
       "      <td>президент</td>\n",
       "      <td>masc</td>\n",
       "      <td>past</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1710395</th>\n",
       "      <td>\"мы думаем, что они (страны-члены совбеза) пре...</td>\n",
       "      <td>\\\" мы думать, что они (страна-член совбез) пре...</td>\n",
       "      <td>мы</td>\n",
       "      <td>undefined</td>\n",
       "      <td>pres</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1434398</th>\n",
       "      <td>социологи \"росгосстраха\" оценили сознательност...</td>\n",
       "      <td>социолог \"росгосстрах\"оценить сознательность р...</td>\n",
       "      <td>социологи</td>\n",
       "      <td>masc</td>\n",
       "      <td>past</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1832873</th>\n",
       "      <td>российская сборная сохранила за собой 24 строчку.</td>\n",
       "      <td>российский сборная сохранить за себя 24 строчка.</td>\n",
       "      <td>сборная</td>\n",
       "      <td>fem</td>\n",
       "      <td>past</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537488</th>\n",
       "      <td>эксперты компании связывают это с прекращением...</td>\n",
       "      <td>эксперт компания связывать это с прекращение и...</td>\n",
       "      <td>эксперты</td>\n",
       "      <td>masc</td>\n",
       "      <td>pres</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>926362 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                orig_texts  \\\n",
       "1245806  об этом сообщает риа новости со ссылкой на мат...   \n",
       "1594042  генеральный прокурор рф владимир устинов счита...   \n",
       "705659   телеканал «дождь» восстановил вещание, прерван...   \n",
       "603796   соответствующее требование прозвучало во время...   \n",
       "1430273  в пятницу вечером на сайтах \"единой россии\", \"...   \n",
       "...                                                    ...   \n",
       "1331395  об этом заявил президент - председатель правле...   \n",
       "1710395  \"мы думаем, что они (страны-члены совбеза) пре...   \n",
       "1434398  социологи \"росгосстраха\" оценили сознательност...   \n",
       "1832873  российская сборная сохранила за собой 24 строчку.   \n",
       "1537488  эксперты компании связывают это с прекращением...   \n",
       "\n",
       "                                                lemm_texts       nsubj  \\\n",
       "1245806  о это сообщать риа новость с ссылка на мать по...         риа   \n",
       "1594042  генеральный прокурор рф владимир устинов счита...    прокурор   \n",
       "705659   телеканал « дождь » восстановить вещание, прер...   телеканал   \n",
       "603796   соответствующий требование прозвучать в время ...  требование   \n",
       "1430273  в пятница вечером на сайт \"единый россия\", \"гр...   заявления   \n",
       "...                                                    ...         ...   \n",
       "1331395  о это заявить президент - председатель правлен...   президент   \n",
       "1710395  \\\" мы думать, что они (страна-член совбез) пре...          мы   \n",
       "1434398  социолог \"росгосстрах\"оценить сознательность р...   социологи   \n",
       "1832873   российский сборная сохранить за себя 24 строчка.     сборная   \n",
       "1537488  эксперт компания связывать это с прекращение и...    эксперты   \n",
       "\n",
       "            gender tense  \n",
       "1245806       neut  pres  \n",
       "1594042       masc  pres  \n",
       "705659        masc  past  \n",
       "603796        neut  past  \n",
       "1430273       neut  past  \n",
       "...            ...   ...  \n",
       "1331395       masc  past  \n",
       "1710395  undefined  pres  \n",
       "1434398       masc  past  \n",
       "1832873        fem  past  \n",
       "1537488       masc  pres  \n",
       "\n",
       "[926362 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(frac=0.5, random_state=RANDOM_STATE)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d32849-1b18-45e8-8e44-8bb2e70efed0",
   "metadata": {},
   "source": [
    "### Загрузка готового токенайзера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d6fe2d1-7b8b-4fe3-b356-f127143b9eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8612700e-b7b8-4da7-9c33-083cb44626f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "added_tokens_amount = tokenizer.add_special_tokens({\n",
    "    'bos_token': '[BOS]',\n",
    "    'eos_token': '[EOS]'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b023979c-329f-4d8a-988a-18821d0bf143",
   "metadata": {},
   "source": [
    "### Разбиение данных на тренировочные, валидационные, тестовые"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05ec5a14-b914-4927-a56b-074b4d9fe49e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df, test_df = model_selection.train_test_split(df, test_size=0.1, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fa7ccb1-bd26-4002-a5f3-08598cf27c6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df, val_df = model_selection.train_test_split(test_df, test_size=0.5, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896733ea-b65f-469e-a652-d284c528a938",
   "metadata": {},
   "source": [
    "### Инициализация гиперпараметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f3b9bb9-3296-47d9-ad18-21ff2ae2cd11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "epochs_amount = 50\n",
    "hidden_size = 768\n",
    "embedding_size = 300\n",
    "max_norm = 1.0\n",
    "dropout_p = 0.5\n",
    "gender_input_size = 4\n",
    "tense_input_size = 3\n",
    "context_hidden_size = 256\n",
    "context_output_size = hidden_size\n",
    "patience = 3\n",
    "output_size = tokenizer.vocab_size # 119547\n",
    "vocab_size = output_size\n",
    "pad_idx = tokenizer.pad_token_id\n",
    "model_path = './models/'\n",
    "model_name = 'seq2seq_attention (fixed).model'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0622ca5-f238-46ed-9f8e-615ac0e6d7ce",
   "metadata": {},
   "source": [
    "### One-hot кодирование значений рода и времени"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d532be7-85e3-43c9-94bb-e2177cbed569",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gender_to_vec = {\n",
    "    'masc': [1, 0, 0, 0],\n",
    "    'fem': [0, 1, 0, 0],\n",
    "    'neut': [0, 0, 1, 0],\n",
    "    'undefined': [0, 0, 0, 1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c716cfb-43ad-48b9-a99a-de6219807853",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tense_to_vec = {\n",
    "    'pres': [1, 0, 0],\n",
    "    'past': [0, 1, 0],\n",
    "    'fut': [0, 0, 1]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53357bb5-6448-44ff-b754-fcf38cd209e0",
   "metadata": {},
   "source": [
    "### Разбиение текстов на батчи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b6a4ddf-e9a3-4d7c-997a-1014769d93f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_batches(df, batch_size=batch_size):\n",
    "    return len(df) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbb39b2b-bb38-4fc5-8166-820db3cdc6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_n_batches = get_n_batches(train_df)\n",
    "test_n_batches = get_n_batches(test_df)\n",
    "val_n_batches = get_n_batches(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db7d2521-767e-494b-beb1-1911ff7954aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batches(df, n_batches, batch_size=batch_size, cache_path='./data/cached/train_batches_64.btch', download_cache=True):\n",
    "    try:\n",
    "        if not download_cache:\n",
    "            raise Exception\n",
    "            \n",
    "        with open(cache_path, 'rb') as f:\n",
    "            batches = pickle.load(f)\n",
    "            print(f'Batches are loaded from cache: {cache_path}')\n",
    "    except:\n",
    "        batches = []\n",
    "    \n",
    "        for n_batch in tqdm(range(n_batches), 'Making batches'):\n",
    "            batch = {\n",
    "                'orig_texts': df.orig_texts.to_list()[batch_size * n_batch:batch_size * (n_batch + 1)],\n",
    "                'lemm_texts': df.lemm_texts.to_list()[batch_size * n_batch:batch_size * (n_batch + 1)],\n",
    "                'nsubj': df.nsubj.to_list()[batch_size * n_batch:batch_size * (n_batch + 1)],\n",
    "                'gender': df.gender.to_list()[batch_size * n_batch:batch_size * (n_batch + 1)],\n",
    "                'tense': df.tense.to_list()[batch_size * n_batch:batch_size * (n_batch + 1)]\n",
    "            }\n",
    "\n",
    "            batches.append(batch)\n",
    "    \n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b43a4457-bb0a-4bba-9eea-f8a19865b09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches are loaded from cache: ./data/cached/train_batches_64.btch\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72d492d60ebd446f98e01256b778f085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Making batches', max=723.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf6f35e7c96e41bdbdc55ee0a8bac409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Making batches', max=723.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_batches = make_batches(train_df, train_n_batches, download_cache=True)\n",
    "test_batches = make_batches(test_df, test_n_batches, download_cache=False)\n",
    "val_batches = make_batches(val_df, val_n_batches, download_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dddc9315-bf58-4635-bba4-74226f98c6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./data/cached/train_batches_64.btch', 'wb') as f:\n",
    "#     pickle.dump(train_batches, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d498eb3e-2ecb-4916-8a73-548d88fa7caf",
   "metadata": {},
   "source": [
    "### Токенизация текстов, индексация токенов, batch padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "466cbd0f-d549-4922-8da8-16cc3d7b188e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_batch_to_tensor(text_batch, tokenizer=tokenizer, add_special_tokens=True):\n",
    "    tensor = tokenizer(\n",
    "        text_batch,\n",
    "        add_special_tokens=add_special_tokens,\n",
    "        padding='longest',\n",
    "        return_attention_mask=False,\n",
    "        return_token_type_ids=False,\n",
    "        return_tensors='pt'\n",
    "    ).get('input_ids')\n",
    "    \n",
    "    tensor[tensor == tokenizer.cls_token_id] = tokenizer.bos_token_id\n",
    "    tensor[tensor == tokenizer.sep_token_id] = tokenizer.eos_token_id\n",
    "\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dbacbed4-5b8c-4cf7-9bbd-004207b6d49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensors(batches, tokenizer=tokenizer):\n",
    "#     tensors_batches = []\n",
    "    \n",
    "    for batch in batches:\n",
    "        orig_tensor = text_batch_to_tensor(batch.get('orig_texts'))\n",
    "        lemm_tensor = text_batch_to_tensor(batch.get('lemm_texts'))\n",
    "        \n",
    "#         nsubj_tensor = text_batch_to_tensor(batch.get('nsubj'), add_special_tokens=False)\n",
    "        nsubj_tensor = tokenizer(list(chain(*[batch.get('nsubj') for batch in batches])),\n",
    "                                 add_special_tokens=False, padding='longest',\n",
    "                                 return_tensors='pt').get('input_ids')\n",
    "    \n",
    "        gender_tensor = torch.tensor([gender_to_vec.get(gender) for gender in batch.get('gender')],\n",
    "                                     dtype=torch.float32)\n",
    "        tense_tensor = torch.tensor([tense_to_vec.get(tense) for tense in batch.get('tense')],\n",
    "                                    dtype=torch.float32)\n",
    "        \n",
    "        tensors_batch = [orig_tensor, lemm_tensor,\n",
    "                         nsubj_tensor, gender_tensor, tense_tensor]\n",
    "        \n",
    "        yield tensors_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "738f1f0d-e0d5-4e66-82ae-c80cc38267ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensors = to_tensors(train_batches)\n",
    "test_tensors = to_tensors(test_batches)\n",
    "val_tensors = to_tensors(val_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7711c727-7d6a-409a-8d10-7d44d005f274",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-739bf9c5c71c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-26-1a29665ff0b8>\u001b[0m in \u001b[0;36mto_tensors\u001b[1;34m(batches, tokenizer)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#         nsubj_tensor = text_batch_to_tensor(batch.get('nsubj'), add_special_tokens=False)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         nsubj_tensor = tokenizer(list(chain(*[batch.get('nsubj') for batch in batches])),\n\u001b[0m\u001b[0;32m     10\u001b[0m                                  \u001b[0madd_special_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'longest'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m                                  return_tensors='pt').get('input_ids')\n",
      "\u001b[1;32mc:\\users\\iluxa\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2286\u001b[0m                 )\n\u001b[0;32m   2287\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2288\u001b[1;33m             return self.batch_encode_plus(\n\u001b[0m\u001b[0;32m   2289\u001b[0m                 \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2290\u001b[0m                 \u001b[0madd_special_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\iluxa\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2471\u001b[0m         )\n\u001b[0;32m   2472\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2473\u001b[1;33m         return self._batch_encode_plus(\n\u001b[0m\u001b[0;32m   2474\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2475\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\iluxa\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[0;32m    395\u001b[0m         \u001b[1;31m#                    ]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m         \u001b[1;31m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 397\u001b[1;33m         tokens_and_encodings = [\n\u001b[0m\u001b[0;32m    398\u001b[0m             self._convert_encoding(\n\u001b[0;32m    399\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\iluxa\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\tokenization_utils_fast.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    396\u001b[0m         \u001b[1;31m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    397\u001b[0m         tokens_and_encodings = [\n\u001b[1;32m--> 398\u001b[1;33m             self._convert_encoding(\n\u001b[0m\u001b[0;32m    399\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    400\u001b[0m                 \u001b[0mreturn_token_type_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_token_type_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\iluxa\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_convert_encoding\u001b[1;34m(self, encoding, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[0;32m    208\u001b[0m                 \u001b[0mencoding_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"token_type_ids\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mreturn_attention_mask\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m                 \u001b[0mencoding_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"attention_mask\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mreturn_special_tokens_mask\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m                 \u001b[0mencoding_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"special_tokens_mask\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspecial_tokens_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sample = next(train_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35df5a79-109b-4fb0-9730-801b0534bce4",
   "metadata": {},
   "source": [
    "## Построение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7435f92-3273-44a4-b705-a110171c9f83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ContextMem(nn.Module):\n",
    "    def __init__(self, gender_input_size, tense_input_size, hidden_size, output_size, nsubj_embedding_size, device):\n",
    "        super(ContextMem, self).__init__()\n",
    "        \n",
    "        self.device = device\n",
    "\n",
    "        self.gender_proj = nn.Linear(gender_input_size, hidden_size, bias=False)\n",
    "        self.tense_proj = nn.Linear(tense_input_size, hidden_size, bias=False)\n",
    "        self.fc_out = nn.Linear(hidden_size * 2 + nsubj_embedding_size, output_size, bias=False)\n",
    "        \n",
    "    def forward(self, nsubj_embedding, gender, tense):\n",
    "        # nsubj_embedding_shape: (token_len, batch_size, embedding_size)\n",
    "        # gender_shape: (batch_size, input_size)\n",
    "        # tense_shape: (batch_size, input_size)\n",
    "        print('nsubj shape:', nsubj_embedding.shape)\n",
    "        token_len = nsubj_embedding.shape[0]\n",
    "        \n",
    "        gender = self.gender_proj(gender).repeat(token_len, 1, 1)\n",
    "        # gender_shape: (nsubj_len, batch_size, hidden_size)\n",
    "        print('gender shape:', gender.shape)\n",
    "        \n",
    "        tense = self.tense_proj(tense).repeat(token_len, 1, 1)\n",
    "        # tense_shape: (nsubj_len, batch_size, hidden_size)    \n",
    "        print('tense shape:', tense.shape)\n",
    "        \n",
    "        context = torch.cat([nsubj_embedding, gender, tense], dim=-1)\n",
    "        # context_shape: (nsubj_len, batch_size, hidden_size * 2 + embedding_size) \n",
    "        \n",
    "        context = self.fc_out(context)\n",
    "        # context_shape: (nsubj_len, batch_size, output_size)\n",
    "        \n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8939770c-b556-4d31-bab7-9188897fe687",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_size: int, hidden_size: int, pad_idx: int,\n",
    "                 device, dropout_p: float, embedding=None, pretrained_embedding_loaded=False):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = embedding\n",
    "        self.pretrained_embedding_loaded = pretrained_embedding_loaded\n",
    "    \n",
    "        self.rnn = nn.GRU(embedding_size, hidden_size, dropout=0.0, bidirectional=True)\n",
    "        \n",
    "    def forward(self, sequence, hidden):\n",
    "        # sequence_shape: (seq_len, batch_size)\n",
    "        if self.pretrained_embedding_loaded:\n",
    "            with torch.no_grad():\n",
    "                embedding = self.embedding(sequence)\n",
    "        else:\n",
    "            embedding = self.embedding(sequence)\n",
    "        # embedding_shape: (seq_len, batch_size, embedding_size)\n",
    "        encoder_states = self.rnn(embedding, hidden)[0]\n",
    "        # encoder_states: (seq_len, batch_size, hidden_size * 2)\n",
    "        \n",
    "        return encoder_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18d377db-4756-42a3-a734-d104c4af9943",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_size: int, hidden_size: int, output_size: int, pad_idx: int,\n",
    "                 device, dropout_p: float, embedding=None, pretrained_embedding_loaded=False):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = embedding\n",
    "        self.pretrained_embedding_loaded = pretrained_embedding_loaded\n",
    "        \n",
    "        self.attn_weights = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size, bias=False),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 1, bias=False),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        self.rnn = nn.GRU(embedding_size + 2 * hidden_size, hidden_size, dropout=0.0)\n",
    "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, token, encoder_states):\n",
    "        # token_shape: (token_len, batch_size)\n",
    "        \n",
    "        encoder_states = torch.transpose(encoder_states, 1, 0)\n",
    "        # encoder_states_shape: (batch_size, seq_len, hidden_size * 2)\n",
    "        if self.pretrained_embedding_loaded:\n",
    "            with torch.no_grad():\n",
    "                embedding = self.embedding(token)\n",
    "        else:\n",
    "            embedding = self.embedding(token)\n",
    "        # embedding_shape: (token_len, batch_size, embedding_size)\n",
    "        \n",
    "        seq_len = encoder_states.shape[1]\n",
    "        token_len = token.shape[0]\n",
    "        \n",
    "        attn_weights = self.attn_weights(encoder_states)\n",
    "        # attn_weights_shape: (batch_size, seq_len, 1)\n",
    "        \n",
    "        context_vec = torch.bmm(attn_weights.permute(0, 2, 1), encoder_states).permute(1, 0, 2).repeat(token_len, 1, 1)\n",
    "        # context_vec_shape: (token_len, batch_size, hidden_size * 2)\n",
    "        \n",
    "        combined = torch.cat((embedding, context_vec), dim=2)\n",
    "        # combined_shape: (token_len, batch_size, embedding_size + 2 * hidden_size)\n",
    "        \n",
    "        rnn_out = self.rnn(combined)[0]\n",
    "        # lstm_out_shape: (seq_len=1, batch_size, hidden_size)\n",
    "        fc_out = self.fc_out(rnn_out)\n",
    "        # fc_out_shape: (seq_len=1, batch_size, output_size)\n",
    "        \n",
    "        return fc_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a008b865-a547-490b-9a27-0fb5cd9105d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Seq2SeqModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_size, embedding_size, hidden_size, output_size,\n",
    "                 gender_input_size, tense_input_size, context_hidden_size, context_output_size,\n",
    "                 pad_idx, device, dropout_p, pretrained_embedding=None):\n",
    "        super(Seq2SeqModel, self).__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        if pretrained_embedding is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(pretrained_embedding, padding_idx=pad_idx)\n",
    "            self.pretrained_embedding_loaded = True\n",
    "        else:\n",
    "            self.embedding = nn.Sequential(\n",
    "                nn.Embedding(vocab_size, embedding_size, padding_idx=pad_idx),\n",
    "                nn.Dropout(dropout_p)\n",
    "            )\n",
    "            self.pretrained_embedding_loaded = False\n",
    "        \n",
    "        self.context_mem = ContextMem(gender_input_size, tense_input_size, context_hidden_size, context_output_size, embedding_size, device).to(device)\n",
    "        self.encoder = EncoderRNN(vocab_size, embedding_size, hidden_size,\n",
    "                                  pad_idx, device, dropout_p,\n",
    "                                  self.embedding, self.pretrained_embedding_loaded).to(device)\n",
    "        self.decoder = DecoderRNN(vocab_size, embedding_size, hidden_size, output_size,\n",
    "                                  pad_idx, device, dropout_p,\n",
    "                                  self.embedding, self.pretrained_embedding_loaded).to(device)\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "    def forward(self, input, target, context, teacher_forcing_ratio=0.0):\n",
    "        batch_size = input.shape[1]\n",
    "        target_len = target.shape[0]\n",
    "        target_vocab_size = self.vocab_size\n",
    "        \n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size, device=self.device)\n",
    "        \n",
    "        nsubj, gender, tense = context\n",
    "        nsubj = nsubj.permute(1, 0)\n",
    "        # nsubj_shape:  (token_len, batch_size)\n",
    "        # gender_shape: (batch_size, gender_input_size)\n",
    "        # tense_shape:  (batch_size, tense_input_size)\n",
    "        \n",
    "        if self.pretrained_embedding_loaded:\n",
    "            with torch.no_grad():\n",
    "                nsubj_embedding = self.embedding(nsubj)\n",
    "        else:\n",
    "            nsubj_embedding = self.embedding(nsubj)\n",
    "            # nsubj_embedding_shape: (token_len, batch_size, embedding_size)\n",
    "        \n",
    "        hidden = self.context_mem(nsubj_embedding, gender, tense)[:2]\n",
    "        # hidden_shape: (2, batch_size, context_output_size=hidden_size)\n",
    "        \n",
    "        encoder_states = self.encoder(input, hidden)\n",
    "        # encoder_states_shape: (seq_len, batch_size, hidden_size * 2)\n",
    "        \n",
    "        prev_token_idx = target[0] # sos_token\n",
    "        # prev_token_shape: (batch_size)\n",
    "        \n",
    "        for t in range(1, target_len):\n",
    "            output, hidden, cell = self.decoder(prev_token_idx, encoder_states)\n",
    "            # output_shape: (1, batch_size, output_size)\n",
    "            outputs[t] = output.squeeze(0)\n",
    "            \n",
    "            best_prediction = outputs[t].argmax(dim=1)\n",
    "            # best_prediction_shape: (batch_size)\n",
    "            prev_token_idx = target[t] if random() < teacher_forcing_ratio else best_prediction\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2fef8c-007d-4bdf-ae38-22140eaddfc6",
   "metadata": {},
   "source": [
    "## Обучение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6b74de-5edc-4bdb-841a-76f6e22c3f55",
   "metadata": {},
   "source": [
    "### Функция сохранения текущего состояния модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25c2f62f-09a9-486a-93db-7fab5f8dfc38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, epoch, path):\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'criterion': criterion,\n",
    "        'epoch': epoch\n",
    "    }\n",
    "    \n",
    "    torch.save(checkpoint, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd179dba-77ac-4069-a769-2df334fa0edf",
   "metadata": {},
   "source": [
    "### Функция загрузки уже тренировавшейся модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6925f759-c53d-4b02-87f1-2fc4a6fc5a1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_model(model, optimizer, criterion, path, for_inference=True, device=torch.device('cpu')):\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    if not for_inference:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        epoch = checkpoint['epoch']\n",
    "        criterion = checkpoint['criterion']\n",
    "\n",
    "        return epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "82df29b6-5f0a-48d5-8c59-d089dc0b46c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d20d0490-1df5-4838-ac3a-65d19b924d28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Seq2SeqModel(vocab_size, embedding_size, hidden_size, output_size,\n",
    "                     gender_input_size, tense_input_size, context_hidden_size, context_output_size, \n",
    "                     pad_idx, device, dropout_p).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fa06727e-4218-4536-9312-a68eabb7abd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e16b66b9-0b96-435a-b962-3447bbc64ba7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d45d8b71-0c9c-4f89-a720-57632e5c0b79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No models found at ./models/\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    epoch = load_model(model, optimizer, criterion, model_path + model_name, for_inference=True)\n",
    "    print(f'Loaded model from {model_path}')\n",
    "except:\n",
    "    print(f'No models found at {model_path}')\n",
    "    epoch = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4164258-6317-4ee9-a228-a156ef7dc261",
   "metadata": {},
   "source": [
    "### Определение функции обучения сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bdc09134-0caa-441d-b070-92e3657629bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, train_data, val_data, test_data, epochs_amount, max_norm, patience=3, current_epoch=1, n_prints=5):\n",
    "    min_mean_val_loss = float('+inf')\n",
    "    initial_patiece = patience\n",
    "    print_every = len(train_df) // n_prints\n",
    "    \n",
    "    for epoch in tqdm(range(current_epoch, epochs_amount + 1), 'Epochs'):\n",
    "        print(f'\\nEpoch [{epoch} / {epochs_amount}]')\n",
    "        \n",
    "        model.train()\n",
    "        for iteration, (input, target, nsubj, gender, tense) in enumerate(tqdm(train_data, 'Epoch training iterations')):\n",
    "            optimizer.zero_grad()\n",
    "            # input = lemm_texts, target = orig_texts\n",
    "            \n",
    "            input = torch.transpose(input, 1, 0).to(device)\n",
    "            # input_shape: (seq_len, batch_size)\n",
    "            \n",
    "            target = torch.transpose(target, 1, 0).to(device)\n",
    "            # target_shape: (seq_len, batch_size)\n",
    "            \n",
    "            context = (nsubj.to(device), gender.to(device), tense.to(device))\n",
    "            \n",
    "            output = model(input, target, context)\n",
    "            # output_shape: (seq_len, batch_size, vocab_size) but need (N, vocab_size)\n",
    "            \n",
    "            target = target[1:].reshape(-1)\n",
    "            # now target_shape is (seq_len * batch_size)\n",
    "            \n",
    "            vocab_size = output.shape[2]\n",
    "            \n",
    "            output = output[1:].reshape(-1, vocab_size)\n",
    "            # now output_shape is (seq_len * batch_size, vocab_size)\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            if iteration % print_every == 0:\n",
    "                print(f'\\tIteration #{iteration}: training loss = {loss.item()}')\n",
    "            elif iteration == len(train_data):\n",
    "                print(f'\\tIteration #{iteration}: training loss = {loss.item()}')\n",
    "            \n",
    "            \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            val_loss = []\n",
    "            \n",
    "            for input, target, nsubj, gender, tense in tqdm(val_data, 'Epoch validating iterations'):\n",
    "                input = torch.transpose(input, 1, 0).to(device)\n",
    "                target = torch.transpose(target, 1, 0).to(device)\n",
    "                context = (nsubj.to(device), gender.to(device), tense.to(device))\n",
    "                \n",
    "                output = model(input, target, context)\n",
    "                vocab_size = output.shape[2]\n",
    "                output = output[1:].reshape(-1, orig_vocab_size)\n",
    "                target = target[1:].reshape(-1)\n",
    "                \n",
    "                val_loss.append(criterion(output, target).item())\n",
    "            \n",
    "            mean_val_loss = sum(val_loss) / len(val_loss)\n",
    "            print(f'\\tValidation loss = {mean_val_loss}')\n",
    "            if mean_val_loss < min_mean_val_loss:\n",
    "                try:\n",
    "                    save_model(model, optimizer, epoch, model_path + model_name)\n",
    "                    min_mean_val_loss = mean_val_loss\n",
    "                    patience = initial_patiece\n",
    "                except Exception as exc:\n",
    "                    print(exc)\n",
    "            else:\n",
    "                patience -= 1\n",
    "            \n",
    "            test_data = DataLoader(test_data.dataset, batch_size=1, shuffle=True)\n",
    "            for input, target, nsubj, gender, tense in test_data:\n",
    "                target = target.squeeze(0).to(device)\n",
    "                context = (nsubj.to(device), gender.to(device), tense.to(device))\n",
    "                \n",
    "                input = torch.transpose(input, 1, 0)\n",
    "                target_len = target.shape[0]\n",
    "                \n",
    "                output = test_evaluate(model, input, context, target_len)\n",
    "                decoded_input = [text_transformer.vocab.idx_to_token(idx.item()) for idx in input]\n",
    "                decoded_target = [text_transformer.vocab.idx_to_token(idx.item()) for idx in target]\n",
    "                \n",
    "                print(f'\\tInput: {decoded_input}')\n",
    "                print(f'\\tOutput: {output}')\n",
    "                print(f'\\tTarget: {decoded_target}')\n",
    "                break\n",
    "        \n",
    "        if patience == 0:\n",
    "            print(f'\\nModel learning finished due to early stopping')\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37684db3-c1e5-47bb-9bfc-92fd1b4f3922",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abbfc103071943e6b6eccfc3e3f4e645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epochs', max=50.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [1 / 50]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb8a15c7762649ce8535220f066b453a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Epoch training iterations', layout=Layo…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nsubj shape: torch.Size([3, 64, 300])\n",
      "gender shape: torch.Size([3, 64, 256])\n",
      "tense shape: torch.Size([3, 64, 256])\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuDNN error: CUDNN_STATUS_EXECUTION_FAILED",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-875a6e0062bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs_amount\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-31-abd0e451c31f>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, criterion, train_data, val_data, test_data, epochs_amount, max_norm, patience, current_epoch, n_prints)\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnsubj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgender\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtense\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m             \u001b[1;31m# output_shape: (seq_len, batch_size, vocab_size) but need (N, vocab_size)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\iluxa\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-577f40e18a11>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target, context, teacher_forcing_ratio)\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[1;31m# hidden_shape: (2, batch_size, context_output_size=hidden_size)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m         \u001b[0mencoder_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m         \u001b[1;31m# encoder_states_shape: (seq_len, batch_size, hidden_size * 2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\iluxa\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-7e18802234c0>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, sequence, hidden)\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0membedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;31m# embedding_shape: (seq_len, batch_size, embedding_size)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mencoder_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[1;31m# encoder_states: (seq_len, batch_size, hidden_size * 2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\iluxa\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\iluxa\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    819\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    820\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 821\u001b[1;33m             result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[0;32m    822\u001b[0m                              self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0;32m    823\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, criterion, train_tensors, val_tensors, test_tensors, epochs_amount, max_norm, patience, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade83f53-bbe7-42a2-a457-3017d194934c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
