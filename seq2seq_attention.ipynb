{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Импорт необходимых зависимостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from random import random, sample\n",
    "from typing import List\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from functools import reduce\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn import model_selection\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/russian_literature/processed/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orig_texts</th>\n",
       "      <th>lemm_texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>вообще он опасался правил.</td>\n",
       "      <td>вообще он опасаться правило.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>кто сможет ему родить?</td>\n",
       "      <td>кто смочь он родить?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>вагон был совершенно гнилой.</td>\n",
       "      <td>вагон быть совершенно гнилой.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>это оказалась станция железнодорожная.</td>\n",
       "      <td>это оказаться станция железнодорожный.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>и тестирования весьма неожиданного.</td>\n",
       "      <td>и тестирование весьма неожиданный.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27889</th>\n",
       "      <td>лаудон же, вместо того чтобы оборонять колонии...</td>\n",
       "      <td>лаудон же, вместо тот чтобы оборонять колония ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27890</th>\n",
       "      <td>я с готовностью оказал ей несколько маленьких ...</td>\n",
       "      <td>я с готовность оказать она несколько маленький...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27891</th>\n",
       "      <td>это конечно особая тема, и она требует самого ...</td>\n",
       "      <td>это конечно особый тема, и она требовать сам п...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27892</th>\n",
       "      <td>конечно, если вы смогли вскарабкаться на верши...</td>\n",
       "      <td>конечно, если вы смочь вскарабкаться на вершин...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27893</th>\n",
       "      <td>в ответ на это я составил свой план и попросил...</td>\n",
       "      <td>в ответ на это я составить свой план и попроси...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27894 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              orig_texts  \\\n",
       "0                             вообще он опасался правил.   \n",
       "1                                 кто сможет ему родить?   \n",
       "2                           вагон был совершенно гнилой.   \n",
       "3                 это оказалась станция железнодорожная.   \n",
       "4                    и тестирования весьма неожиданного.   \n",
       "...                                                  ...   \n",
       "27889  лаудон же, вместо того чтобы оборонять колонии...   \n",
       "27890  я с готовностью оказал ей несколько маленьких ...   \n",
       "27891  это конечно особая тема, и она требует самого ...   \n",
       "27892  конечно, если вы смогли вскарабкаться на верши...   \n",
       "27893  в ответ на это я составил свой план и попросил...   \n",
       "\n",
       "                                              lemm_texts  \n",
       "0                           вообще он опасаться правило.  \n",
       "1                                   кто смочь он родить?  \n",
       "2                          вагон быть совершенно гнилой.  \n",
       "3                 это оказаться станция железнодорожный.  \n",
       "4                     и тестирование весьма неожиданный.  \n",
       "...                                                  ...  \n",
       "27889  лаудон же, вместо тот чтобы оборонять колония ...  \n",
       "27890  я с готовность оказать она несколько маленький...  \n",
       "27891  это конечно особый тема, и она требовать сам п...  \n",
       "27892  конечно, если вы смочь вскарабкаться на вершин...  \n",
       "27893  в ответ на это я составить свой план и попроси...  \n",
       "\n",
       "[27894 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Определение классов словаря и трансформера текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, tokens: List[str], unk_idx: int):\n",
    "        self._tokens = tokens\n",
    "        self._token_to_idx = {token: idx for idx, token in enumerate(tokens)}\n",
    "        self._unk_idx = unk_idx\n",
    "        \n",
    "    def token_to_idx(self, token: str) -> int:\n",
    "        return self._token_to_idx.get(token, self._unk_idx)\n",
    "    \n",
    "    def idx_to_token(self, idx: int) -> str:\n",
    "        return self._tokens[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTransformer:\n",
    "    def __init__(self, vocab_size: int):\n",
    "        self.vocab = None\n",
    "        self.vocab_size = vocab_size\n",
    "        self.special_tokens_to_idx = {'<UNK>': 0, '<PAD>': 1, '<SOS>': 2, '<EOS>': 3}\n",
    "        self._tokenizer = nltk.tokenize.wordpunct_tokenize\n",
    "    \n",
    "    def tokenize(self, text) -> List[str]:\n",
    "        return self._tokenizer(text.lower())\n",
    "    \n",
    "    def build_vocab(self, tokens: List[str]):\n",
    "        tokens_ = [special_token for special_token in self.special_tokens_to_idx.keys()]\n",
    "        special_tokens_amount = len(self.special_tokens_to_idx)\n",
    "        \n",
    "        for token, _ in Counter(tokens).most_common(self.vocab_size - special_tokens_amount):\n",
    "            tokens_.append(token)\n",
    "        \n",
    "        unk_idx = self.special_tokens_to_idx.get('<UNK>')\n",
    "        self.vocab = Vocab(tokens_, unk_idx)\n",
    "        \n",
    "    def transform_text(self, text: str) -> List[int]:\n",
    "        tokenized_text = self.tokenize(text)\n",
    "        transformed = [self.vocab.token_to_idx(token) for token in tokenized_text]\n",
    "        return transformed\n",
    "    \n",
    "    def fit(self, texts: List[str]) -> None:\n",
    "        transformed_texts = []\n",
    "        \n",
    "        tokenized_texts = [self.tokenize(text) for text in tqdm(texts, 'Tokenizing texts')]\n",
    "        tokens = chain(*tokenized_texts)\n",
    "        self.build_vocab(tokens)\n",
    "        \n",
    "        for tokenized_text in tqdm(tokenized_texts, 'Transforming texts'):\n",
    "            transformed = [self.vocab.token_to_idx(token) for token in tokenized_text]\n",
    "            transformed_texts.append(transformed)\n",
    "    \n",
    "    def transform_texts(self, texts: List[str]) -> List[List[int]]:\n",
    "        transformed_texts = [transform_text(text) for text in tqdm(texts, 'Transforming texts')]\n",
    "        return transformed_texts\n",
    "    \n",
    "    def text_to_tensor(self, text: str, max_seq_len) -> torch.tensor:\n",
    "        transformed_text = self.transform_text(text)\n",
    "        pad_idx = self.special_tokens_to_idx.get('<PAD>')\n",
    "        sos_idx = self.special_tokens_to_idx.get('<SOS>')\n",
    "        eos_idx = self.special_tokens_to_idx.get('<EOS>')\n",
    "        \n",
    "        pad_size = 0\n",
    "        if len(transformed_text) >= max_seq_len:\n",
    "            transformed_text = transformed_text[:max_seq_len]\n",
    "        else:\n",
    "            pad_size = max_seq_len - len(transformed_text)\n",
    "            transformed_text.extend([pad_idx] * pad_size)   \n",
    "        transformed_text.insert(0, sos_idx)\n",
    "        transformed_text.insert(len(transformed_text) - pad_size, eos_idx)\n",
    "        \n",
    "        tensor = torch.tensor(transformed_text, dtype=torch.long)\n",
    "        return tensor.unsqueeze(0)\n",
    "    \n",
    "    def texts_to_tensor(self, texts: List[str], max_seq_len) -> torch.tensor:\n",
    "        pad_idx = self.special_tokens_to_idx.get('<PAD>')\n",
    "        sos_idx = self.special_tokens_to_idx.get('<SOS>')\n",
    "        eos_idx = self.special_tokens_to_idx.get('<EOS>')\n",
    "        transformed_texts = []\n",
    "        \n",
    "        for text in tqdm(texts, 'Building tensor'):\n",
    "            transformed_text = self.transform_text(text)\n",
    "            pad_size = 0\n",
    "            if len(transformed_text) >= max_seq_len:\n",
    "                transformed_text = transformed_text[:max_seq_len]\n",
    "            else:\n",
    "                pad_size = max_seq_len - len(transformed_text)\n",
    "                transformed_text.extend([pad_idx] * pad_size)   \n",
    "            transformed_text.insert(0, sos_idx)\n",
    "            transformed_text.insert(len(transformed_text) - pad_size, eos_idx)\n",
    "            transformed_texts.append(transformed_text)\n",
    "        \n",
    "        tensor = torch.tensor(transformed_texts, dtype=torch.long).permute(1, 0)\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разбиение данных на обучающую, тестовую и валидационную выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = model_selection.train_test_split(df, test_size=0.1, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df, val_df = model_selection.train_test_split(test_df, test_size=0.5, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Токенизация текстов и индексация токенов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemm_vocab_size = 23000\n",
    "orig_vocab_size = 65000\n",
    "max_seq_len = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemm_text_transformer = TextTransformer(lemm_vocab_size)\n",
    "orig_text_transformer = TextTransformer(orig_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b08b7a4b4b224a1aa72f3d9a4ab2ca50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing texts:   0%|          | 0/25104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c309b1a1db04de593745cbbc13c9921",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transforming texts:   0%|          | 0/25104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lemm_text_transformer.fit(train_df.lemm_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "870d5abaedb643a79840691a60c72146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing texts:   0%|          | 0/25104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc7e93bc1705476aac6c18689ceb3ad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transforming texts:   0%|          | 0/25104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "orig_text_transformer.fit(train_df.orig_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Перевод данных в тензоры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed839711a0214cb4851fb63f644822e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building tensor:   0%|          | 0/25104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94dcb5ea4ae64737b381ffe661b11d51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building tensor:   0%|          | 0/1395 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8abccfc685b14db98acf38f505dbbacc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building tensor:   0%|          | 0/1395 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_lemm_tensor = lemm_text_transformer.texts_to_tensor(train_df.lemm_texts.to_list(), max_seq_len)\n",
    "test_lemm_tensor = lemm_text_transformer.texts_to_tensor(test_df.lemm_texts.to_list(), max_seq_len)\n",
    "val_lemm_tensor = lemm_text_transformer.texts_to_tensor(val_df.lemm_texts.to_list(), max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cc52758a87c444686f17cb86a839e33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building tensor:   0%|          | 0/25104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d08017f7b4b540dfb9ba4d45a4f8f5f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building tensor:   0%|          | 0/1395 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87fef628a0554ce09b3df7d0b6e4ed4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building tensor:   0%|          | 0/1395 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_orig_tensor = orig_text_transformer.texts_to_tensor(train_df.orig_texts.to_list(), max_seq_len)\n",
    "test_orig_tensor = orig_text_transformer.texts_to_tensor(test_df.orig_texts.to_list(), max_seq_len)\n",
    "val_orig_tensor = orig_text_transformer.texts_to_tensor(val_df.orig_texts.to_list(), max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_to_fit_batch(tensor: torch.Tensor, batch_size: int):\n",
    "    n_samples = tensor.shape[1]\n",
    "    new_n_samples = (n_samples // batch_size) * batch_size\n",
    "    result = tensor.split(new_n_samples, dim=1)[0]\n",
    "    return torch.transpose(result, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Построение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_size: int, hidden_size: int, pad_idx: int,\n",
    "                 device, num_layers, dropout_p: float):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, embedding_size, pad_idx),\n",
    "            nn.Dropout(dropout_p)\n",
    "        )\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=0.0, bidirectional=True)\n",
    "        self.fc_compressor_hidden = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.fc_compressor_cell = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x_shape: (seq_len, batch_size)\n",
    "        embedding = self.embedding(x)\n",
    "        # embedding_shape: (seq_len, batch_size, embedding_size)\n",
    "        encoder_states, (hidden, cell) = self.lstm(embedding)\n",
    "        # encoder_states: (seq_len, batch_size, hidden_size * 2)\n",
    "        # hidden_shape: (num_layers=1 * 2, batch_size, hidden_size)\n",
    "        # cell_shape: (num_layers=1 * 2, batch_size, hidden_size)\n",
    "        \n",
    "        bi_hidden = torch.cat((hidden[0], hidden[1]), dim=1).unsqueeze(0).permute(1, 0, 2)\n",
    "        bi_cell = torch.cat((cell[0], cell[1]), dim=1).unsqueeze(0).permute(1, 0, 2)\n",
    "        # bi_hidden, bi_cell shapes: (batch_size, 1, hidden_size * 2)\n",
    "        \n",
    "        hidden_compressed = self.fc_compressor_hidden(bi_hidden).permute(1, 0, 2)\n",
    "        cell_compressed = self.fc_compressor_hidden(bi_cell).permute(1, 0, 2)\n",
    "        # hidden_compressed, cell_compressed shapes: (1, batch_size, hidden_size)\n",
    "        \n",
    "        return encoder_states, hidden_compressed, cell_compressed\n",
    "    \n",
    "    def init_hidden_state(self, batch_size: int):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(self.device)\n",
    "        cell = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(self.device)\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_size: int, hidden_size: int, output_size: int, pad_idx: int,\n",
    "                 device, num_layers, dropout_p: float):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, embedding_size, pad_idx),\n",
    "            nn.Dropout(dropout_p)\n",
    "        )\n",
    "        self.attn_weights = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 3, hidden_size, bias=False),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 1, bias=False),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        self.lstm = nn.LSTM(embedding_size + 2 * hidden_size, hidden_size, num_layers, dropout=0.0)\n",
    "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x, encoder_states, hidden, cell):\n",
    "        x = x.unsqueeze(0)\n",
    "        # x_shape: (seq_len=1, batch_size)\n",
    "        # hidden_shape: (1, batch_size, hidden_size)\n",
    "        # cell_shape: (1, batch_size, hidden_size)\n",
    "        encoder_states = torch.transpose(encoder_states, 1, 0)\n",
    "        # encoder_states_shape: (batch_size, seq_len, hidden_size * 2)\n",
    "        embedding = self.embedding(x)\n",
    "        # embedding_shape: (seq_len=1, batch_size, embedding_size)\n",
    "        \n",
    "        seq_len = encoder_states.shape[1]\n",
    "        hidden_repeated = hidden.repeat(seq_len, 1, 1).permute(1, 0, 2)\n",
    "        # hidden_repeated_shape: (batch_size, seq_len, hidden_size)\n",
    "        \n",
    "        attn_weights = self.attn_weights(torch.cat((hidden_repeated, encoder_states), dim=2))\n",
    "        # attn_weights_shape: (batch_size, seq_len, 1)\n",
    "        \n",
    "        context_vec = torch.bmm(attn_weights.permute(0, 2, 1), encoder_states).permute(1, 0, 2)\n",
    "        # context_vec_shape: (1, batch_size, hidden_size * 2)\n",
    "        \n",
    "        combined = torch.cat((embedding, context_vec), dim=2)\n",
    "        # combined_shape: (1, batch_size, embedding_size + 2 * hidden_size)\n",
    "        \n",
    "        lstm_out, (hidden, cell) = self.lstm(combined, (hidden, cell))\n",
    "        # lstm_out_shape: (seq_len=1, batch_size, hidden_size)\n",
    "        fc_out = self.fc_out(lstm_out)\n",
    "        # fc_out_shape: (seq_len=1, batch_size, output_size)\n",
    "        \n",
    "        return fc_out, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqModel(nn.Module):\n",
    "    def __init__(self, encoder_vocab_size: int, decoder_vocab_size: int, embedding_size: int, hidden_size: int, output_size: int,\n",
    "                 pad_idx: int, device, num_layers, dropout_p: float):\n",
    "        super(Seq2SeqModel, self).__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        self.encoder = EncoderRNN(encoder_vocab_size, embedding_size, hidden_size, pad_idx, device, num_layers, dropout_p).to(device)\n",
    "        self.decoder = DecoderRNN(decoder_vocab_size, embedding_size, hidden_size, output_size, pad_idx, device, num_layers, dropout_p).to(device)\n",
    "        self.decoder_vocab_size = decoder_vocab_size\n",
    "        \n",
    "    def forward(self, input, target, teacher_forcing_ratio=0.5):\n",
    "        batch_size = input.shape[1]\n",
    "        target_len = target.shape[0]\n",
    "        target_vocab_size = self.decoder_vocab_size\n",
    "        \n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size, device=self.device)\n",
    "        \n",
    "        encoder_states, hidden, cell = self.encoder(input)\n",
    "        # hidden, cell shapes: (num_layers * 2, batch_size, hidden_size)\n",
    "        \n",
    "        prev_token_idx = target[0]\n",
    "        # prev_token_shape: (batch_size)\n",
    "        \n",
    "        for t in range(1, target_len):\n",
    "            output, hidden, cell = self.decoder(prev_token_idx, encoder_states, hidden, cell)\n",
    "            # output_shape: (1, batch_size, output_size)\n",
    "            outputs[t] = output.squeeze(0)\n",
    "            \n",
    "            best_prediction = outputs[t].argmax(dim=1)\n",
    "            # best_prediction_shape: (batch_size)\n",
    "            prev_token_idx = target[t] if random() < teacher_forcing_ratio else best_prediction\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция сохранения текущего состояния модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, epoch, path):\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'criterion': criterion,\n",
    "        'epoch': epoch\n",
    "    }\n",
    "    \n",
    "    torch.save(checkpoint, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция загрузки уже тренировавшейся модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, optimizer, criterion, path, for_inference=True, device=torch.device('cpu')):\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    if not for_inference:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        epoch = checkpoint['epoch']\n",
    "        criterion = checkpoint['criterion']\n",
    "\n",
    "        return epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Инициализация гиперпараметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "batch_size = 16\n",
    "epochs_amount = 50\n",
    "hidden_size = 512\n",
    "embedding_size = 300\n",
    "num_layers = 1\n",
    "max_norm = 1.0\n",
    "dropout_p = 0.5\n",
    "patience = 5\n",
    "output_size = orig_vocab_size\n",
    "pad_idx = lemm_text_transformer.special_tokens_to_idx.get('<PAD>')\n",
    "model_path = './models/'\n",
    "model_name = 'seq2seq_attention.model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2SeqModel(lemm_vocab_size, orig_vocab_size, embedding_size, hidden_size, output_size, pad_idx, device, num_layers, dropout_p).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No models found at ./\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    epoch = load_model(model, optimizer, criterion, model_path + model_name, for_inference=False)\n",
    "    print(f'Loaded model from {model_path}')\n",
    "except:\n",
    "    print(f'No models found at {model_path}')\n",
    "    epoch = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Урезание данных для соответствия размеру батча"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lemm_tensor_f = cut_to_fit_batch(train_lemm_tensor, batch_size)\n",
    "train_orig_tensor_f = cut_to_fit_batch(train_orig_tensor, batch_size)\n",
    "\n",
    "test_lemm_tensor_f = cut_to_fit_batch(test_lemm_tensor, batch_size)\n",
    "test_orig_tensor_f = cut_to_fit_batch(test_orig_tensor, batch_size)\n",
    "\n",
    "val_lemm_tensor_f = cut_to_fit_batch(val_lemm_tensor, batch_size)\n",
    "val_orig_tensor_f = cut_to_fit_batch(val_orig_tensor, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Инициализация данных итерируемых по батчам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(train_lemm_tensor_f, train_orig_tensor_f)\n",
    "test_dataset = TensorDataset(test_lemm_tensor_f, test_orig_tensor_f)\n",
    "val_dataset = TensorDataset(val_lemm_tensor_f, val_orig_tensor_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Определение функции проверки работы сети между эпохами обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_evaluate(model, input, target_len):\n",
    "    input = input.to(device)\n",
    "    sos_idx = lemm_text_transformer.special_tokens_to_idx.get('<SOS>')\n",
    "    eos_idx = lemm_text_transformer.special_tokens_to_idx.get('<EOS>')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        hidden, cell = model.encoder(input)\n",
    "        \n",
    "        predicted_indexes = [sos_idx]\n",
    "        \n",
    "        for _ in range(1, target_len):\n",
    "            prev_idx = torch.tensor([predicted_indexes[-1]], dtype=torch.long, device=device)\n",
    "            \n",
    "            output, hidden, cell = model.decoder(prev_idx, hidden, cell)\n",
    "            output = output.squeeze(0)\n",
    "            \n",
    "            best_prediction = output.argmax(dim=1).item()\n",
    "            \n",
    "            if best_prediction == eos_idx:\n",
    "                break\n",
    "                \n",
    "            predicted_indexes.append(best_prediction)\n",
    "                        \n",
    "        \n",
    "    predicted_tokens = [orig_text_transformer.vocab.idx_to_token(idx) for idx in predicted_indexes]\n",
    "    return predicted_tokens[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Определение функции обучения сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, train_data, val_data, test_data, epochs_amount, max_norm, patience=2, current_epoch=1, n_prints=5):\n",
    "    min_mean_val_loss = float('+inf')\n",
    "    initial_patiece = patience\n",
    "    print_every = len(train_data) // n_prints\n",
    "    \n",
    "    for epoch in tqdm(range(current_epoch, epochs_amount + 1), 'Epochs'):\n",
    "        print(f'\\nEpoch [{epoch} / {epochs_amount}]')\n",
    "        \n",
    "        model.train()\n",
    "        for iteration, (input, target) in enumerate(tqdm(train_data, 'Epoch training iterations')):\n",
    "            optimizer.zero_grad()\n",
    "            # input = lemm_texts, target = orig_texts\n",
    "            input = torch.transpose(input, 1, 0).to(device)\n",
    "            # input_shape: (seq_len, batch_size)\n",
    "            target = torch.transpose(target, 1, 0).to(device)\n",
    "            # target_shape: (seq_len, batch_size)\n",
    "            output = model(input, target)\n",
    "            # output_shape: (seq_len, batch_size, orig_vocab_size) but need (N, orig_vocab_size)\n",
    "            target = target[1:].reshape(-1)\n",
    "            # now target_shape is (seq_len * batch_size)\n",
    "            orig_vocab_size = output.shape[2]\n",
    "            output = output[1:].reshape(-1, orig_vocab_size)\n",
    "            # now output_shape is (seq_len * batch_size, orig_vocab_size)\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            if iteration % print_every == 0:\n",
    "                print(f'\\tIteration #{iteration}: training loss = {loss.item()}')\n",
    "            elif iteration == len(train_data):\n",
    "                print(f'\\tIteration #{iteration}: training loss = {loss.item()}')\n",
    "            \n",
    "            \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            val_loss = []\n",
    "            \n",
    "            for input, target in tqdm(val_data, 'Epoch validating iterations'):\n",
    "                input = torch.transpose(input, 1, 0).to(device)\n",
    "                target = torch.transpose(target, 1, 0).to(device)\n",
    "                \n",
    "                output = model(input, target)\n",
    "                orig_vocab_size = output.shape[2]\n",
    "                output = output[1:].reshape(-1, orig_vocab_size)\n",
    "                target = target[1:].reshape(-1)\n",
    "                \n",
    "                val_loss.append(criterion(output, target).item())\n",
    "            \n",
    "            mean_val_loss = sum(val_loss) / len(val_loss)\n",
    "            print(f'\\tValidation loss = {mean_val_loss}')\n",
    "            if mean_val_loss < min_mean_val_loss:\n",
    "                try:\n",
    "                    save_model(model, optimizer, epoch, model_path + model_name)\n",
    "                    min_mean_val_loss = mean_val_loss\n",
    "                    patience = initial_patiece\n",
    "                except Exception as exc:\n",
    "                    print(exc)\n",
    "            else:\n",
    "                patience -= 1\n",
    "            \n",
    "            test_data = DataLoader(test_data.dataset, batch_size=1, shuffle=True)\n",
    "            for input, target in test_data:\n",
    "                target = target.squeeze(0).to(device)\n",
    "                \n",
    "                input = torch.transpose(input, 1, 0)\n",
    "                target_len = target.shape[0]\n",
    "                \n",
    "                output = test_evaluate(model, input, target_len)\n",
    "                decoded_input = [lemm_text_transformer.vocab.idx_to_token(idx.item()) for idx in input]\n",
    "                decoded_target = [orig_text_transformer.vocab.idx_to_token(idx.item()) for idx in target]\n",
    "                \n",
    "                print(f'\\tInput: {decoded_input}')\n",
    "                print(f'\\tOutput: {output}')\n",
    "                print(f'\\tTarget: {decoded_target}')\n",
    "                break\n",
    "        \n",
    "        if patience == 0:\n",
    "            print(f'\\nModel learning finished due to early stopping')\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Определение функции эксплуатации обученной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: Seq2SeqModel, sentence: str, max_seq_len):\n",
    "    input_tensor = lemm_text_transformer.text_to_tensor(sentence, max_seq_len).to(device)\n",
    "    input_tensor = torch.transpose(input_tensor, 1, 0)\n",
    "    sos_idx = lemm_text_transformer.special_tokens_to_idx.get('<SOS>')\n",
    "    eos_idx = lemm_text_transformer.special_tokens_to_idx.get('<EOS>')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        hidden, cell = model.encoder(input_tensor)\n",
    "        \n",
    "        predicted_indexes = [sos_idx]\n",
    "        \n",
    "#         while True:\n",
    "#             prev_idx = torch.tensor([predicted_indexes[-1]], dtype=torch.long, device=device)\n",
    "            \n",
    "#             output, hidden, cell = model.decoder(prev_idx, hidden, cell)\n",
    "#             output = output.squeeze(0)\n",
    "            \n",
    "#             best_prediction = output.argmax(dim=1).item()\n",
    "            \n",
    "#             if best_prediction == eos_idx:\n",
    "#                 break\n",
    "            \n",
    "#             predicted_indexes.append(best_prediction)\n",
    "                       \n",
    "        \n",
    "        for _ in range(1, max_seq_len):\n",
    "            prev_idx = torch.tensor([predicted_indexes[-1]], dtype=torch.long, device=device)\n",
    "            \n",
    "            output, hidden, cell = model.decoder(prev_idx, hidden, cell)\n",
    "            output = output.squeeze(0)\n",
    "            \n",
    "            best_prediction = output.argmax(dim=1).item()\n",
    "            \n",
    "            if best_prediction == eos_idx:\n",
    "                break\n",
    "                \n",
    "            predicted_indexes.append(best_prediction)\n",
    "        \n",
    "    predicted_tokens = [orig_text_transformer.vocab.idx_to_token(idx) for idx in predicted_indexes]\n",
    "    return predicted_tokens[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8897004841154c21a014a35aee2bfc21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [1 / 50]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6905de60a8654b06863ee86a7b6eb721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch training iterations:   0%|          | 0/1569 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tIteration #0: training loss = 11.095197677612305\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-e37acd47fd24>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs_amount\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-31-676b31d05647>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, criterion, train_data, val_data, test_data, epochs_amount, max_norm, patience, current_epoch, n_prints)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\iluxa\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 245\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\iluxa\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, optimizer, criterion, train_loader, val_loader, test_loader, epochs_amount, max_norm, patience, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# del model\n",
    "# del optimizer\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = test_df.sample(10)\n",
    "test_input = test_sample.lemm_texts.to_list()\n",
    "test_target = test_sample.orig_texts.to_list()\n",
    "test_pair = list(zip(test_input, test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for input_sentence, target_sentence in test_pair:\n",
    "    model_output = evaluate(model, input_sentence, max_seq_len)\n",
    "    print(f'Input: {input_sentence}')\n",
    "    print(f'Output: {model_output}')\n",
    "    print(f'Target: {target_sentence}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
