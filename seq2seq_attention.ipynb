{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Импорт необходимых зависимостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from random import random, sample\n",
    "from typing import List\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from functools import reduce\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn import model_selection\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchtext.data.metrics import bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RANDOM_STATE = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/lenta/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orig_texts</th>\n",
       "      <th>lemm_texts</th>\n",
       "      <th>nsubj</th>\n",
       "      <th>gender</th>\n",
       "      <th>tense</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1245806</th>\n",
       "      <td>об этом сообщает риа новости со ссылкой на мат...</td>\n",
       "      <td>о это сообщать риа новость с ссылка на мать по...</td>\n",
       "      <td>риа</td>\n",
       "      <td>neut</td>\n",
       "      <td>pres</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594042</th>\n",
       "      <td>генеральный прокурор рф владимир устинов счита...</td>\n",
       "      <td>генеральный прокурор рф владимир устинов счита...</td>\n",
       "      <td>прокурор</td>\n",
       "      <td>masc</td>\n",
       "      <td>pres</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705659</th>\n",
       "      <td>телеканал «дождь» восстановил вещание, прерван...</td>\n",
       "      <td>телеканал « дождь » восстановить вещание, прер...</td>\n",
       "      <td>телеканал</td>\n",
       "      <td>masc</td>\n",
       "      <td>past</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603796</th>\n",
       "      <td>соответствующее требование прозвучало во время...</td>\n",
       "      <td>соответствующий требование прозвучать в время ...</td>\n",
       "      <td>требование</td>\n",
       "      <td>neut</td>\n",
       "      <td>past</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1430273</th>\n",
       "      <td>в пятницу вечером на сайтах \"единой россии\", \"...</td>\n",
       "      <td>в пятница вечером на сайт \"единый россия\", \"гр...</td>\n",
       "      <td>заявления</td>\n",
       "      <td>neut</td>\n",
       "      <td>past</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1331395</th>\n",
       "      <td>об этом заявил президент - председатель правле...</td>\n",
       "      <td>о это заявить президент - председатель правлен...</td>\n",
       "      <td>президент</td>\n",
       "      <td>masc</td>\n",
       "      <td>past</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1710395</th>\n",
       "      <td>\"мы думаем, что они (страны-члены совбеза) пре...</td>\n",
       "      <td>\\\" мы думать, что они (страна-член совбез) пре...</td>\n",
       "      <td>мы</td>\n",
       "      <td>undefined</td>\n",
       "      <td>pres</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1434398</th>\n",
       "      <td>социологи \"росгосстраха\" оценили сознательност...</td>\n",
       "      <td>социолог \"росгосстрах\"оценить сознательность р...</td>\n",
       "      <td>социологи</td>\n",
       "      <td>masc</td>\n",
       "      <td>past</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1832873</th>\n",
       "      <td>российская сборная сохранила за собой 24 строчку.</td>\n",
       "      <td>российский сборная сохранить за себя 24 строчка.</td>\n",
       "      <td>сборная</td>\n",
       "      <td>fem</td>\n",
       "      <td>past</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537488</th>\n",
       "      <td>эксперты компании связывают это с прекращением...</td>\n",
       "      <td>эксперт компания связывать это с прекращение и...</td>\n",
       "      <td>эксперты</td>\n",
       "      <td>masc</td>\n",
       "      <td>pres</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>926362 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                orig_texts  \\\n",
       "1245806  об этом сообщает риа новости со ссылкой на мат...   \n",
       "1594042  генеральный прокурор рф владимир устинов счита...   \n",
       "705659   телеканал «дождь» восстановил вещание, прерван...   \n",
       "603796   соответствующее требование прозвучало во время...   \n",
       "1430273  в пятницу вечером на сайтах \"единой россии\", \"...   \n",
       "...                                                    ...   \n",
       "1331395  об этом заявил президент - председатель правле...   \n",
       "1710395  \"мы думаем, что они (страны-члены совбеза) пре...   \n",
       "1434398  социологи \"росгосстраха\" оценили сознательност...   \n",
       "1832873  российская сборная сохранила за собой 24 строчку.   \n",
       "1537488  эксперты компании связывают это с прекращением...   \n",
       "\n",
       "                                                lemm_texts       nsubj  \\\n",
       "1245806  о это сообщать риа новость с ссылка на мать по...         риа   \n",
       "1594042  генеральный прокурор рф владимир устинов счита...    прокурор   \n",
       "705659   телеканал « дождь » восстановить вещание, прер...   телеканал   \n",
       "603796   соответствующий требование прозвучать в время ...  требование   \n",
       "1430273  в пятница вечером на сайт \"единый россия\", \"гр...   заявления   \n",
       "...                                                    ...         ...   \n",
       "1331395  о это заявить президент - председатель правлен...   президент   \n",
       "1710395  \\\" мы думать, что они (страна-член совбез) пре...          мы   \n",
       "1434398  социолог \"росгосстрах\"оценить сознательность р...   социологи   \n",
       "1832873   российский сборная сохранить за себя 24 строчка.     сборная   \n",
       "1537488  эксперт компания связывать это с прекращение и...    эксперты   \n",
       "\n",
       "            gender tense  \n",
       "1245806       neut  pres  \n",
       "1594042       masc  pres  \n",
       "705659        masc  past  \n",
       "603796        neut  past  \n",
       "1430273       neut  past  \n",
       "...            ...   ...  \n",
       "1331395       masc  past  \n",
       "1710395  undefined  pres  \n",
       "1434398       masc  past  \n",
       "1832873        fem  past  \n",
       "1537488       masc  pres  \n",
       "\n",
       "[926362 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(frac=0.5, random_state=RANDOM_STATE)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Определение классов словаря и трансформера текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, tokens: List[str], unk_idx: int):\n",
    "        self._tokens = tokens\n",
    "        self._token_to_idx = {token: idx for idx, token in enumerate(tqdm(tokens, 'Transforming tokens'))}\n",
    "        self._unk_idx = unk_idx\n",
    "        \n",
    "    def token_to_idx(self, token: str) -> int:\n",
    "        return self._token_to_idx.get(token, self._unk_idx)\n",
    "    \n",
    "    def idx_to_token(self, idx: int) -> str:\n",
    "        return self._tokens[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TextTransformer:\n",
    "    def __init__(self, vocab_size: int = 250000):\n",
    "        self.vocab = None\n",
    "        self.vocab_size = vocab_size\n",
    "        self.special_tokens_to_idx = {'<unk>': 0, '<pad>': 1, '<sos>': 2, '<eos>': 3}\n",
    "#         self.special_tokens_to_idx = None\n",
    "        self._tokenizer = nltk.tokenize.word_tokenize\n",
    "    \n",
    "    def tokenize(self, text, language='russian') -> List[str]:\n",
    "        return self._tokenizer(text.lower(), language)\n",
    "    \n",
    "    def save_vocab(self, path='./vocab.vcb'):\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(self.vocab, f)\n",
    "            \n",
    "    def load_vocab(self, path):\n",
    "        with open(path, 'rb') as f:\n",
    "            self.vocab = pickle.load(f)\n",
    "    \n",
    "    def build_vocab(self, tokens: List[str], unk_idx: int = 0, pad_idx: int = 1):\n",
    "#         self.special_tokens_to_idx = {'<unk>': unk_idx, '<pad>': pad_idx, '<sos>': unk_idx + 1, '<eos>': unk_idx + 2}\n",
    "#         tokens.extend(list(self.special_tokens_to_idx.keys()))\n",
    "#         self.vocab = Vocab(tokens, unk_idx)\n",
    "        tokens_ = [special_token for special_token in self.special_tokens_to_idx.keys()]\n",
    "        special_tokens_amount = len(self.special_tokens_to_idx)\n",
    "        \n",
    "        for token, _ in Counter(tokens).most_common(self.vocab_size - special_tokens_amount):\n",
    "            tokens_.append(token)\n",
    "        \n",
    "        unk_idx = self.special_tokens_to_idx.get('<unk>')\n",
    "        self.vocab = Vocab(tokens_, unk_idx)\n",
    "        \n",
    "    def transform_text(self, text: str) -> List[int]:\n",
    "        tokenized_text = self.tokenize(text)\n",
    "        transformed = [self.vocab.token_to_idx(token) for token in tokenized_text]\n",
    "        return transformed\n",
    "    \n",
    "    def fit(self, texts: List[str]) -> None:\n",
    "        transformed_texts = []\n",
    "        \n",
    "        tokenized_texts = [self.tokenize(text) for text in tqdm(texts, 'Tokenizing texts')]\n",
    "        tokens = chain(*tokenized_texts)\n",
    "        self.build_vocab(tokens)\n",
    "        \n",
    "#         for tokenized_text in tqdm(tokenized_texts, 'Transforming texts'):\n",
    "#             transformed = [self.vocab.token_to_idx(token) for token in tokenized_text]\n",
    "#             transformed_texts.append(transformed)\n",
    "    \n",
    "    def transform_texts(self, texts: List[str]) -> List[List[int]]:\n",
    "        transformed_texts = [transform_text(text) for text in tqdm(texts, 'Transforming texts')]\n",
    "        return transformed_texts\n",
    "    \n",
    "    def text_to_tensor(self, text: str, max_seq_len) -> torch.tensor:\n",
    "        transformed_text = self.transform_text(text)\n",
    "        pad_idx = self.special_tokens_to_idx.get('<pad>')\n",
    "        sos_idx = self.special_tokens_to_idx.get('<sos>')\n",
    "        eos_idx = self.special_tokens_to_idx.get('<eos>')\n",
    "        \n",
    "        pad_size = 0\n",
    "        if len(transformed_text) >= max_seq_len:\n",
    "            transformed_text = transformed_text[:max_seq_len]\n",
    "        else:\n",
    "            pad_size = max_seq_len - len(transformed_text)\n",
    "            transformed_text.extend([pad_idx] * pad_size)   \n",
    "        transformed_text.insert(0, sos_idx)\n",
    "        transformed_text.insert(len(transformed_text) - pad_size, eos_idx)\n",
    "        \n",
    "        tensor = torch.tensor(transformed_text, dtype=torch.long)\n",
    "        return tensor.unsqueeze(0)\n",
    "    \n",
    "    def texts_to_tensor(self, texts: List[str], max_seq_len) -> torch.tensor:\n",
    "        pad_idx = self.special_tokens_to_idx.get('<pad>')\n",
    "        sos_idx = self.special_tokens_to_idx.get('<sos>')\n",
    "        eos_idx = self.special_tokens_to_idx.get('<eos>')\n",
    "        transformed_texts = []\n",
    "        \n",
    "        for text in tqdm(texts, 'Building tensor'):\n",
    "            transformed_text = self.transform_text(text)\n",
    "            pad_size = 0\n",
    "            if len(transformed_text) >= max_seq_len:\n",
    "                transformed_text = transformed_text[:max_seq_len]\n",
    "            else:\n",
    "                pad_size = max_seq_len - len(transformed_text)\n",
    "                transformed_text.extend([pad_idx] * pad_size)   \n",
    "            transformed_text.insert(0, sos_idx)\n",
    "            transformed_text.insert(len(transformed_text) - pad_size, eos_idx)\n",
    "            transformed_texts.append(transformed_text)\n",
    "        \n",
    "        tensor = torch.tensor(transformed_texts, dtype=torch.long).permute(1, 0)\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разбиение данных на обучающую, тестовую и валидационную выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df, test_df = model_selection.train_test_split(df, test_size=0.1, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df, val_df = model_selection.train_test_split(test_df, test_size=0.5, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Токенизация текстов и индексация токенов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab_size = 125000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_seq_len = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_transformer = TextTransformer(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_transformer.load_vocab('./data/cached/vocab.vcb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_transformer.build_vocab(embedding.vocab.words[0:-2], embedding.vocab.unk_id, embedding.vocab.pad_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemm_vocab_size = 23000\n",
    "# orig_vocab_size = 65000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemm_text_transformer = TextTransformer(lemm_vocab_size)\n",
    "# orig_text_transformer = TextTransformer(orig_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-04T14:44:46.923175Z",
     "iopub.status.busy": "2021-06-04T14:44:46.922842Z",
     "iopub.status.idle": "2021-06-04T14:52:18.097778Z",
     "shell.execute_reply": "2021-06-04T14:52:18.096702Z",
     "shell.execute_reply.started": "2021-06-04T14:44:46.923142Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d278b0d335d14ff481f75ddcb71c8d7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing texts:   0%|          | 0/1667450 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f600fa7a34c44a6388598b22e9f25a99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transforming tokens:   0%|          | 0/125000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# text_transformer.fit(train_df.orig_texts.to_list() + train_df.lemm_texts.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# with open('./data/cached/tokens.list', 'rb') as f:\n",
    "#     tokens = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80d212d8688b4f19afd3d6be5df89471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transforming tokens:   0%|          | 0/124999 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# text_transformer.build_vocab(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-04T15:10:50.166507Z",
     "iopub.status.busy": "2021-06-04T15:10:50.166068Z",
     "iopub.status.idle": "2021-06-04T15:10:50.245493Z",
     "shell.execute_reply": "2021-06-04T15:10:50.244508Z",
     "shell.execute_reply.started": "2021-06-04T15:10:50.166468Z"
    }
   },
   "outputs": [],
   "source": [
    "# tokens = [text_transformer.vocab.idx_to_token(idx) for idx in range(4, 124999)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-04T15:10:51.731501Z",
     "iopub.status.busy": "2021-06-04T15:10:51.731134Z",
     "iopub.status.idle": "2021-06-04T15:10:51.785580Z",
     "shell.execute_reply": "2021-06-04T15:10:51.784699Z",
     "shell.execute_reply.started": "2021-06-04T15:10:51.731470Z"
    }
   },
   "outputs": [],
   "source": [
    "# with open('./tokens.list', 'wb') as f:\n",
    "#     pickle.dump(tokens, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# orig_text_transformer.fit(train_df.orig_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Перевод данных в тензоры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-04T15:08:01.290951Z",
     "iopub.status.busy": "2021-06-04T15:08:01.290629Z",
     "iopub.status.idle": "2021-06-04T15:08:02.174897Z",
     "shell.execute_reply": "2021-06-04T15:08:02.174048Z",
     "shell.execute_reply.started": "2021-06-04T15:08:01.290921Z"
    }
   },
   "outputs": [],
   "source": [
    "# tensors = {\n",
    "#     'train_lemm_tensor': train_lemm_tensor,\n",
    "#     'test_lemm_tensor': test_lemm_tensor,\n",
    "#     'val_lemm_tensor': val_lemm_tensor,\n",
    "#     'train_orig_tensor': train_orig_tensor,\n",
    "#     'test_orig_tensor': test_orig_tensor,\n",
    "#     'val_orig_tensor': val_orig_tensor\n",
    "# }\n",
    "\n",
    "# with open('./data_tensors.data', 'wb') as f:\n",
    "#     pickle.dump(tensors, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('./data/cached/data_tensors.data', 'rb') as f:\n",
    "    tensors = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_lemm_tensor, test_lemm_tensor, val_lemm_tensor,\\\n",
    "train_orig_tensor, test_orig_tensor, val_orig_tensor = tensors.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ebaaa394aa74a96a8841d2c1ce3f331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building tensor:   0%|          | 0/833725 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-2dcd9a7819b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_lemm_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext_transformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtexts_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemm_texts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtest_lemm_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext_transformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtexts_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemm_texts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mval_lemm_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext_transformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtexts_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemm_texts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-8ddaee079c3e>\u001b[0m in \u001b[0;36mtexts_to_tensor\u001b[1;34m(self, texts, max_seq_len)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Building tensor'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m             \u001b[0mtransformed_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m             \u001b[0mpad_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformed_text\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-8ddaee079c3e>\u001b[0m in \u001b[0;36mtransform_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtransform_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mtokenized_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[0mtransformed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoken_to_idx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtransformed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-8ddaee079c3e>\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(self, text, language)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'russian'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msave_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'./vocab.vcb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\iluxa\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    128\u001b[0m     \"\"\"\n\u001b[0;32m    129\u001b[0m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m     return [\n\u001b[0m\u001b[0;32m    131\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m     ]\n",
      "\u001b[1;32mc:\\users\\iluxa\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     return [\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m     ]\n",
      "\u001b[1;32mc:\\users\\iluxa\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\nltk\\tokenize\\destructive.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mregexp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCONTRACTIONS2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m             \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\" \\1 \\2 \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mregexp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCONTRACTIONS3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m             \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\" \\1 \\2 \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train_lemm_tensor = text_transformer.texts_to_tensor(train_df.lemm_texts.to_list(), max_seq_len)\n",
    "# test_lemm_tensor = text_transformer.texts_to_tensor(test_df.lemm_texts.to_list(), max_seq_len)\n",
    "# val_lemm_tensor = text_transformer.texts_to_tensor(val_df.lemm_texts.to_list(), max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-04T14:56:33.178350Z",
     "iopub.status.busy": "2021-06-04T14:56:33.177972Z",
     "iopub.status.idle": "2021-06-04T15:00:54.140536Z",
     "shell.execute_reply": "2021-06-04T15:00:54.139509Z",
     "shell.execute_reply.started": "2021-06-04T14:56:33.178310Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60acdf48b3054c328a970115c99ded2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building tensor:   0%|          | 0/833725 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46a4aabffe744bb59952ab91f8442765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building tensor:   0%|          | 0/46318 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddb6c45ad7c44b43852aaaa98fba5ca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building tensor:   0%|          | 0/46319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train_orig_tensor = text_transformer.texts_to_tensor(train_df.orig_texts.to_list(), max_seq_len)\n",
    "# test_orig_tensor = text_transformer.texts_to_tensor(test_df.orig_texts.to_list(), max_seq_len)\n",
    "# val_orig_tensor = text_transformer.texts_to_tensor(val_df.orig_texts.to_list(), max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gender_to_vec = {\n",
    "    'masc': [1, 0, 0, 0],\n",
    "    'fem': [0, 1, 0, 0],\n",
    "    'neut': [0, 0, 1, 0],\n",
    "    'undefined': [0, 0, 0, 1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tense_to_vec = {\n",
    "    'pres': [1, 0, 0],\n",
    "    'past': [0, 1, 0],\n",
    "    'fut': [0, 0, 1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transform_context(df, df_type: str):\n",
    "    transformed_gender = [gender_to_vec.get(gender) for gender in tqdm(df.gender, f'Transforming gender ({df_type})')]\n",
    "    transformed_tense = [tense_to_vec.get(tense) for tense in tqdm(df.tense, f'Transforming tense ({df_type})')]\n",
    "    transformed_nsubj = [text_transformer.vocab.token_to_idx(nsubj) for nsubj in tqdm(df.nsubj, f'Transforming nsubj ({df_type})')]\n",
    "    \n",
    "    context = [transformed_nsubj, transformed_gender, transformed_tense]\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def context_to_tensors(context):\n",
    "    nsubj, gender, tense = context\n",
    "    \n",
    "    nsubj_tensor = torch.tensor(nsubj)\n",
    "    gender_tensor = torch.tensor(gender, dtype=torch.float32)\n",
    "    tense_tensor = torch.tensor(tense, dtype=torch.float32)\n",
    "    \n",
    "    context_tensors = [nsubj_tensor, gender_tensor, tense_tensor]\n",
    "    return context_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "484b69e5a8714600b5b27f4f5d5572d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transforming gender (train):   0%|          | 0/833725 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d350c52dd01400eae07e5a1676b2dd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transforming tense (train):   0%|          | 0/833725 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82d1d1344c414a388362ecf8a4990fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transforming nsubj (train):   0%|          | 0/833725 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdd73a004fea44e38f807dee99665782",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transforming gender (test):   0%|          | 0/46318 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82f3931ba4cc4b12a68f4e408b24e045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transforming tense (test):   0%|          | 0/46318 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0583c5edab254a0993f6136c9cbdbdec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transforming nsubj (test):   0%|          | 0/46318 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cbb99219a1143a1ad2b09b107c0a65d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transforming gender (validation):   0%|          | 0/46319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e982672f3e8416a8e6b9632a122b69d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transforming tense (validation):   0%|          | 0/46319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e8dd83ee84a4829ba15189072b32135",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transforming nsubj (validation):   0%|          | 0/46319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_context = transform_context(train_df, 'train')\n",
    "test_context = transform_context(test_df, 'test')\n",
    "val_context = transform_context(val_df, 'validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_context_tensors = context_to_tensors(train_context)\n",
    "test_context_tensors = context_to_tensors(test_context)\n",
    "val_context_tensors = context_to_tensors(val_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cut_to_fit_batch(tensor: torch.Tensor, batch_size: int):\n",
    "    n_samples = tensor.shape[1]\n",
    "    new_n_samples = (n_samples // batch_size) * batch_size\n",
    "    result = tensor.split(new_n_samples, dim=1)[0]\n",
    "    return torch.transpose(result, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Построение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ContextMem(nn.Module):\n",
    "    def __init__(self, gender_input_size, tense_input_size, hidden_size, output_size, nsubj_embedding_size, device):\n",
    "        super(ContextMem, self).__init__()\n",
    "        \n",
    "        self.device = device\n",
    "\n",
    "        self.gender_proj = nn.Linear(gender_input_size, hidden_size, bias=False)\n",
    "        self.tense_proj = nn.Linear(tense_input_size, hidden_size, bias=False)\n",
    "        self.fc_out = nn.Linear(hidden_size * 2 + nsubj_embedding_size, output_size, bias=False)\n",
    "        \n",
    "    def forward(self, nsubj_embedding, gender, tense):\n",
    "        # nsubj_embedding_shape: (batch_size, embedding_size)\n",
    "        # gender_shape: (batch_size, input_size)\n",
    "        # tense_shape: (batch_size, input_size)\n",
    "        \n",
    "        gender = self.gender_proj(gender)\n",
    "        # gender_shape: (batch_size, hidden_size)\n",
    "        \n",
    "        tense = self.tense_proj(tense)\n",
    "        # tense_shape: (batch_size, hidden_size)    \n",
    "        \n",
    "        context = torch.cat([nsubj_embedding, gender, tense], dim=1)\n",
    "        # context_shape: (batch_size, hidden_size * 2 + embedding_size) \n",
    "        \n",
    "        context = self.fc_out(context)\n",
    "        # context_shape: (batch_size, output_size)\n",
    "        \n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_size: int, hidden_size: int, pad_idx: int,\n",
    "                 device, num_layers, dropout_p: float, embedding=None, pretrained_embedding_loaded=False):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = embedding\n",
    "        self.pretrained_embedding_loaded = pretrained_embedding_loaded\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=0.0, bidirectional=True)\n",
    "        self.fc_compressor_hidden = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.fc_compressor_cell = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        \n",
    "    def forward(self, x, hidden, cell):\n",
    "        # x_shape: (seq_len, batch_size)\n",
    "        if self.pretrained_embedding_loaded:\n",
    "            with torch.no_grad():\n",
    "                embedding = self.embedding(x)\n",
    "        else:\n",
    "            embedding = self.embedding(x)\n",
    "        # embedding_shape: (seq_len, batch_size, embedding_size)\n",
    "        encoder_states, (hidden, cell) = self.lstm(embedding, (hidden, cell))\n",
    "        # encoder_states: (seq_len, batch_size, hidden_size * 2)\n",
    "        # hidden_shape: (num_layers=1 * 2, batch_size, hidden_size)\n",
    "        # cell_shape: (num_layers=1 * 2, batch_size, hidden_size)\n",
    "        \n",
    "        bi_hidden = torch.cat((hidden[0], hidden[1]), dim=1).unsqueeze(0).permute(1, 0, 2)\n",
    "        bi_cell = torch.cat((cell[0], cell[1]), dim=1).unsqueeze(0).permute(1, 0, 2)\n",
    "        # bi_hidden, bi_cell shapes: (batch_size, 1, hidden_size * 2)\n",
    "        \n",
    "        hidden_compressed = self.fc_compressor_hidden(bi_hidden).permute(1, 0, 2)\n",
    "        cell_compressed = self.fc_compressor_hidden(bi_cell).permute(1, 0, 2)\n",
    "        # hidden_compressed, cell_compressed shapes: (1, batch_size, hidden_size)\n",
    "        \n",
    "        return encoder_states, hidden_compressed, cell_compressed\n",
    "    \n",
    "    def init_hidden_state(self, batch_size: int):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(self.device)\n",
    "        cell = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(self.device)\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_size: int, hidden_size: int, output_size: int, pad_idx: int,\n",
    "                 device, num_layers, dropout_p: float, embedding=None, pretrained_embedding_loaded=False):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = embedding\n",
    "        self.pretrained_embedding_loaded = pretrained_embedding_loaded\n",
    "        \n",
    "        self.attn_weights = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 3, hidden_size, bias=False),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 1, bias=False),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        self.lstm = nn.LSTM(embedding_size + 2 * hidden_size, hidden_size, num_layers, dropout=0.0)\n",
    "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x, encoder_states, hidden, cell):\n",
    "        x = x.unsqueeze(0)\n",
    "        # x_shape: (seq_len=1, batch_size)\n",
    "        # hidden_shape: (1, batch_size, hidden_size)\n",
    "        # cell_shape: (1, batch_size, hidden_size)\n",
    "        encoder_states = torch.transpose(encoder_states, 1, 0)\n",
    "        # encoder_states_shape: (batch_size, seq_len, hidden_size * 2)\n",
    "        if self.pretrained_embedding_loaded:\n",
    "            with torch.no_grad():\n",
    "                embedding = self.embedding(x)\n",
    "        else:\n",
    "            embedding = self.embedding(x)\n",
    "        # embedding_shape: (seq_len=1, batch_size, embedding_size)\n",
    "        \n",
    "        seq_len = encoder_states.shape[1]\n",
    "        hidden_repeated = hidden.repeat(seq_len, 1, 1).permute(1, 0, 2)\n",
    "        # hidden_repeated_shape: (batch_size, seq_len, hidden_size)\n",
    "        \n",
    "        attn_weights = self.attn_weights(torch.cat((hidden_repeated, encoder_states), dim=2))\n",
    "        # attn_weights_shape: (batch_size, seq_len, 1)\n",
    "        \n",
    "        context_vec = torch.bmm(attn_weights.permute(0, 2, 1), encoder_states).permute(1, 0, 2)\n",
    "        # context_vec_shape: (1, batch_size, hidden_size * 2)\n",
    "        \n",
    "        combined = torch.cat((embedding, context_vec), dim=2)\n",
    "        # combined_shape: (1, batch_size, embedding_size + 2 * hidden_size)\n",
    "        \n",
    "        lstm_out, (hidden, cell) = self.lstm(combined, (hidden, cell))\n",
    "        # lstm_out_shape: (seq_len=1, batch_size, hidden_size)\n",
    "        fc_out = self.fc_out(lstm_out)\n",
    "        # fc_out_shape: (seq_len=1, batch_size, output_size)\n",
    "        \n",
    "        return fc_out, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Seq2SeqModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_size, embedding_size, hidden_size, output_size,\n",
    "                 gender_input_size, tense_input_size, context_hidden_size, context_output_size,\n",
    "                 pad_idx, device, num_layers, dropout_p, pretrained_embedding=None):\n",
    "        super(Seq2SeqModel, self).__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        if pretrained_embedding is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(pretrained_embedding, padding_idx=pad_idx)\n",
    "            self.pretrained_embedding_loaded = True\n",
    "        else:\n",
    "            self.embedding = nn.Sequential(\n",
    "                nn.Embedding(vocab_size, embedding_size, padding_idx=pad_idx),\n",
    "                nn.Dropout(dropout_p)\n",
    "            )\n",
    "            self.pretrained_embedding_loaded = False\n",
    "        \n",
    "        self.context_mem = ContextMem(gender_input_size, tense_input_size, context_hidden_size, context_output_size, embedding_size, device).to(device)\n",
    "        self.encoder = EncoderRNN(vocab_size, embedding_size, hidden_size,\n",
    "                                  pad_idx, device, num_layers, dropout_p,\n",
    "                                  self.embedding, self.pretrained_embedding_loaded).to(device)\n",
    "        self.decoder = DecoderRNN(vocab_size, embedding_size, hidden_size, output_size,\n",
    "                                  pad_idx, device, num_layers, dropout_p,\n",
    "                                  self.embedding, self.pretrained_embedding_loaded).to(device)\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "    def forward(self, input, target, context, teacher_forcing_ratio=0.5):\n",
    "        batch_size = input.shape[1]\n",
    "        target_len = target.shape[0]\n",
    "        target_vocab_size = self.vocab_size\n",
    "        \n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size, device=self.device)\n",
    "        \n",
    "        nsubj, gender, tense = context\n",
    "        # nsubj_shape:  (batch_size)\n",
    "        # gender_shape: (batch_size, gender_input_size)\n",
    "        # tense_shape:  (batch_size, tense_input_size)\n",
    "        if self.pretrained_embedding_loaded:\n",
    "            with torch.no_grad():\n",
    "                nsubj_embedding = self.embedding(nsubj).squeeze(0)\n",
    "        else:\n",
    "            nsubj_embedding = self.embedding(nsubj).squeeze(0)\n",
    "            # nsubj_embedding_shape: (batch_size, embedding_size)\n",
    "        \n",
    "        hidden = self.context_mem(nsubj_embedding, gender, tense)\n",
    "        cell = hidden.clone()\n",
    "        # hidden, cell shapes: (batch_size, context_output_size=hidden_size)\n",
    "        \n",
    "        hidden = torch.cat([hidden.unsqueeze(0)] * 2, 0)\n",
    "        cell = torch.cat([cell.unsqueeze(0)] * 2, 0)\n",
    "        # hidden, cell shapes: (2, batch_size, context_output_size=hidden_size)\n",
    "        \n",
    "        encoder_states, hidden, cell = self.encoder(input, hidden, cell)\n",
    "        # hidden, cell shapes: (2, batch_size, hidden_size)\n",
    "        # encoder_states_shape: (seq_len, batch_size, hidden_size * 2)\n",
    "        \n",
    "        prev_token_idx = target[0]\n",
    "        # prev_token_shape: (batch_size)\n",
    "        \n",
    "        for t in range(1, target_len):\n",
    "            output, hidden, cell = self.decoder(prev_token_idx, encoder_states, hidden, cell)\n",
    "            # output_shape: (1, batch_size, output_size)\n",
    "            outputs[t] = output.squeeze(0)\n",
    "            \n",
    "            best_prediction = outputs[t].argmax(dim=1)\n",
    "            # best_prediction_shape: (batch_size)\n",
    "            prev_token_idx = target[t] if random() < teacher_forcing_ratio else best_prediction\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция сохранения текущего состояния модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, epoch, path):\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'criterion': criterion,\n",
    "        'epoch': epoch\n",
    "    }\n",
    "    \n",
    "    torch.save(checkpoint, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция загрузки уже тренировавшейся модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_model(model, optimizer, criterion, path, for_inference=True, device=torch.device('cpu')):\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    if not for_inference:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        epoch = checkpoint['epoch']\n",
    "        criterion = checkpoint['criterion']\n",
    "\n",
    "        return epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Инициализация гиперпараметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "epochs_amount = 50\n",
    "hidden_size = 1024\n",
    "embedding_size = 300\n",
    "num_layers = 1\n",
    "max_norm = 1.0\n",
    "dropout_p = 0.5\n",
    "gender_input_size = 4\n",
    "tense_input_size = 3\n",
    "context_hidden_size = hidden_size // 2\n",
    "context_output_size = hidden_size\n",
    "patience = 4\n",
    "output_size = vocab_size\n",
    "pad_idx = text_transformer.special_tokens_to_idx.get('<pad>')\n",
    "model_path = './models/'\n",
    "model_name = 'seq2seq_attention.model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-04T15:21:48.765789Z",
     "iopub.status.busy": "2021-06-04T15:21:48.765426Z",
     "iopub.status.idle": "2021-06-04T15:21:48.813281Z",
     "shell.execute_reply": "2021-06-04T15:21:48.812375Z",
     "shell.execute_reply.started": "2021-06-04T15:21:48.765758Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Seq2SeqModel(vocab_size, embedding_size, hidden_size, output_size,\n",
    "                     gender_input_size, tense_input_size, context_hidden_size, context_output_size, \n",
    "                     pad_idx, device, num_layers, dropout_p).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from ./models/\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    epoch = load_model(model, optimizer, criterion, model_path + model_name, for_inference=True)\n",
    "    print(f'Loaded model from {model_path}')\n",
    "except:\n",
    "    print(f'No models found at {model_path}')\n",
    "    epoch = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Урезание данных для соответствия размеру батча"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_lemm_tensor_f = cut_to_fit_batch(train_lemm_tensor, batch_size)\n",
    "train_orig_tensor_f = cut_to_fit_batch(train_orig_tensor, batch_size)\n",
    "\n",
    "test_lemm_tensor_f = cut_to_fit_batch(test_lemm_tensor, batch_size)\n",
    "test_orig_tensor_f = cut_to_fit_batch(test_orig_tensor, batch_size)\n",
    "\n",
    "val_lemm_tensor_f = cut_to_fit_batch(val_lemm_tensor, batch_size)\n",
    "val_orig_tensor_f = cut_to_fit_batch(val_orig_tensor, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_context_tensors_f = [cut_to_fit_batch(tensor.unsqueeze(0), batch_size).squeeze(1) for tensor in train_context_tensors]\n",
    "test_context_tensors_f = [cut_to_fit_batch(tensor.unsqueeze(0), batch_size).squeeze(1) for tensor in test_context_tensors]\n",
    "val_context_tensors_f = [cut_to_fit_batch(tensor.unsqueeze(0), batch_size).squeeze(1) for tensor in val_context_tensors]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Инициализация данных итерируемых по батчам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(train_lemm_tensor_f, train_orig_tensor_f, *train_context_tensors_f)\n",
    "test_dataset = TensorDataset(test_lemm_tensor_f, test_orig_tensor_f, *test_context_tensors_f)\n",
    "val_dataset = TensorDataset(val_lemm_tensor_f, val_orig_tensor_f, *val_context_tensors_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Определение функции проверки работы сети между эпохами обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_evaluate(model, input, context, target_len=45):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        \n",
    "        input = input.to(device)\n",
    "\n",
    "        nsubj, gender, tense = context\n",
    "        nsubj_embedding = model.decoder.embedding(nsubj)\n",
    "\n",
    "        hidden = model.context_mem(nsubj_embedding, gender, tense)\n",
    "        cell = hidden.clone()\n",
    "\n",
    "        if model.num_layers == 1:\n",
    "            hidden.unsqueeze_(0)\n",
    "            cell.unsqueeze_(0)\n",
    "            # hidden, cell shapes: (1, batch_size, context_output_size=hidden_size)\n",
    "        else:\n",
    "            hidden = torch.cat([hidden.unsqueeze(0)] * model.num_layers, 0)\n",
    "            cell = torch.cat([cell.unsqueeze(0)] * model.num_layers, 0)\n",
    "            # hidden, cell shapes: (num_layers, batch_size, context_output_size=hidden_size)\n",
    "\n",
    "        sos_idx = text_transformer.special_tokens_to_idx.get('<sos>')\n",
    "        eos_idx = text_transformer.special_tokens_to_idx.get('<eos>')\n",
    "    \n",
    "        encoder_states, hidden, cell = model.encoder(input, hidden, cell)\n",
    "        \n",
    "        predicted_indexes = [sos_idx]\n",
    "        \n",
    "        for _ in range(1, target_len):\n",
    "            prev_idx = torch.tensor([predicted_indexes[-1]], dtype=torch.long, device=device)\n",
    "            \n",
    "            output, hidden, cell = model.decoder(prev_idx, encoder_states, hidden, cell)\n",
    "            output = output.squeeze(0)\n",
    "            \n",
    "            best_prediction = output.argmax(dim=1).item()\n",
    "            \n",
    "            if best_prediction == eos_idx:\n",
    "                break\n",
    "                \n",
    "            predicted_indexes.append(best_prediction)\n",
    "                        \n",
    "        \n",
    "    predicted_tokens = [text_transformer.vocab.idx_to_token(idx) for idx in predicted_indexes]\n",
    "    return predicted_tokens[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Определение функции обучения сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, train_data, val_data, test_data, epochs_amount, max_norm, patience=3, current_epoch=1, n_prints=5):\n",
    "    min_mean_val_loss = float('+inf')\n",
    "    initial_patiece = patience\n",
    "    print_every = len(train_data) // n_prints\n",
    "    \n",
    "    for epoch in tqdm(range(current_epoch, epochs_amount + 1), 'Epochs'):\n",
    "        print(f'\\nEpoch [{epoch} / {epochs_amount}]')\n",
    "        \n",
    "        model.train()\n",
    "        for iteration, (input, target, nsubj, gender, tense) in enumerate(tqdm(train_data, 'Epoch training iterations')):\n",
    "            optimizer.zero_grad()\n",
    "            # input = lemm_texts, target = orig_texts\n",
    "            \n",
    "            input = torch.transpose(input, 1, 0).to(device)\n",
    "            # input_shape: (seq_len, batch_size)\n",
    "            \n",
    "            target = torch.transpose(target, 1, 0).to(device)\n",
    "            # target_shape: (seq_len, batch_size)\n",
    "            \n",
    "            context = (nsubj.to(device), gender.to(device), tense.to(device))\n",
    "            \n",
    "            output = model(input, target, context)\n",
    "            # output_shape: (seq_len, batch_size, vocab_size) but need (N, vocab_size)\n",
    "            \n",
    "            target = target[1:].reshape(-1)\n",
    "            # now target_shape is (seq_len * batch_size)\n",
    "            \n",
    "            vocab_size = output.shape[2]\n",
    "            \n",
    "            output = output[1:].reshape(-1, vocab_size)\n",
    "            # now output_shape is (seq_len * batch_size, vocab_size)\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            if iteration % print_every == 0:\n",
    "                print(f'\\tIteration #{iteration}: training loss = {loss.item()}')\n",
    "            elif iteration == len(train_data):\n",
    "                print(f'\\tIteration #{iteration}: training loss = {loss.item()}')\n",
    "            \n",
    "            \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            val_loss = []\n",
    "            \n",
    "            for input, target, nsubj, gender, tense in tqdm(val_data, 'Epoch validating iterations'):\n",
    "                input = torch.transpose(input, 1, 0).to(device)\n",
    "                target = torch.transpose(target, 1, 0).to(device)\n",
    "                context = (nsubj.to(device), gender.to(device), tense.to(device))\n",
    "                \n",
    "                output = model(input, target, context)\n",
    "                vocab_size = output.shape[2]\n",
    "                output = output[1:].reshape(-1, orig_vocab_size)\n",
    "                target = target[1:].reshape(-1)\n",
    "                \n",
    "                val_loss.append(criterion(output, target).item())\n",
    "            \n",
    "            mean_val_loss = sum(val_loss) / len(val_loss)\n",
    "            print(f'\\tValidation loss = {mean_val_loss}')\n",
    "            if mean_val_loss < min_mean_val_loss:\n",
    "                try:\n",
    "                    save_model(model, optimizer, epoch, model_path + model_name)\n",
    "                    min_mean_val_loss = mean_val_loss\n",
    "                    patience = initial_patiece\n",
    "                except Exception as exc:\n",
    "                    print(exc)\n",
    "            else:\n",
    "                patience -= 1\n",
    "            \n",
    "            test_data = DataLoader(test_data.dataset, batch_size=1, shuffle=True)\n",
    "            for input, target, nsubj, gender, tense in test_data:\n",
    "                target = target.squeeze(0).to(device)\n",
    "                context = (nsubj.to(device), gender.to(device), tense.to(device))\n",
    "                \n",
    "                input = torch.transpose(input, 1, 0)\n",
    "                target_len = target.shape[0]\n",
    "                \n",
    "                output = test_evaluate(model, input, context, target_len)\n",
    "                decoded_input = [text_transformer.vocab.idx_to_token(idx.item()) for idx in input]\n",
    "                decoded_target = [text_transformer.vocab.idx_to_token(idx.item()) for idx in target]\n",
    "                \n",
    "                print(f'\\tInput: {decoded_input}')\n",
    "                print(f'\\tOutput: {output}')\n",
    "                print(f'\\tTarget: {decoded_target}')\n",
    "                break\n",
    "        \n",
    "        if patience == 0:\n",
    "            print(f'\\nModel learning finished due to early stopping')\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Определение функции эксплуатации обученной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(model: Seq2SeqModel, sentence: str, context, max_seq_len=45):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        \n",
    "        nsubj, gender, tense = context\n",
    "        \n",
    "        nsubj = torch.tensor([text_transformer.vocab.token_to_idx(nsubj)], device=device).unsqueeze(0)\n",
    "        gender = torch.tensor([gender_to_vec[gender]], dtype=torch.float32, device=device)\n",
    "        tense = torch.tensor([tense_to_vec[tense]], dtype=torch.float32, device=device)\n",
    "        \n",
    "        nsubj_embedding = model.decoder.embedding(nsubj).squeeze(0)\n",
    "\n",
    "        hidden = model.context_mem(nsubj_embedding, gender, tense)\n",
    "        cell = hidden.clone()\n",
    "\n",
    "        hidden = torch.cat([hidden.unsqueeze(0)] * 2, 0)\n",
    "        cell = torch.cat([cell.unsqueeze(0)] * 2, 0)\n",
    "        # hidden, cell shapes: (2, batch_size, context_output_size=hidden_size)\n",
    "        \n",
    "        input_tensor = text_transformer.text_to_tensor(sentence, max_seq_len).to(device)\n",
    "        input_tensor = torch.transpose(input_tensor, 1, 0)\n",
    "        sos_idx = text_transformer.special_tokens_to_idx.get('<sos>')\n",
    "        eos_idx = text_transformer.special_tokens_to_idx.get('<eos>')\n",
    "    \n",
    "    \n",
    "        encoder_states, hidden, cell = model.encoder(input_tensor, hidden, cell)\n",
    "        \n",
    "        predicted_indexes = [sos_idx]\n",
    "        \n",
    "#         while True:\n",
    "#             prev_idx = torch.tensor([predicted_indexes[-1]], dtype=torch.long, device=device)\n",
    "            \n",
    "#             output, hidden, cell = model.decoder(prev_idx, hidden, cell)\n",
    "#             output = output.squeeze(0)\n",
    "            \n",
    "#             best_prediction = output.argmax(dim=1).item()\n",
    "            \n",
    "#             if best_prediction == eos_idx:\n",
    "#                 break\n",
    "            \n",
    "#             predicted_indexes.append(best_prediction)\n",
    "                       \n",
    "        \n",
    "        for _ in range(1, max_seq_len):\n",
    "            prev_idx = torch.tensor([predicted_indexes[-1]], dtype=torch.long, device=device)\n",
    "            \n",
    "            output, hidden, cell = model.decoder(prev_idx, encoder_states, hidden, cell)\n",
    "            output = output.squeeze(0)\n",
    "            \n",
    "            best_prediction = output.argmax(dim=1).item()\n",
    "            \n",
    "            if best_prediction == eos_idx:\n",
    "                break\n",
    "                \n",
    "            predicted_indexes.append(best_prediction)\n",
    "        \n",
    "    predicted_tokens = [text_transformer.vocab.idx_to_token(idx) for idx in predicted_indexes]\n",
    "    return predicted_tokens[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-04T15:22:02.720799Z",
     "iopub.status.busy": "2021-06-04T15:22:02.720428Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2f4581c4e0e4ec0b732bbb1307fdf82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [1 / 50]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80b9d895eacc41c5abc8b4916fae4cdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch training iterations:   0%|          | 0/13026 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tIteration #0: training loss = 11.735000610351562\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, criterion, train_loader, val_loader, test_loader, epochs_amount, max_norm, patience, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-04T15:07:34.103145Z",
     "iopub.status.busy": "2021-06-04T15:07:34.102825Z",
     "iopub.status.idle": "2021-06-04T15:07:34.722069Z",
     "shell.execute_reply": "2021-06-04T15:07:34.721236Z",
     "shell.execute_reply.started": "2021-06-04T15:07:34.103115Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import gc\n",
    "# del model\n",
    "# del optimizer\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = test_df.sample(10)\n",
    "test_lemm_sents = test_sample.lemm_texts.to_list()\n",
    "test_target_sents = test_sample.orig_texts.to_list()\n",
    "test_nsubj = test_sample.nsubj.to_list()\n",
    "test_gender = test_sample.gender.to_list()\n",
    "test_tense = test_sample.tense.to_list()\n",
    "test_input = zip(test_lemm_sents, test_target_sents, test_nsubj, test_gender, test_tense) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: американский студент, арестованный в прошлый год в лондон по подозрение в предоставление боевик \"аль-каеда\"оборудование для нападение на американский солдат, стать первый подозреваемый в террористический деятельность, выданный сша британский власть.\n",
      "Output: ['американский', 'студент', ',', 'арестованный', 'в', 'прошлом', 'году', 'в', 'лондоне', 'по', 'подозрению', 'в', 'предоставлении', 'боевик', '``', 'аль-каеды', \"''\", 'оборудования', 'для', 'нападений', 'на', 'американский', 'солдат', ',', 'стал', 'первым', 'подозреваемыми', 'в', 'террористической', 'деятельности', ',', 'выданных', 'сша', 'британских', 'властями', '.']\n",
      "Target: американский студент, арестованный в прошлом году в лондоне по подозрению в предоставлении боевикам \"аль-каеды\" оборудования для нападения на американских солдат, стал первым подозреваемым в террористической деятельности, выданным сша британскими властями.\n",
      "Nsubj: студент\n",
      "Gender: masc\n",
      "Tense: past\n",
      "\n",
      "\n",
      "Input: сборная италия обыграть испанец в 1/8 финал чемпионат европа по футбол.\n",
      "Output: ['сборная', 'италии', 'обыграла', 'испанцев', 'в', '1/8', 'финала', 'чемпионата', 'европы', 'по', 'футболу', '.']\n",
      "Target: сборная италии обыграла испанцев в 1/8 финала чемпионата европы по футболу.\n",
      "Nsubj: сборная\n",
      "Gender: fem\n",
      "Tense: past\n",
      "\n",
      "\n",
      "Input: о это сообщать риа новость.\n",
      "Output: ['об', 'этом', 'сообщает', 'риа', 'новости', '.']\n",
      "Target: об этом сообщает риа новости.\n",
      "Nsubj: риа\n",
      "Gender: neut\n",
      "Tense: pres\n",
      "\n",
      "\n",
      "Input: также в ход обыск в дом романчук сотрудник правоохранительный орган наслать наличный средство на сумма в несколько сотня тысяча доллар сша, золотой украшение и награда.\n",
      "Output: ['также', 'в', 'ходе', 'обысков', 'в', 'доме', 'романчука', 'сотрудники', 'правоохранительных', 'органов', 'нашли', 'наличные', 'средства', 'на', 'сумму', 'в', 'несколько', 'сотен', 'тысяч', 'долларов', 'сша', ',', 'золотые', 'украшения', 'и', 'награду', '.']\n",
      "Target: также в ходе обыска в доме романчука сотрудники правоохранительных органов нашли наличные средства на сумму в несколько сотен тысяч долларов сша, золотые украшения и награды.\n",
      "Nsubj: сотрудники\n",
      "Gender: masc\n",
      "Tense: past\n",
      "\n",
      "\n",
      "Input: американский учёный выяснить, какой ограничение накладывать общий теория относительность на хронология, состоящий из более чем два событие.\n",
      "Output: ['американские', 'ученые', 'выяснили', ',', 'какие', 'ограничения', 'накладывать', 'общего', 'теории', 'относительности', 'на', 'хронологию', ',', 'состоящим', 'из', 'более', 'чем', 'два', 'события', '.']\n",
      "Target: американские ученые выяснили, какие ограничения накладывает общая теория относительности на хронологии, состоящие из более чем двух событий.\n",
      "Nsubj: ученые\n",
      "Gender: masc\n",
      "Tense: past\n",
      "\n",
      "\n",
      "Input: неизвестный открыть огонь в московский аэропорт « шереметьево » утром 13 ноябрь, сообщать итар-тасс.\n",
      "Output: ['неизвестный', 'открыл', 'огонь', 'в', 'московском', 'аэропорту', '«', 'шереметьево', '»', 'утром', '13', 'ноября', ',', 'сообщает', 'итар-тасс', '.']\n",
      "Target: неизвестный открыл огонь в московском аэропорту «шереметьево» утром 13 ноября, сообщает итар-тасс.\n",
      "Nsubj: неизвестный\n",
      "Gender: masc\n",
      "Tense: past\n",
      "\n",
      "\n",
      "Input: в итог к конец первый этап к россия, норвегия и германия приблизиться сборный чехия и италия.\n",
      "Output: ['в', 'итоге', 'к', 'концу', 'первого', 'этапа', 'к', 'россии', ',', 'норвегии', 'и', 'германии', 'приблизились', 'сборные', 'чехии', 'и', 'италии', '.']\n",
      "Target: в итоге к концу первого этапа к россии, норвегии и германии приблизились сборные чехии и италии.\n",
      "Nsubj: сборные\n",
      "Gender: fem\n",
      "Tense: past\n",
      "\n",
      "\n",
      "Input: руководитель служба безопасность украина валентин наливайченко счесть правомерный размещение приветствие в адрес дивизия сс \"галичина\"в львов, сообщать \"украiньскi новини\".\n",
      "Output: ['руководитель', 'службы', 'безопасности', 'украины', 'валентин', 'наливайченко', 'счел', 'правомерным', 'размещение', 'приветствие', 'в', 'адрес', 'дивизии', 'сс', '``', 'галичина', \"''\", 'в', 'львове', ',', 'сообщает', '``', 'украiньскi', 'новини', \"''\", '.']\n",
      "Target: руководитель службы безопасности украины валентин наливайченко счел правомерным размещение приветствий в адрес дивизии сс \"галичина\" во львове, сообщает \"украiньскi новини\".\n",
      "Nsubj: руководитель\n",
      "Gender: masc\n",
      "Tense: past\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for lemm_sent, target_sent, nsubj, gender, tense in test_input:\n",
    "    model_output = evaluate(model, lemm_sent, (nsubj, gender, tense))\n",
    "    print(f'Input: {lemm_sent}')\n",
    "    print(f'Output: {model_output}')\n",
    "    print(f'Target: {target_sent}')\n",
    "    print(f'Nsubj: {nsubj}')\n",
    "    print(f'Gender: {gender}')\n",
    "    print(f'Tense: {tense}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_samples = 15000\n",
    "\n",
    "test_lemm_sent = test_df.lemm_texts.to_list()[:max_samples]\n",
    "test_orig_sent = test_df.orig_texts.to_list()[:max_samples]\n",
    "test_nsubj = test_df.nsubj.to_list()[:max_samples]\n",
    "test_gender = test_df.gender.to_list()[:max_samples]\n",
    "test_tense = test_df.tense.to_list()[:max_samples]\n",
    "test_input = zip(test_lemm_sent, test_nsubj, test_gender, test_tense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqModel(\n",
       "  (embedding): Sequential(\n",
       "    (0): Embedding(125000, 300, padding_idx=1)\n",
       "    (1): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (context_mem): ContextMem(\n",
       "    (gender_proj): Linear(in_features=4, out_features=512, bias=False)\n",
       "    (tense_proj): Linear(in_features=3, out_features=512, bias=False)\n",
       "    (fc_out): Linear(in_features=1324, out_features=1024, bias=False)\n",
       "  )\n",
       "  (encoder): EncoderRNN(\n",
       "    (embedding): Sequential(\n",
       "      (0): Embedding(125000, 300, padding_idx=1)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (lstm): LSTM(300, 1024, bidirectional=True)\n",
       "    (fc_compressor_hidden): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "    (fc_compressor_cell): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "  )\n",
       "  (decoder): DecoderRNN(\n",
       "    (embedding): Sequential(\n",
       "      (0): Embedding(125000, 300, padding_idx=1)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (attn_weights): Sequential(\n",
       "      (0): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=1024, out_features=1, bias=False)\n",
       "      (3): Softmax(dim=1)\n",
       "    )\n",
       "    (lstm): LSTM(2348, 1024)\n",
       "    (fc_out): Linear(in_features=1024, out_features=125000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92307c3ac7c0487c9de30aaf3e1a9ccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d4f17a941dc438992876e362f4d815b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outputs = [evaluate(model, lemm_sent, (nsubj, gender, tense)) for lemm_sent, nsubj, gender, tense in tqdm(test_input)]\n",
    "targets = [[text_transformer.tokenize(target)] for target in tqdm(test_orig_sent)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.844"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = round(bleu_score(outputs, targets, max_n=1, weights=[1]), 3)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
